{
  "db": [
    {
      "meta": {
        "exported_on": 1768368672828,
        "version": "5.0.0"
      },
      "data": {
        "posts": [
          {
            "id": "1",
            "title": "Claude Opus 4.5: Anthropic Reclaims the Coding Crown with 80.9% SWE-bench Score",
            "slug": "claude-opus-4-5-release",
            "html": "<p>Anthropic just dropped Claude Opus 4.5, and the benchmarks are impressive. With an 80.9% score on SWE-bench Verified—the industry&#39;s most rigorous coding benchmark—Opus 4.5 reclaims the coding crown from Google&#39;s Gemini 3 Pro while delivering dramatic cost reductions and efficiency improvements.</p>\n<p><img src=\"https://jasoncochran.io/blog/claude-opus-4-5-release/coding-benchmarks.png\" alt=\"Claude Opus 4.5 SWE-bench Performance\"></p>\n<h2>The Headline Numbers</h2>\n<p><strong>SWE-bench Verified: 80.9%</strong> — This isn&#39;t just a marginal improvement. Opus 4.5 outperforms all frontier models including GPT-5.1 (76.3%), GPT-5.1-Codex-Max (77.9%), and Gemini 3 Pro (76.2%) on real-world software engineering tasks.</p>\n<p><strong>Pricing: 66% reduction</strong> — At $5 per million input tokens and $25 per million output tokens, Opus 4.5 costs dramatically less than its predecessor Opus 4.1 ($15/$75). This makes frontier-level AI accessible to a much broader range of developers and enterprises.</p>\n<p><strong>Token Efficiency: Up to 76% fewer tokens</strong> — The new effort parameter lets developers balance capability vs. cost. At medium effort, Opus 4.5 matches Sonnet 4.5&#39;s best performance while using 76% fewer output tokens.</p>\n<h2>Comprehensive Benchmark Dominance</h2>\n<p>The benchmark table tells the full story. Opus 4.5 leads or ties in nearly every category that matters for developers:</p>\n<p><img src=\"https://jasoncochran.io/blog/claude-opus-4-5-release/swe-bench-chart.png\" alt=\"Claude Opus 4.5 Benchmark Comparison\"></p>\n<h3>Agentic Capabilities</h3>\n<ul>\n<li><strong>SWE-bench Verified</strong>: 80.9% (industry-leading)</li>\n<li><strong>Terminal-bench 2.0</strong>: 59.3% vs Sonnet 4.5&#39;s 50.0%</li>\n<li><strong>OSWorld (Computer Use)</strong>: 66.3% — the best computer-using model available</li>\n<li><strong>MCP Atlas (Scaled Tool Use)</strong>: 62.3% vs Sonnet 4.5&#39;s 43.8%</li>\n<li><strong>τ2-bench (Agentic Tool Use)</strong>: 88.9% Retail, 98.2% Telecom</li>\n</ul>\n<h3>Reasoning and Knowledge</h3>\n<ul>\n<li><strong>ARC-AGI-2 (Novel Problem Solving)</strong>: 37.6% vs GPT-5.1&#39;s 17.6%—a 2x advantage</li>\n<li><strong>GPQA Diamond (Graduate Reasoning)</strong>: 87.0%</li>\n<li><strong>MMMU (Visual Reasoning)</strong>: 80.7%</li>\n<li><strong>MMMLU (Multilingual Q&amp;A)</strong>: 90.8%</li>\n</ul>\n<h2>The Effort Parameter: A Game Changer</h2>\n<p>Perhaps the most significant feature for developers is the new &quot;effort&quot; parameter. This API setting (low, medium, high) lets you control how much computational work the model does for each request.</p>\n<p><img src=\"https://jasoncochran.io/blog/claude-opus-4-5-release/performance-chart-1.png\" alt=\"Effort Parameter Performance\"></p>\n<p>The implications are significant:</p>\n<ul>\n<li><strong>Low effort</strong>: Fast responses for simple queries, minimal token usage</li>\n<li><strong>Medium effort</strong>: Matches Sonnet 4.5&#39;s peak performance with 76% fewer tokens</li>\n<li><strong>High effort</strong>: Exceeds Sonnet 4.5 by 4.3 percentage points while still using 48% fewer tokens</li>\n</ul>\n<p>This means you can optimize for cost on routine tasks and dial up capability when tackling complex problems—all with the same model.</p>\n<h2>Multilingual Coding Excellence</h2>\n<p>Opus 4.5 leads on 7 of 8 programming languages on SWE-bench Multilingual:</p>\n<p><img src=\"https://jasoncochran.io/blog/claude-opus-4-5-release/agentic-tasks.png\" alt=\"Multilingual Coding Performance\"></p>\n<p>Strong performance across C, C++, Java, JavaScript/TypeScript, PHP, Ruby, and Rust demonstrates broad coding competency rather than optimization for a single language ecosystem.</p>\n<h2>Agentic Search and Deep Research</h2>\n<p>For developers building AI agents that need to browse the web, search for information, and synthesize results, Opus 4.5 shows substantial improvements:</p>\n<p><img src=\"https://jasoncochran.io/blog/claude-opus-4-5-release/performance-chart-2.png\" alt=\"Deep Research Agents Performance\"></p>\n<p>The BrowseComp-Plus benchmark specifically tests frontier agentic search capabilities—the kind of work that matters for building research assistants, competitive intelligence tools, and automated analysis systems.</p>\n<h2>Real-World Validation</h2>\n<p>Beyond benchmarks, Anthropic shared a compelling data point: Opus 4.5 scored higher than any human candidate on Anthropic&#39;s notoriously difficult internal take-home engineering exam within the 2-hour time limit.</p>\n<p>The τ2-bench airline service scenario provides another illustration. The model identified a creative solution to a customer problem by recognizing that cabin upgrades were allowed for basic economy before flight modifications—finding a legitimate workaround the benchmark creators hadn&#39;t anticipated. This demonstrates genuine problem-solving intelligence rather than pattern matching.</p>\n<h2>Product Ecosystem Updates</h2>\n<p>Alongside the model release, Anthropic announced several product improvements:</p>\n<p><strong>Claude Code in Desktop App</strong>: Run multiple local and remote coding sessions in parallel directly from the desktop app. Plan Mode now generates editable markdown plans before execution.</p>\n<p><strong>Endless Chat</strong>: Long conversations no longer hit token limits. The system automatically summarizes context so conversations can continue indefinitely.</p>\n<p><strong>Claude for Chrome</strong>: Extended to all Max users for cross-tab task automation.</p>\n<p><strong>Claude for Excel</strong>: Expanded beta access to Max, Team, and Enterprise subscribers.</p>\n<h2>Technical Specifications</h2>\n<ul>\n<li><strong>Model ID</strong>: <code>claude-opus-4-5-20251101</code></li>\n<li><strong>Context Window</strong>: 200,000 tokens</li>\n<li><strong>Max Output</strong>: 64,000 tokens</li>\n<li><strong>Knowledge Cutoff</strong>: March 2025</li>\n<li><strong>Pricing</strong>: $5/million input, $25/million output tokens</li>\n</ul>\n<h2>Safety and Alignment</h2>\n<p>Anthropic describes Opus 4.5 as &quot;the most robustly aligned model we have released to date&quot; and likely the best-aligned frontier model available. Notably:</p>\n<ul>\n<li>Substantial progress in robustness against prompt injection attacks</li>\n<li>Harder to deceive than competing frontier models</li>\n<li>Industry-leading resistance to malicious manipulation attempts</li>\n</ul>\n<h2>What This Means for Developers</h2>\n<p>For those of us using AI coding assistants daily, Opus 4.5 represents a meaningful upgrade:</p>\n<p><strong>Cost-Performance Optimization</strong>: The effort parameter lets you match workload complexity to computational resources. Simple tasks stay cheap; complex problems get the full capability.</p>\n<p><strong>Agent Development</strong>: The 66.3% OSWorld score and 62.3% MCP Atlas performance make Opus 4.5 the strongest foundation for building autonomous agents that interact with computers and tools.</p>\n<p><strong>Enterprise Viability</strong>: The 66% price reduction makes frontier AI economically viable for more use cases. Combined with the alignment improvements, this addresses two of the biggest enterprise adoption barriers.</p>\n<p><strong>Competitive Coding</strong>: The 80.9% SWE-bench score means fewer iterations, better first-attempt code, and less time debugging AI-generated implementations.</p>\n<h2>The Competitive Landscape</h2>\n<p>This release puts pressure on both OpenAI and Google. A few weeks ago, Gemini 3&#39;s benchmark dominance suggested Google had pulled ahead. Opus 4.5 shifts the narrative back.</p>\n<p>The AI model race continues to accelerate. For developers, this competition means better tools, lower prices, and faster capability improvements. The challenge is keeping up with which model works best for which use case—a problem that gets harder as models converge in capability while diverging in specific strengths.</p>\n<h2>My Take</h2>\n<p>Having used Claude models extensively for the past year, the combination of improved coding capability, better token efficiency, and dramatic cost reduction makes Opus 4.5 compelling for daily development work. The effort parameter addresses a real pain point—paying premium prices for simple queries while getting underwhelming results on complex ones.</p>\n<p>The alignment focus is also notable. As AI models become more capable, the safety question becomes more important. Anthropic&#39;s emphasis on robustness against prompt injection is particularly relevant for anyone building production systems where adversarial inputs are possible.</p>\n<p>For my work, Opus 4.5 is now my default model for complex coding tasks and agent development. The SWE-bench scores translate to real productivity gains when working through challenging implementation problems.</p>\n<hr>\n<p><strong>Resources</strong>:</p>\n<ul>\n<li><a href=\"https://www.anthropic.com/news/claude-opus-4-5\">Claude Opus 4.5 Announcement</a> - Official Anthropic announcement</li>\n<li><a href=\"https://www.anthropic.com/claude/opus\">Claude Opus Product Page</a> - Detailed specifications</li>\n<li><a href=\"https://docs.anthropic.com\">API Documentation</a> - Developer integration guide</li>\n</ul>\n<p><strong>Sources</strong>:</p>\n<ul>\n<li><a href=\"https://www.anthropic.com/news/claude-opus-4-5\">Anthropic Official Announcement</a></li>\n<li><a href=\"https://www.cnbc.com/2025/11/24/anthropic-unveils-claude-opus-4point5-its-latest-ai-model.html\">CNBC Coverage</a></li>\n<li><a href=\"https://techcrunch.com/2025/11/24/anthropic-releases-opus-4-5-with-new-chrome-and-excel-integrations/\">TechCrunch Analysis</a></li>\n<li><a href=\"https://thenewstack.io/anthropics-new-claude-opus-4-5-reclaims-the-coding-crown-from-gemini-3/\">The New Stack Technical Review</a></li>\n</ul>\n",
            "feature_image": "https://jasoncochran.io/blog/claude-opus-4-5-release/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-11-23 00:00:00",
            "updated_at": "2025-11-23 00:00:00",
            "published_at": "2025-11-23 00:00:00",
            "custom_excerpt": "Anthropic releases Claude Opus 4.5, achieving industry-leading 80.9% on SWE-bench Verified, 66% price reduction, and breakthrough efficiency with the new effort parameter. Here's what developers need to know."
          },
          {
            "id": "2",
            "title": "The BMAD Method: Transforming AI-Assisted Development with Structured Workflows",
            "slug": "bmad-method-ai-driven-development",
            "html": "<p>After a year of daily work with AI coding assistants like Claude Code and Cursor, I&#39;ve witnessed both their remarkable potential and their limitations. While AI can generate impressive code quickly, the challenge has always been maintaining architectural consistency, preventing context loss, and ensuring that generated code aligns with complex business requirements across multi-week projects.</p>\n<p>The BMAD Method offers a compelling solution to these challenges through a fundamentally different approach to AI-assisted development.</p>\n<h2>What Is the BMAD Method?</h2>\n<p>BMAD stands for &quot;Breakthrough Method for Agile AI-Driven Development&quot; (also interpreted as &quot;Build More, Architect Dreams&quot;). It&#39;s an open-source framework that replaces generic AI prompting with structured, battle-tested workflows guided by specialized AI agents.</p>\n<p>Unlike traditional approaches where you prompt a single AI assistant and hope for consistent results, BMAD introduces a team of 19 specialized agents working collaboratively across four distinct domains:</p>\n<ul>\n<li><strong>Development</strong>: Developer, UX Designer, Tech Writer</li>\n<li><strong>Architecture</strong>: Architect, Test Architect, Game Architect</li>\n<li><strong>Product</strong>: PM, Analyst, Game Designer</li>\n<li><strong>Leadership</strong>: Scrum Master, BMad Master, Game Developer</li>\n</ul>\n<p>Each agent carries specific expertise and delivers consistent outputs within their domain of responsibility.</p>\n<h2>The Core Philosophy: Collaboration-Optimized Reflection Engine</h2>\n<p>At its heart, BMAD Core functions as a &quot;Collaboration Optimized Reflection Engine&quot;—a modular foundation that enables human-AI teamwork across the entire development lifecycle. This isn&#39;t about replacing developers; it&#39;s about augmenting teams with specialized AI collaborators that maintain context, architectural decisions, and requirements across complex projects.</p>\n<p>The framework&#39;s genius lies in how it structures collaboration. Rather than dumping all context into a single AI conversation, BMAD distributes expertise across specialized agents that hand off work at defined phases, much like a well-functioning development team.</p>\n<h2>The Four-Phase Development Lifecycle</h2>\n<p>BMAD structures development into four distinct phases, each handled by specialized agents:</p>\n<h3>1. Analysis — Exploration and Research</h3>\n<p>The <strong>Analyst Agent</strong> conducts project discovery through strategic questioning about target users, pain points, market positioning, and success metrics. This phase is optional for small projects but invaluable for greenfield development where requirements aren&#39;t yet crystallized.</p>\n<p>In practice, this means you answer targeted questions rather than writing lengthy requirements documents. The agent probes for edge cases, user motivations, and business constraints that might otherwise surface weeks into development.</p>\n<h3>2. Planning — Document Creation</h3>\n<p>The <strong>PM Agent</strong> converts project briefs into comprehensive Product Requirements Documents (PRDs), complete with:</p>\n<ul>\n<li>Integrated epic definitions</li>\n<li>Functional requirements mapped to user stories</li>\n<li>Success criteria and acceptance tests</li>\n<li>User story hierarchies</li>\n</ul>\n<p>The <strong>Product Owner Agent</strong> then validates alignment between requirements and technical constraints, catching mismatches before they become expensive problems during implementation.</p>\n<h3>3. Solutioning — Architecture and Design</h3>\n<p>The <strong>Architect Agent</strong> designs technical systems including:</p>\n<ul>\n<li>Database schemas optimized for your use case</li>\n<li>API specifications following RESTful or GraphQL patterns</li>\n<li>Security architecture (authentication flows, authorization models)</li>\n<li>Integration patterns for third-party services</li>\n<li>Technology stack recommendations</li>\n</ul>\n<p>This phase produces detailed technical specifications that serve as guardrails during implementation. The architect&#39;s decisions are documented in a format that subsequent agents can reference, maintaining consistency across the codebase.</p>\n<h3>4. Implementation — Story-Driven Development</h3>\n<p>The <strong>Scrum Master Agent</strong> breaks down architecture into actionable development stories with full context. Each story includes:</p>\n<ul>\n<li>Architectural decisions that inform the implementation</li>\n<li>Security requirements specific to the feature</li>\n<li>Integration points with existing systems</li>\n<li>Acceptance criteria tied back to PRD requirements</li>\n</ul>\n<p>The <strong>Dev Agent</strong> then implements features with awareness of prior design decisions—no more explaining JWT choices or OAuth integration requirements repeatedly. The <strong>QA Agent</strong> provides senior-level code reviews, catching issues before they reach production.</p>\n<h2>Scale-Adaptive Intelligence</h2>\n<p>One of BMAD&#39;s smartest features is automatic adjustment of planning depth based on project scope:</p>\n<p><strong>Quick Flow (&lt; 5 minutes)</strong>: For bug fixes and small features, the framework streamlines to essential planning only.</p>\n<p><strong>BMad Method (&lt; 15 minutes)</strong>: Full four-phase workflow for products and platforms.</p>\n<p><strong>Enterprise (&lt; 30 minutes)</strong>: Extended planning for compliance-heavy, large-scale systems requiring audit trails and extensive documentation.</p>\n<p>This means you&#39;re not forced into heavyweight processes for trivial changes, but the structure is there when complexity demands it.</p>\n<h2>Document Sharding: The Secret to 90% Token Efficiency</h2>\n<p>Traditional AI development suffers from context window limitations. You either:</p>\n<ol>\n<li>Dump everything into the context and hit token limits</li>\n<li>Lose important context as conversations grow longer</li>\n<li>Repeat architectural decisions across multiple AI conversations</li>\n</ol>\n<p>BMAD solves this through <strong>document sharding</strong>—breaking comprehensive planning documents into focused, consumable pieces for specific implementation tasks. Each shard contains:</p>\n<ul>\n<li>The specific feature requirements</li>\n<li>Relevant architectural decisions</li>\n<li>Security considerations for that component</li>\n<li>Integration contracts</li>\n</ul>\n<p>This achieves <strong>90% token efficiency gains</strong> according to the project documentation, while maintaining persistent context across development phases. The Dev agent has complete context for its current task without carrying irrelevant information from unrelated features.</p>\n<h2>Real-World Productivity Metrics</h2>\n<p>The productivity claims around AI tools often feel exaggerated, but BMAD&#39;s numbers are backed by controlled studies:</p>\n<p><strong>Time Reduction</strong>:</p>\n<ul>\n<li>Planning: 6 hours vs. 2-3 weeks of traditional requirements discovery</li>\n<li>Development: 3x faster than typical timelines</li>\n<li>Code review: 70% reduction in review time</li>\n</ul>\n<p><strong>Cost Analysis</strong> (medium-complexity project):</p>\n<ul>\n<li>Traditional development: $48,000</li>\n<li>BMAD approach: $20,700</li>\n<li><strong>Cost reduction: 57%</strong></li>\n</ul>\n<p><strong>Quality Improvements</strong>:</p>\n<ul>\n<li>Near 100% architectural consistency</li>\n<li>Minimal technical debt through documented decisions</li>\n<li>Reduced refactoring needs post-launch</li>\n</ul>\n<p>These metrics align with broader industry research. GitHub&#39;s controlled studies show structured AI tools deliver 55% faster task completion. Microsoft and Accenture studies demonstrate enterprise teams using structured AI approaches achieved 12.92% to 21.83% more pull requests per week.</p>\n<h2>Multi-Tool Compatibility</h2>\n<p>BMAD works with major AI platforms:</p>\n<ul>\n<li>ChatGPT (GPT-4, GPT-4o, o1, o3)</li>\n<li>Claude (Sonnet 4.5, Opus 4)</li>\n<li>Google Gemini (Gemini 3 Pro, Deep Think)</li>\n</ul>\n<p>You run the planning phases in web-based AI tools where longer context windows are available, then transition to IDE-integrated tools like Cursor or Claude Code for implementation.</p>\n<p>This tool-agnostic design means teams can adopt BMAD without vendor lock-in. The specifications and architectural decisions become the contract, independent of which AI generates the implementation.</p>\n<h2>Practical Implementation</h2>\n<p>Getting started with BMAD is straightforward:</p>\n<h3>Week 1 Setup</h3>\n<ol>\n<li>Clone the BMAD Method repository from GitHub</li>\n<li>Download agent configuration files for your preferred AI platform</li>\n<li>Upload configurations with provided instructions</li>\n<li>Initialize with <code>*help</code> command to explore available agents</li>\n</ol>\n<h3>Standard Greenfield Workflow</h3>\n<pre><code class=\"language-bash\">*analyst          # Project discovery and requirements gathering\n*agent pm         # Generate comprehensive PRD\n*agent architect  # Create technical architecture\n*agent po         # Validate alignment between requirements and design\n*agent sm         # Break down into development stories\n*agent dev        # Implement features with full context\n*agent qa         # Code review and optimization\n</code></pre>\n<p>Each agent command produces structured outputs that feed into the next phase, building a complete project specification before code generation begins.</p>\n<h2>Real-World Example: Goal Management Application</h2>\n<p>The BMAD documentation showcases implementation through a goal management app called &quot;Steps.&quot; The workflow demonstrates how the Dev agent maintained complete context about:</p>\n<ul>\n<li>JWT token choices made during architecture phase</li>\n<li>Encryption standards selected for sensitive data</li>\n<li>OAuth integration requirements for third-party services</li>\n<li>Database schema decisions for user hierarchies</li>\n</ul>\n<p>Rather than re-explaining these decisions with each new feature, the agent referenced prior architectural documentation. This eliminated the frustrating cycle of &quot;didn&#39;t I already tell you about our auth strategy?&quot;</p>\n<h2>How BMAD Compares to Other Approaches</h2>\n<h3>vs. OpenSpec (Spec-Driven Development)</h3>\n<p>I wrote recently about OpenSpec, which introduces specification workflows for AI assistants. BMAD and OpenSpec share similar philosophies—establish specifications before implementation—but differ in scope:</p>\n<p><strong>OpenSpec</strong>: Lightweight framework focused on spec-driven workflows within your existing development process. Best for teams wanting to add structure to AI prompting without changing their entire methodology.</p>\n<p><strong>BMAD</strong>: Comprehensive framework covering the full development lifecycle from market research through QA. Best for greenfield projects or teams adopting AI-first development approaches.</p>\n<p>The two aren&#39;t mutually exclusive. Teams could use BMAD for initial planning and architecture, then transition to OpenSpec workflows for ongoing feature development.</p>\n<h3>vs. Raw AI Prompting</h3>\n<p>Traditional AI prompting:</p>\n<ul>\n<li>Fast initial results</li>\n<li>Frequent context loss</li>\n<li>Inconsistent architectural decisions</li>\n<li>High iteration overhead</li>\n</ul>\n<p>BMAD Method:</p>\n<ul>\n<li>Slower initial setup (6-15 minutes of planning)</li>\n<li>Persistent context across development</li>\n<li>Documented architectural decisions</li>\n<li>Lower iteration overhead during implementation</li>\n</ul>\n<p>For trivial scripts and one-off tasks, raw prompting remains faster. For anything that will grow beyond a few hundred lines or require architectural consistency, BMAD&#39;s structure pays dividends.</p>\n<h2>Who Should Use the BMAD Method?</h2>\n<p>BMAD is particularly valuable for:</p>\n<p><strong>Solo developers building complex products</strong>: Maintain architectural consistency across your codebase without the overhead of traditional specification documents. The agents serve as a virtual team providing checks and balances.</p>\n<p><strong>Small teams with limited senior resources</strong>: The Architect and QA agents provide senior-level perspective that might otherwise require expensive consulting or mentorship.</p>\n<p><strong>Agencies building client projects</strong>: Generate comprehensive documentation that clients can review before development begins. The transparent decision trail improves client confidence and reduces scope creep.</p>\n<p><strong>Regulated industries</strong>: Create audit trails showing what was specified vs. what was implemented, useful for compliance and code review processes.</p>\n<p><strong>Greenfield development with uncertain requirements</strong>: The Analyst and PM agents help crystallize requirements through structured questioning rather than assuming you&#39;ve thought through every edge case.</p>\n<h2>Limitations and Considerations</h2>\n<p>BMAD isn&#39;t a silver bullet. Some considerations:</p>\n<p><strong>Learning curve</strong>: Approximately two months before mastering advanced techniques. The initial workflows feel rigid until you understand when to streamline.</p>\n<p><strong>Upfront time investment</strong>: 6-15 minutes of planning before coding begins. For rapid prototyping or exploratory coding, this overhead may not be justified.</p>\n<p><strong>Platform switching overhead</strong>: Moving from web-based planning tools to IDE-based implementation tools requires context transfer. The document sharding helps, but there&#39;s still friction.</p>\n<p><strong>Overkill for simple tasks</strong>: Bug fixes and trivial features don&#39;t need the full BMAD workflow. Quick Flow mode helps, but you&#39;re still operating within a framework designed for complexity.</p>\n<p><strong>Agent customization complexity</strong>: While BMAD provides default agents, customizing them for your specific domain requires prompt engineering expertise.</p>\n<p>For complex features, multi-month projects, or systems with multiple contributors (human or AI), these costs are acceptable trade-offs for the architectural consistency BMAD delivers.</p>\n<h2>Integration with Existing Practices</h2>\n<p>BMAD complements other development practices I advocate:</p>\n<p><strong>Test-Driven Development</strong>: BMAD&#39;s specifications define behavior, tests verify it, AI implements it. The QA agent can generate test suites based on architectural specs.</p>\n<p><strong>Hexagonal Architecture</strong>: The Architect agent documents port contracts clearly, making it easier to maintain clean boundaries between core business logic and external adapters.</p>\n<p><strong>Domain-Driven Design</strong>: The PM agent helps capture ubiquitous language and domain rules in the PRD, which then flow through to implementation.</p>\n<p><strong>CI/CD Practices</strong>: BMAD&#39;s structured outputs integrate well with automated deployment pipelines. The documented architectural decisions help configure infrastructure as code.</p>\n<h2>The Economics Beyond Time</h2>\n<p>While the 57% cost reduction and 3x development speed are compelling, BMAD&#39;s economic value extends further:</p>\n<p><strong>Reduced technical debt</strong>: Documented architectural decisions prevent the accumulation of inconsistent patterns that plague projects developed through ad-hoc AI prompting.</p>\n<p><strong>Faster onboarding</strong>: New team members (human or AI) can review planning documents to understand system design rather than reverse-engineering decisions from code.</p>\n<p><strong>Lower refactoring costs</strong>: Getting architecture right upfront reduces the expensive refactoring that typically occurs when scaling from MVP to production system.</p>\n<p><strong>Client confidence</strong>: For consultants and agencies, the transparent planning phase provides deliverables that clients can review before significant development costs are incurred.</p>\n<p><strong>Regulatory compliance</strong>: In regulated industries, the audit trail of specifications vs. implementation provides documentation often required for compliance reviews.</p>\n<h2>The Future of Structured AI Development</h2>\n<p>BMAD represents where AI-assisted development is heading:</p>\n<p><strong>Explicit workflows</strong> over implicit prompting\n<strong>Specialized agents</strong> over general-purpose assistants\n<strong>Persistent context</strong> over conversation-based memory\n<strong>Reproducible generation</strong> over unpredictable outputs\n<strong>Multi-agent collaboration</strong> over single-tool dependence</p>\n<p>As AI coding assistants become more powerful, the constraint shifts from generation capability to alignment and consistency. BMAD provides the scaffolding for maintaining that alignment across complex, long-running projects.</p>\n<h2>Getting Started</h2>\n<p>The BMAD Method is open-source and available on GitHub at <a href=\"https://github.com/bmad-code-org/BMAD-METHOD\">bmad-code-org/BMAD-METHOD</a>, licensed under MIT.</p>\n<p>The repository includes:</p>\n<ul>\n<li>Agent configuration files for ChatGPT, Claude, and Gemini</li>\n<li>Comprehensive documentation and workflow guides</li>\n<li>Example projects demonstrating the methodology</li>\n<li>Visual workflow diagrams showing complete methodology</li>\n<li>50+ workflow templates for common scenarios</li>\n</ul>\n<p>Version 6 represents a complete architectural evolution, introducing:</p>\n<ul>\n<li>Modular architecture enabling custom domain solutions</li>\n<li>Scale-adaptive intelligence adjusting automatically to project scope</li>\n<li>BMad Builder module for creating custom agent teams</li>\n<li>Beautiful SVG diagrams documenting workflows</li>\n</ul>\n<p>Start small—try the Quick Flow mode on a single feature to understand the workflow. Once comfortable, graduate to full BMAD Method for greenfield projects where the structured approach provides maximum value.</p>\n<h2>Conclusion</h2>\n<p>After years of building enterprise systems and recently integrating AI tools into my workflow, I&#39;ve learned that raw AI generation capability isn&#39;t the bottleneck—it&#39;s maintaining architectural consistency and preventing context loss across complex projects.</p>\n<p>The BMAD Method addresses this through specialized agents, structured workflows, and document sharding that maintains persistent context. The upfront planning investment pays dividends in development velocity, code quality, and architectural consistency.</p>\n<p>For developers building anything beyond simple scripts—especially those working solo or in small teams without dedicated architecture resources—BMAD provides the structure to leverage AI assistants at scale without sacrificing quality.</p>\n<p>The productivity metrics are impressive, but the real value is in sleeping better knowing your AI-generated codebase maintains architectural consistency rather than accumulating technical debt through inconsistent prompting.</p>\n<p>If you&#39;re using AI assistants for serious development work, BMAD is worth exploring. The learning curve is real, but so are the benefits.</p>\n<hr>\n<p><strong>Resources</strong>:</p>\n<ul>\n<li><a href=\"https://github.com/bmad-code-org/BMAD-METHOD\">BMAD Method on GitHub</a> - Open-source framework</li>\n<li><a href=\"https://bmadcodes.com/\">BMad Code Official Site</a> - Documentation and guides</li>\n<li><a href=\"https://buildmode.dev/blog/mastering-bmad-method-2025/\">Implementation Guide</a> - Detailed walkthrough</li>\n<li><a href=\"https://bennycheung.github.io/bmad-reclaiming-control-in-ai-dev\">BMAD vs Traditional Development</a> - Comparative analysis</li>\n</ul>\n",
            "feature_image": "https://jasoncochran.io/blog/bmad-method-ai-driven-development/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-11-22 00:00:00",
            "updated_at": "2025-11-22 00:00:00",
            "published_at": "2025-11-22 00:00:00",
            "custom_excerpt": "Discover how the BMAD Method brings enterprise-grade structure to AI development through specialized agents, achieving 57% cost reduction and 3x faster delivery while maintaining architectural consistency."
          },
          {
            "id": "3",
            "title": "Google Gemini 3: Achieving Benchmark Dominance Across AI Leaderboards",
            "slug": "google-gemini-3-benchmark-dominance",
            "html": "<p>Google has released Gemini 3, and the results are nothing short of remarkable. With both a Pro version and Deep Think reasoning variant, Gemini 3 has established itself as the new benchmark leader, outperforming competitors from OpenAI and Anthropic across nearly every metric.</p>\n<h2>Unprecedented Benchmark Performance</h2>\n<p>Gemini 3 Pro achieved top scores on <strong>19 of 20 benchmarks</strong> tested against leading competitors including GPT-5.1 and Claude Sonnet 4.5. This isn&#39;t just incremental improvement—it&#39;s a comprehensive demonstration of technical superiority across the board.</p>\n<p><img src=\"https://jasoncochran.io/blog/google-gemini-3-benchmark-dominance/benchmarks.webp\" alt=\"Gemini 3 Benchmark Comparison\"></p>\n<h3>Key Performance Highlights</h3>\n<p><strong>Humanity&#39;s Last Exam</strong>: Gemini 3 achieved an 11 percentage-point improvement, reaching 37.5% compared to GPT-5.1&#39;s 26.5%. This benchmark tests advanced reasoning capabilities that push AI systems to their limits.</p>\n<p><strong>Vending-Bench 2</strong>: In practical economic scenarios, Gemini 3 generated approximately $5,500 in profit compared to Claude Sonnet 4.5&#39;s $3,800—a substantial 45% advantage in real-world decision-making tasks.</p>\n<p><strong>SimpleQA Verified</strong>: The model demonstrated a ~40% gap between Gemini 3 Pro and the competition on factuality measures, showcasing superior accuracy in providing verifiable information.</p>\n<p><strong>Artificial Analysis Intelligence Index</strong>: Gemini 3 achieved the largest lead gap seen in considerable time, scoring three points above GPT-5.1—a significant margin in this comprehensive evaluation framework.</p>\n<h2>The ARC-AGI 2 Breakthrough</h2>\n<p>Perhaps the most striking achievement is Gemini 3&#39;s performance on ARC-AGI 2, a benchmark specifically designed to measure fluid intelligence rather than mere pattern memorization:</p>\n<ul>\n<li><strong>Gemini 3 Pro</strong>: 31.1% accuracy</li>\n<li><strong>Gemini 3 Deep Think</strong>: 45.1% accuracy</li>\n<li><strong>GPT-5.1 Thinking</strong>: 17.6% accuracy</li>\n</ul>\n<p>This represents a <strong>2-3x performance advantage</strong> over OpenAI&#39;s flagship reasoning model. ARC-AGI 2 tests the kind of abstract reasoning and pattern recognition that many consider a prerequisite for artificial general intelligence, making this gap particularly significant.</p>\n<p>The Deep Think variant&#39;s 45.1% score suggests that Google has made substantial progress in building models that can reason through novel problems without relying on memorized patterns from training data.</p>\n<h2>Technical Architecture</h2>\n<p>Gemini 3 represents a ground-up architectural achievement:</p>\n<ul>\n<li><strong>Native multimodal design</strong>: Built from scratch rather than retrofitted onto previous models</li>\n<li><strong>Sparse mixture-of-experts architecture</strong>: Efficiently activates only relevant model components for each task</li>\n<li><strong>Massive context windows</strong>: 1 million token input capacity with 64K output</li>\n<li><strong>Custom hardware training</strong>: Trained exclusively on Google&#39;s TPU infrastructure</li>\n<li><strong>Enhanced capabilities</strong>: Autonomous software task execution, complex multi-step workflows, and superior understanding of handwritten and visual content across multiple languages</li>\n</ul>\n<h2>Strategic Implications</h2>\n<p>The comprehensiveness of Gemini 3&#39;s advantages is particularly noteworthy. As analyst Alberto Romero points out, this isn&#39;t a case of excelling in one area while compromising others. Google has achieved improvements distributed across all model components—reasoning, factuality, multimodal understanding, and practical task execution.</p>\n<p>This release demonstrates that Google maintains leadership across multiple AI development fronts simultaneously. The company&#39;s investment in custom hardware (TPUs) and native multimodal architecture appears to be paying dividends in model capability.</p>\n<h2>What This Means for Developers</h2>\n<p>For developers and organizations choosing AI infrastructure, Gemini 3 presents a compelling option:</p>\n<p><strong>Strengths</strong>:</p>\n<ul>\n<li>Best-in-class performance across diverse tasks</li>\n<li>Superior reasoning capabilities for complex problems</li>\n<li>Massive context windows for handling large codebases and documents</li>\n<li>Strong multimodal understanding for working with images, handwriting, and visual content</li>\n</ul>\n<p><strong>Considerations</strong>:</p>\n<ul>\n<li>Higher API pricing compared to some alternatives</li>\n<li>Ecosystem maturity compared to more established competitors</li>\n<li>Integration complexity depending on existing toolchains</li>\n</ul>\n<h2>The Competitive Landscape</h2>\n<p>This release intensifies competition in the frontier AI model space. While OpenAI and Anthropic have made significant advances with their recent releases, Google has demonstrated that the race for AI capability leadership remains wide open.</p>\n<p>The particularly strong showing on ARC-AGI 2 suggests that progress toward more general AI capabilities continues to accelerate. The gap between Gemini 3 Deep Think and GPT-5.1 Thinking on this benchmark indicates that different architectural approaches and training methodologies can yield substantially different results on tests of fluid intelligence.</p>\n<h2>Looking Forward</h2>\n<p>Gemini 3&#39;s benchmark dominance represents more than just competitive positioning—it demonstrates continued rapid progress in AI capabilities. The model&#39;s strong performance on reasoning tasks, factuality measures, and practical applications suggests we&#39;re seeing genuine advances in AI capability rather than mere optimization of existing approaches.</p>\n<p>As the AI landscape continues to evolve at breakneck speed, one thing is clear: the competition between frontier labs is driving unprecedented progress. For those building AI-powered applications, the expanding capabilities of models like Gemini 3 open up new possibilities that seemed out of reach just months ago.</p>\n<p>The question now isn&#39;t whether AI models can handle complex tasks—it&#39;s how quickly we can adapt our workflows and applications to leverage these rapidly expanding capabilities.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/google-gemini-3-benchmark-dominance/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-11-19 00:00:00",
            "updated_at": "2025-11-19 00:00:00",
            "published_at": "2025-11-19 00:00:00",
            "custom_excerpt": "Google's Gemini 3 has emerged as a powerhouse, topping 19 of 20 benchmarks and achieving a stunning 2-3x advantage on ARC-AGI 2 over GPT-5.1. Here's what this means for the AI landscape."
          },
          {
            "id": "4",
            "title": "OpenSpec: Aligning AI and Developers Through Spec-Driven Development",
            "slug": "openspec-ai-driven-specification-workflow",
            "html": "<p>After working with AI coding assistants like Claude Code and Cursor for the past year, I&#39;ve experienced both their transformative potential and their occasional frustration. AI can generate impressive code quickly, but without clear specifications, you often get code that <em>almost</em> does what you want—requiring multiple rounds of clarification and regeneration.</p>\n<p>Enter OpenSpec: a lightweight framework designed to solve this exact problem.</p>\n<h2>The Problem OpenSpec Solves</h2>\n<p>When you prompt an AI assistant without a specification, you&#39;re essentially hoping it infers your complete requirements from a brief description. This leads to:</p>\n<ul>\n<li><strong>Unpredictable outputs</strong>: Different interpretations of vague requirements</li>\n<li><strong>Missing features</strong>: AI doesn&#39;t know what you consider essential</li>\n<li><strong>Unwanted additions</strong>: AI adds &quot;helpful&quot; features you didn&#39;t need</li>\n<li><strong>Iteration overhead</strong>: Multiple back-and-forth cycles to align on behavior</li>\n<li><strong>Lost context</strong>: Previous decisions get forgotten across conversations</li>\n</ul>\n<p>Sound familiar? This is the reality of prompt-driven development at scale.</p>\n<h2>What Is OpenSpec?</h2>\n<p>OpenSpec is an open-source framework that introduces <strong>spec-driven development for AI coding assistants</strong>. Instead of jumping straight to code generation, OpenSpec establishes a workflow where you and your AI assistant first agree on <em>what to build</em> through explicit specifications.</p>\n<p>As the project states: &quot;OpenSpec aligns humans and AI coding assistants with spec-driven development so you agree on what to build before any code is written.&quot;</p>\n<h3>The Two-Folder Architecture</h3>\n<p>OpenSpec uses a simple but powerful structure:</p>\n<pre><code>openspec/\n├── specs/          # Source of truth - current specifications\n└── changes/        # Active proposals and tasks\n</code></pre>\n<p><strong>specs/</strong>: Contains your authoritative specifications—the single source of truth for how your system should behave.</p>\n<p><strong>changes/</strong>: Holds active change proposals. Each change groups related spec updates, implementation tasks, and context together. This keeps your diffs explicit and makes it easy to see exactly what&#39;s being modified.</p>\n<h2>The OpenSpec Workflow</h2>\n<p>The framework introduces a five-stage workflow that transforms how you work with AI assistants:</p>\n<h3>1. Draft</h3>\n<p>Request an OpenSpec change proposal. Your AI assistant analyzes the request and drafts specification updates rather than jumping directly to code.</p>\n<pre><code class=\"language-bash\">&quot;Create an OpenSpec change to add user authentication with JWT tokens&quot;\n</code></pre>\n<p>The AI generates a change proposal that includes:</p>\n<ul>\n<li>Specification updates detailing the authentication flow</li>\n<li>API contract definitions</li>\n<li>Security considerations</li>\n<li>Implementation tasks broken down</li>\n</ul>\n<h3>2. Review</h3>\n<p>Iterate on the specifications with your AI assistant until you reach agreement. This is where you catch misunderstandings early—before any code is generated.</p>\n<p>During review, you might refine:</p>\n<ul>\n<li>API endpoint signatures</li>\n<li>Error handling behavior</li>\n<li>Validation rules</li>\n<li>Edge cases</li>\n</ul>\n<h3>3. Implement</h3>\n<p>With specifications agreed upon, now you implement. Your AI assistant references the approved specs when generating code, resulting in deterministic, reviewable outputs that match your expectations.</p>\n<h3>4. Complete</h3>\n<p>Work through the implementation tasks. Because the specs are clear, there&#39;s less back-and-forth and fewer surprises.</p>\n<h3>5. Archive</h3>\n<p>Once implementation is complete and tested, archive the change. This merges the approved specification updates back into your source-of-truth specs, maintaining an auditable history.</p>\n<h2>Multi-Tool Support</h2>\n<p>One of OpenSpec&#39;s smartest design decisions is its tool-agnostic approach. It works with:</p>\n<ul>\n<li><strong>Claude Code</strong>: Native slash commands</li>\n<li><strong>Cursor</strong>: Native slash commands</li>\n<li><strong>CodeBuddy</strong>: Native slash commands</li>\n<li><strong>Other AI tools</strong>: Via the <code>AGENTS.md</code> convention</li>\n</ul>\n<p>This means teams can use different AI assistants while sharing unified specifications. The specs become the contract, independent of which tool generates the implementation.</p>\n<h2>Real-World Impact</h2>\n<p>Let me share how this changes the development experience:</p>\n<h3>Before OpenSpec</h3>\n<p><strong>Prompt</strong>: &quot;Add a user dashboard with analytics&quot;</p>\n<p><strong>AI Response</strong>: Generates a dashboard component with charts you didn&#39;t want, missing the metrics you did want, using a state management pattern different from the rest of your app.</p>\n<p><strong>You</strong>: Spend the next 30 minutes explaining what you actually wanted, regenerating code multiple times.</p>\n<h3>With OpenSpec</h3>\n<p><strong>Phase 1 - Draft Spec</strong>:</p>\n<pre><code class=\"language-markdown\"># User Dashboard Specification\n\n## Overview\nDisplay user activity metrics consistent with existing analytics module.\n\n## Metrics Displayed\n- Active sessions (last 30 days)\n- Total API calls (current billing period)\n- Error rate percentage\n- Storage usage vs quota\n\n## Technical Requirements\n- Use existing AnalyticsService\n- Follow dashboard layout pattern from AdminDashboard\n- Implement real-time updates via WebSocket\n- Cache metrics data for 5 minutes\n</code></pre>\n<p><strong>Phase 2 - Review</strong>: You and the AI refine the spec, clarifying refresh intervals, error states, loading behavior.</p>\n<p><strong>Phase 3 - Implement</strong>: AI generates code that matches the spec exactly, following your existing patterns because they&#39;re documented in the specification.</p>\n<p><strong>Result</strong>: First implementation is 90% correct, requiring only minor tweaks for edge cases you didn&#39;t think of initially.</p>\n<h2>Why This Matters for Experienced Developers</h2>\n<p>Some might think specifications slow down development. In my experience with complex codebases, the opposite is true:</p>\n<p><strong>Faster iteration</strong>: Less code regeneration means faster progress\n<strong>Better architecture</strong>: Forces upfront thinking about system design\n<strong>Maintainable documentation</strong>: Specs serve as living documentation\n<strong>Onboarding efficiency</strong>: New team members (AI or human) understand intent\n<strong>Quality gates</strong>: Easier to review specs than review generated code for missing requirements</p>\n<h2>Getting Started with OpenSpec</h2>\n<p>OpenSpec is available as an open-source project on GitHub at <a href=\"https://github.com/Fission-AI/OpenSpec\">Fission-AI/OpenSpec</a>.</p>\n<p>Installation is straightforward:</p>\n<pre><code class=\"language-bash\">npm install -g openspec\n# or\nyarn global add openspec\n</code></pre>\n<p>Initialize in your project:</p>\n<pre><code class=\"language-bash\">openspec init\n</code></pre>\n<p>This creates the <code>openspec/</code> directory structure and configuration files.</p>\n<p>If you&#39;re using Claude Code or Cursor, the native slash commands are automatically available:</p>\n<pre><code>/openspec-draft &lt;description&gt;\n/openspec-review\n/openspec-implement\n/openspec-archive\n</code></pre>\n<h2>Who Should Use OpenSpec?</h2>\n<p>OpenSpec is particularly valuable for:</p>\n<p><strong>Solo developers using AI assistants</strong>: Maintain architectural consistency across your codebase without the overhead of traditional specification documents.</p>\n<p><strong>Teams with AI-augmented workflows</strong>: Share specifications across team members using different AI tools, ensuring everyone&#39;s AI assistant works from the same source of truth.</p>\n<p><strong>Complex domain implementations</strong>: When building systems with non-trivial business logic, specifications prevent AI from making incorrect assumptions about domain rules.</p>\n<p><strong>Regulated environments</strong>: Generate an audit trail of what was specified vs. what was implemented, useful for compliance and code review processes.</p>\n<h2>The Broader Trend</h2>\n<p>OpenSpec represents a broader shift in how we think about AI-assisted development. The first wave of AI coding tools focused on generating code from prompts. The next wave focuses on <strong>controllable, predictable generation</strong>.</p>\n<p>We&#39;re moving from:</p>\n<ul>\n<li>&quot;Generate code and hope it works&quot;</li>\n<li>To: &quot;Specify behavior, then generate implementation&quot;</li>\n</ul>\n<p>This is analogous to the shift from dynamic to static typing—trading some flexibility for predictability and maintainability.</p>\n<h2>Combining OpenSpec with Other Practices</h2>\n<p>OpenSpec complements other development practices I use:</p>\n<p><strong>Test-Driven Development</strong>: Specs define behavior, tests verify it, AI implements it</p>\n<p><strong>Hexagonal Architecture</strong>: Specs document port contracts clearly</p>\n<p><strong>Domain-Driven Design</strong>: Specs capture ubiquitous language and domain rules</p>\n<p><strong>API-First Development</strong>: OpenSpec naturally extends to API specification</p>\n<h2>Limitations and Considerations</h2>\n<p>OpenSpec isn&#39;t magic. Some considerations:</p>\n<p><strong>Upfront time investment</strong>: Writing specs takes time before coding begins</p>\n<p><strong>Spec maintenance</strong>: Specifications need updates as requirements evolve</p>\n<p><strong>Learning curve</strong>: Teams need to adjust to spec-driven workflow</p>\n<p><strong>Overkill for simple tasks</strong>: One-off scripts don&#39;t need formal specs</p>\n<p>For complex features or systems with multiple contributors (human or AI), the benefits outweigh these costs.</p>\n<h2>The Future of Spec-Driven AI Development</h2>\n<p>Tools like OpenSpec point to where AI-assisted development is heading:</p>\n<ol>\n<li><strong>Explicit contracts</strong>: Clear specifications over implicit prompts</li>\n<li><strong>Reproducible generation</strong>: Same spec = same code</li>\n<li><strong>Multi-agent workflows</strong>: Different AI tools collaborating via shared specs</li>\n<li><strong>Version-controlled intent</strong>: Track why code exists, not just what it does</li>\n</ol>\n<p>As AI coding assistants become more powerful, the constraint isn&#39;t generation capability—it&#39;s <strong>alignment</strong>. OpenSpec provides that alignment mechanism.</p>\n<h2>Conclusion</h2>\n<p>After working with AI coding assistants daily, I&#39;ve learned that the bottleneck isn&#39;t how fast AI generates code—it&#39;s how well that code matches my intent. OpenSpec addresses this by introducing a specification layer between prompt and implementation.</p>\n<p>For developers serious about leveraging AI tools at scale, spec-driven development isn&#39;t optional—it&#39;s essential. OpenSpec provides a lightweight, practical framework for making it happen.</p>\n<p>If you&#39;re using Claude Code, Cursor, or other AI assistants to build anything beyond simple scripts, I recommend checking out OpenSpec. The time invested in specifications pays dividends in code quality and development velocity.</p>\n<hr>\n<p><strong>Resources</strong>:</p>\n<ul>\n<li><a href=\"https://github.com/Fission-AI/OpenSpec\">OpenSpec on GitHub</a> - Open-source framework</li>\n<li><a href=\"https://github.com/Fission-AI/OpenSpec#readme\">OpenSpec Documentation</a> - Getting started guide</li>\n<li><a href=\"https://medium.com/coding-nexus/openspec-a-spec-driven-workflow-for-ai-coding-assistants-no-api-keys-needed-d5b3323294fa\">Spec-Driven Development Principles</a> - Detailed overview</li>\n</ul>\n",
            "feature_image": "https://jasoncochran.io/blog/openspec-ai-driven-specification-workflow/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-11-19 00:00:00",
            "updated_at": "2025-11-19 00:00:00",
            "published_at": "2025-11-19 00:00:00",
            "custom_excerpt": "Discover how OpenSpec brings predictability to AI-assisted development by establishing explicit specifications before code generation, reducing iteration cycles and improving code quality."
          },
          {
            "id": "5",
            "title": "How AI-Assisted Development Helps Me Ship Faster Without Compromising Quality",
            "slug": "ai-assisted-development-ship-faster",
            "html": "<p>After 27 years of building web applications, I&#39;ve witnessed countless technological shifts. But nothing has impacted my productivity quite like AI-assisted development. I&#39;m not talking about letting AI write all my code—I&#39;m talking about a strategic partnership where AI amplifies my expertise rather than replacing it.</p>\n<h2>The Reality Check</h2>\n<p>Let me be direct: <strong>AI doesn&#39;t replace senior engineering judgment</strong>. What it does is eliminate the friction between thinking and implementation. The architectural decisions, system design, and business logic understanding still come from decades of experience. AI just makes the execution dramatically faster.</p>\n<h2>My AI Development Stack</h2>\n<h3>Claude Code: The Heavy Lifter</h3>\n<p>Claude Code has become my primary development partner for complex architectural work. Here&#39;s what makes it invaluable:</p>\n<p><strong>Codebase Understanding</strong>: Claude Code can analyze entire codebases and understand architectural patterns. When I&#39;m working on enterprise applications like the 320K+ LOC Catalyst PSA platform, I can ask it to maintain consistency with existing patterns across dozens of modules.</p>\n<p><strong>Refactoring at Scale</strong>: Recently, I needed to migrate a large application from Prisma to Drizzle ORM. Claude Code handled:</p>\n<ul>\n<li>Analyzing all existing Prisma schema files</li>\n<li>Converting them to Drizzle schema syntax</li>\n<li>Updating 200+ repository methods</li>\n<li>Maintaining type safety across the entire codebase</li>\n</ul>\n<p>What would have taken me 2-3 weeks of careful, methodical work took 3 days with Claude Code.</p>\n<h3>Cursor: The Speed Demon</h3>\n<p>Cursor excels at rapid iteration within files:</p>\n<pre><code class=\"language-typescript\">// I type a comment like this:\n// Create a user service with CRUD operations, validation, and error handling\n\n// Cursor generates something like this:\n@Injectable()\nexport class UserService {\n  constructor(\n    @InjectRepository(User)\n    private readonly userRepository: Repository&lt;User&gt;,\n  ) {}\n\n  async create(createUserDto: CreateUserDto): Promise&lt;User&gt; {\n    // Validate email uniqueness\n    const existingUser = await this.userRepository.findOne({\n      where: { email: createUserDto.email },\n    });\n\n    if (existingUser) {\n      throw new ConflictException(&#39;Email already exists&#39;);\n    }\n\n    const user = this.userRepository.create(createUserDto);\n    return this.userRepository.save(user);\n  }\n\n  async findAll(page = 1, limit = 10): Promise&lt;PaginatedResult&lt;User&gt;&gt; {\n    const [users, total] = await this.userRepository.findAndCount({\n      skip: (page - 1) * limit,\n      take: limit,\n    });\n\n    return {\n      data: users,\n      total,\n      page,\n      lastPage: Math.ceil(total / limit),\n    };\n  }\n\n  // ... more methods\n}\n</code></pre>\n<p>Then I review, refine the business logic, add domain-specific validation, and move on. The boilerplate is done in seconds.</p>\n<h3>GitHub Copilot: The Autocomplete Ninja</h3>\n<p>Copilot shines for:</p>\n<ul>\n<li>TypeScript interface definitions</li>\n<li>Test case generation</li>\n<li>API endpoint boilerplate</li>\n<li>Database query patterns</li>\n</ul>\n<h2>Real Project Impact: WellOS</h2>\n<p>Let me share a concrete example. When building WellOS, an oil &amp; gas field operations platform, I needed to create:</p>\n<ul>\n<li>6 integrated applications (Web, Mobile, Admin, API, Sync, Dashboard)</li>\n<li>Offline-first architecture with sync</li>\n<li>Real-time data collection</li>\n<li>Multi-tenant isolation</li>\n<li>Complete test coverage</li>\n</ul>\n<p><strong>Without AI</strong>: This would have been a 6-8 month project with a small team.</p>\n<p><strong>With AI-assisted development</strong>: I delivered the MVP in 10 weeks as a solo developer.</p>\n<p>Here&#39;s the breakdown:</p>\n<h3>Week 1-2: Architecture &amp; Foundation</h3>\n<ul>\n<li><strong>My role</strong>: Designed the hexagonal architecture, chose the tech stack, planned the database schema</li>\n<li><strong>AI role</strong>: Generated initial NestJS modules, Next.js pages, React Native screens following my architectural patterns</li>\n</ul>\n<h3>Week 3-6: Core Feature Development</h3>\n<ul>\n<li><strong>My role</strong>: Implemented complex business logic (offline sync, conflict resolution, multi-tenant isolation)</li>\n<li><strong>AI role</strong>: Generated CRUD operations, API endpoints, form validations, database migrations</li>\n</ul>\n<h3>Week 7-8: Mobile Development</h3>\n<ul>\n<li><strong>My role</strong>: Architected offline-first sync strategy, designed mobile UX</li>\n<li><strong>AI role</strong>: Generated React Native components, navigation structure, local database setup</li>\n</ul>\n<h3>Week 9-10: Testing &amp; Polish</h3>\n<ul>\n<li><strong>My role</strong>: Integration testing, performance optimization, security review</li>\n<li><strong>AI role</strong>: Generated unit tests, E2E test scenarios, fixed type errors</li>\n</ul>\n<h2>The 80/20 Rule in Practice</h2>\n<p>Here&#39;s what I&#39;ve learned: <strong>AI handles about 80% of the typing, but I make 100% of the decisions</strong>.</p>\n<h3>What AI Does Well:</h3>\n<ul>\n<li>✅ Boilerplate code generation</li>\n<li>✅ Type definitions and interfaces</li>\n<li>✅ Basic CRUD operations</li>\n<li>✅ Test scaffolding</li>\n<li>✅ Documentation generation</li>\n<li>✅ Code refactoring within established patterns</li>\n</ul>\n<h3>What I Still Own:</h3>\n<ul>\n<li>🎯 System architecture decisions</li>\n<li>🎯 Security implementation</li>\n<li>🎯 Performance optimization strategies</li>\n<li>🎯 Complex business logic</li>\n<li>🎯 API design and contracts</li>\n<li>🎯 Database schema design</li>\n<li>🎯 DevOps and deployment strategy</li>\n</ul>\n<h2>Quality Isn&#39;t Sacrificed</h2>\n<p>Some worry that speed means lower quality. In my experience, the opposite is true:</p>\n<p><strong>Consistency</strong>: AI follows patterns religiously. If I establish a pattern for error handling, it applies it consistently across the entire codebase.</p>\n<p><strong>Type Safety</strong>: AI-generated TypeScript code is fully typed. No <code>any</code> types, no shortcuts.</p>\n<p><strong>Testing</strong>: I use AI to generate comprehensive test suites. More time for testing = higher quality.</p>\n<p><strong>Code Review</strong>: I review every line AI generates. My 27 years of experience means I spot issues instantly.</p>\n<h2>The Economics for Startups</h2>\n<p>For startup clients, the value proposition is clear:</p>\n<p><strong>Traditional Development</strong>:</p>\n<ul>\n<li>3-month MVP timeline</li>\n<li>3-person team</li>\n<li>~$150K investment (at market rates)</li>\n</ul>\n<p><strong>AI-Assisted Solo Development</strong>:</p>\n<ul>\n<li>6-8 week MVP timeline</li>\n<li>1 senior developer (me)</li>\n<li>~$60-80K investment</li>\n<li>Higher code quality (single architectural vision)</li>\n<li>Faster iteration cycles</li>\n</ul>\n<h2>Transparency with Clients</h2>\n<p>I&#39;m completely transparent about using AI tools:</p>\n<ol>\n<li>I explain that AI accelerates implementation, not decision-making</li>\n<li>I clarify that my experience guides the AI, not vice versa</li>\n<li>I emphasize that all code is reviewed and tested to enterprise standards</li>\n<li>I highlight that this means faster delivery at better rates</li>\n</ol>\n<p>In 2025, clients care about <strong>outcomes</strong>, not the tools used to achieve them. They want:</p>\n<ul>\n<li>✅ Fast time to market</li>\n<li>✅ High-quality code</li>\n<li>✅ Scalable architecture</li>\n<li>✅ Transparent communication</li>\n</ul>\n<p>AI helps me deliver all of these better than ever.</p>\n<h2>Tips for Developers Considering AI Tools</h2>\n<p>If you&#39;re thinking about incorporating AI into your workflow:</p>\n<ol>\n<li><strong>Start Small</strong>: Begin with code completion (Copilot), then graduate to more advanced tools</li>\n<li><strong>Maintain Standards</strong>: Establish your coding patterns first, then have AI follow them</li>\n<li><strong>Review Everything</strong>: Never blindly accept AI-generated code</li>\n<li><strong>Leverage Experience</strong>: AI amplifies expertise; it doesn&#39;t create it</li>\n<li><strong>Focus on Architecture</strong>: Spend more time on design, less on implementation</li>\n<li><strong>Iterate Faster</strong>: Use the time savings for better testing and refinement</li>\n</ol>\n<h2>The Future Is Hybrid</h2>\n<p>The future of software development isn&#39;t &quot;AI vs Humans&quot;—it&#39;s humans augmented by AI. The developers who thrive will be those who:</p>\n<ul>\n<li>Understand system architecture deeply</li>\n<li>Can guide AI effectively</li>\n<li>Review and refine AI output critically</li>\n<li>Focus on the problems AI can&#39;t solve yet (business logic, UX, performance)</li>\n</ul>\n<p>After 27 years in this industry, I&#39;ve never been more productive or enjoyed development more. AI handles the tedious parts, leaving me free to focus on what I love: solving complex problems and building great software.</p>\n<h2>Conclusion</h2>\n<p>AI-assisted development has transformed me from a traditional software engineer into what I call a &quot;force multiplier developer.&quot; I can deliver enterprise-grade applications at startup speed, maintaining quality while dramatically reducing timelines.</p>\n<p>For startups looking to move fast without breaking things, this is the sweet spot: experienced architectural thinking amplified by cutting-edge tooling.</p>\n<p>Want to see how fast we can ship your MVP? <a href=\"/contact\">Let&#39;s talk</a>.</p>\n<hr>\n<p><strong>Tools Mentioned</strong>:</p>\n<ul>\n<li><a href=\"https://claude.ai/code\">Claude Code</a> - Full-stack AI development assistant</li>\n<li><a href=\"https://cursor.sh\">Cursor</a> - AI-powered code editor</li>\n<li><a href=\"https://github.com/features/copilot\">GitHub Copilot</a> - AI pair programmer</li>\n</ul>\n",
            "feature_image": "https://jasoncochran.io/blog/ai-assisted-development-ship-faster/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-11-13 00:00:00",
            "updated_at": "2025-11-13 00:00:00",
            "published_at": "2025-11-13 00:00:00",
            "custom_excerpt": "After 27 years of software development, AI tools have transformed how I work. Here's how I leverage Claude Code, Cursor, and Copilot to deliver production-ready MVPs at unprecedented speed."
          },
          {
            "id": "6",
            "title": "My 2025 Development Stack: Built for Speed, Designed for Scale",
            "slug": "modern-development-stack-rapid-prototyping",
            "html": "<p>After building 100+ applications over 27 years, I&#39;ve refined my development stack to a sharp edge: <strong>maximum velocity without sacrificing quality</strong>. This isn&#39;t about chasing trends—it&#39;s about using tools that let me ship production-ready MVPs in weeks instead of months.</p>\n<p>Here&#39;s the exact stack I use for rapid prototyping in 2025, and why each tool earned its place.</p>\n<h2>The Foundation: TypeScript Everywhere</h2>\n<p><strong>Why</strong>: Type safety eliminates an entire class of bugs before they reach production. AI tools work better with TypeScript. Refactoring is fearless.</p>\n<p><strong>The Reality</strong>: 27 years ago, I wrote Perl and PHP without types. Today, I won&#39;t touch a project without TypeScript. The upfront investment in types pays dividends every single day.</p>\n<pre><code class=\"language-typescript\">// This catches bugs at compile time, not production\ninterface User {\n  id: string;\n  email: string;\n  role: &#39;admin&#39; | &#39;user&#39; | &#39;guest&#39;;\n}\n\n// TypeScript prevents this mistake:\nconst user: User = {\n  id: &#39;123&#39;,\n  email: &#39;test@example.com&#39;,\n  role: &#39;administrator&#39; // ❌ Error: Type &#39;&quot;administrator&quot;&#39; is not assignable\n}\n</code></pre>\n<h2>Frontend: Next.js 15 (App Router)</h2>\n<p><strong>Why I chose it</strong>:</p>\n<ul>\n<li>Server Components reduce client-side JavaScript</li>\n<li>Built-in API routes for rapid prototyping</li>\n<li>Exceptional developer experience</li>\n<li>SEO and performance out of the box</li>\n<li>Vercel deployment in minutes</li>\n</ul>\n<p><strong>Real-world example</strong>: The Catalyst PSA platform (320K+ LOC) uses Next.js App Router. Features like server-side data fetching and streaming make complex dashboards snappy.</p>\n<pre><code class=\"language-typescript\">// app/dashboard/page.tsx\n// Server Component - data fetching happens on the server\nexport default async function DashboardPage() {\n  // No loading states, no useEffect, just data\n  const projects = await db.query.projects.findMany({\n    where: eq(projects.status, &#39;active&#39;),\n    with: { tasks: true, members: true }\n  });\n\n  return ;\n}\n</code></pre>\n<h3>UI: Tailwind CSS + Shadcn/ui</h3>\n<p><strong>Why</strong>:</p>\n<ul>\n<li>Utility-first CSS = faster styling</li>\n<li>No context switching between files</li>\n<li>AI tools understand Tailwind perfectly</li>\n<li>Shadcn/ui provides beautiful, accessible components</li>\n<li>Copy-paste, customize, own the code</li>\n</ul>\n<p><strong>Speed factor</strong>: Building a complex form with validation used to take hours. Now? 15 minutes.</p>\n<pre><code class=\"language-typescript\">&lt;form className=&quot;space-y-4&quot;&gt;\n  &lt;div&gt;\n    \n    \n  &lt;/div&gt;\n  \n&lt;/form&gt;\n</code></pre>\n<h2>Backend: NestJS</h2>\n<p><strong>Why it&#39;s my go-to</strong>:</p>\n<ul>\n<li>Enterprise architecture patterns out of the box</li>\n<li>Dependency injection makes testing trivial</li>\n<li>Decorator-based routing is clean and intuitive</li>\n<li>Scales from MVP to enterprise</li>\n<li>TypeScript native</li>\n</ul>\n<p><strong>The pattern that saves me weeks</strong>:</p>\n<pre><code class=\"language-typescript\">// users/users.module.ts\n@Module({\n  imports: [TypeOrmModule.forFeature([User])],\n  controllers: [UsersController],\n  providers: [UsersService, UsersRepository],\n  exports: [UsersService], // Clean dependency management\n})\nexport class UsersModule {}\n\n// users/users.controller.ts\n@Controller(&#39;users&#39;)\n@UseGuards(JwtAuthGuard)\nexport class UsersController {\n  constructor(private readonly usersService: UsersService) {}\n\n  @Get()\n  @ApiOperation({ summary: &#39;Get all users&#39; })\n  async findAll(@Query() query: PaginationDto) {\n    return this.usersService.findAll(query);\n  }\n\n  @Post()\n  @ApiOperation({ summary: &#39;Create user&#39; })\n  async create(@Body() createUserDto: CreateUserDto) {\n    return this.usersService.create(createUserDto);\n  }\n}\n</code></pre>\n<p>This structure is so consistent that AI tools can generate entire modules following the pattern. I just review and refine the business logic.</p>\n<h2>Database: PostgreSQL + Drizzle ORM</h2>\n<p><strong>Why I switched from Prisma</strong>:</p>\n<ul>\n<li>Drizzle is lighter and faster</li>\n<li>SQL-like syntax (easier for AI to generate correctly)</li>\n<li>Better TypeScript inference</li>\n<li>Edge-compatible</li>\n</ul>\n<p><strong>Schema definition</strong>:</p>\n<pre><code class=\"language-typescript\">// schema/users.ts\nexport const users = pgTable(&#39;users&#39;, {\n  id: uuid(&#39;id&#39;).defaultRandom().primaryKey(),\n  email: varchar(&#39;email&#39;, { length: 255 }).notNull().unique(),\n  name: varchar(&#39;name&#39;, { length: 255 }).notNull(),\n  role: varchar(&#39;role&#39;, { length: 50 }).notNull(),\n  createdAt: timestamp(&#39;created_at&#39;).defaultNow().notNull(),\n});\n\n// Type inference - automatically generated!\nexport type User = typeof users.$inferSelect;\nexport type NewUser = typeof users.$inferInsert;\n</code></pre>\n<p><strong>Migration generation</strong>: <code>drizzle-kit generate</code> creates SQL migrations automatically. No guesswork.</p>\n<h2>State Management: Zustand + React Query</h2>\n<p><strong>Why this combo wins</strong>:</p>\n<p><strong>Zustand</strong> for global UI state:</p>\n<pre><code class=\"language-typescript\">// stores/auth-store.ts\nexport const useAuthStore = create\n  );\n}\n</code></pre>\n<h2>AI Development Tools</h2>\n<h3>1. Claude Code</h3>\n<p><strong>Use case</strong>: Architecture, large refactors, codebase analysis</p>\n<p><strong>Real example</strong>: &quot;Migrate all authentication logic from session-based to JWT while maintaining backward compatibility.&quot;</p>\n<p>Claude Code analyzes the codebase, generates a migration plan, and executes it across 50+ files.</p>\n<h3>2. Cursor</h3>\n<p><strong>Use case</strong>: Rapid feature development, inline code generation</p>\n<p><strong>Real example</strong>: Typing a comment like <code>// Create invoice PDF generation service with templates</code> generates a complete service with error handling, validation, and types.</p>\n<h3>3. GitHub Copilot</h3>\n<p><strong>Use case</strong>: Autocomplete, test generation, repetitive patterns</p>\n<p><strong>Real example</strong>: Writing one test case, Copilot suggests 10 more covering edge cases.</p>\n<h2>DevOps &amp; Deployment</h2>\n<h3>Vercel (Frontend)</h3>\n<ul>\n<li>Push to main → instant deployment</li>\n<li>Preview deployments for every PR</li>\n<li>Edge functions for global performance</li>\n<li>Zero configuration</li>\n</ul>\n<h3>Railway (Backend &amp; Database)</h3>\n<ul>\n<li>NestJS API deploys in minutes</li>\n<li>PostgreSQL with automatic backups</li>\n<li>Environment variables management</li>\n<li>$5/month to start</li>\n</ul>\n<h3>Alternative: AWS (Enterprise)</h3>\n<p>For larger projects:</p>\n<ul>\n<li>ECS for containerized apps</li>\n<li>RDS for databases</li>\n<li>CloudFront + S3 for static assets</li>\n<li>Route 53 for DNS</li>\n</ul>\n<h2>Monitoring &amp; Analytics</h2>\n<ul>\n<li><strong>Sentry</strong>: Error tracking and performance monitoring</li>\n<li><strong>LogRocket</strong>: Session replay for debugging user issues</li>\n<li><strong>Vercel Analytics</strong>: Web vitals and performance metrics</li>\n<li><strong>Axiom</strong>: Structured logging</li>\n</ul>\n<h2>The Monorepo Structure</h2>\n<p><strong>Tool</strong>: Turborepo</p>\n<pre><code>my-app/\n├── apps/\n│   ├── web/          # Next.js app\n│   ├── api/          # NestJS backend\n│   └── mobile/       # React Native app\n├── packages/\n│   ├── ui/           # Shared components\n│   ├── db/           # Database schemas &amp; migrations\n│   ├── types/        # Shared TypeScript types\n│   └── config/       # ESLint, TypeScript configs\n└── turbo.json        # Build orchestration\n</code></pre>\n<p><strong>Benefits</strong>:</p>\n<ul>\n<li>Share code across apps</li>\n<li>Single command builds everything</li>\n<li>Consistent tooling and configs</li>\n<li>AI tools understand the structure</li>\n</ul>\n<h2>Testing Stack</h2>\n<p><strong>Unit &amp; Integration</strong>: Vitest</p>\n<ul>\n<li>Faster than Jest</li>\n<li>Better TypeScript support</li>\n<li>Compatible with modern tools</li>\n</ul>\n<p><strong>E2E</strong>: Playwright</p>\n<ul>\n<li>Cross-browser testing</li>\n<li>Visual regression testing</li>\n<li>Generated with AI assistance</li>\n</ul>\n<pre><code class=\"language-typescript\">// tests/auth.spec.ts\ntest(&#39;user can login&#39;, async ({ page }) =&gt; {\n  await page.goto(&#39;/login&#39;);\n  await page.fill(&#39;[name=email]&#39;, &#39;test@example.com&#39;);\n  await page.fill(&#39;[name=password]&#39;, &#39;password123&#39;);\n  await page.click(&#39;button[type=submit]&#39;);\n\n  await expect(page).toHaveURL(&#39;/dashboard&#39;);\n});\n</code></pre>\n<h2>Documentation</h2>\n<ul>\n<li><strong>Storybook</strong>: Component documentation</li>\n<li><strong>OpenAPI/Swagger</strong>: Auto-generated API docs from NestJS</li>\n<li><strong>Notion</strong>: Project specs and architecture decisions</li>\n</ul>\n<h2>Version Control &amp; CI/CD</h2>\n<ul>\n<li><strong>GitHub</strong>: Version control</li>\n<li><strong>GitHub Actions</strong>: CI/CD pipelines</li>\n<li><strong>Conventional Commits</strong>: Automated changelog generation</li>\n</ul>\n<pre><code class=\"language-yaml\"># .github/workflows/main.yml\nname: CI\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-node@v3\n      - run: npm install\n      - run: npm run lint\n      - run: npm run test\n      - run: npm run build\n</code></pre>\n<h2>The Complete Stack Summary</h2>\n<table>\n<thead>\n<tr>\n<th>Layer</th>\n<th>Tool</th>\n<th>Why</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Frontend</td>\n<td>Next.js 15</td>\n<td>SSR, App Router, Vercel integration</td>\n</tr>\n<tr>\n<td>Backend</td>\n<td>NestJS</td>\n<td>Enterprise patterns, DI, scalability</td>\n</tr>\n<tr>\n<td>Database</td>\n<td>PostgreSQL + Drizzle</td>\n<td>Reliability, type safety, performance</td>\n</tr>\n<tr>\n<td>Mobile</td>\n<td>React Native + Expo</td>\n<td>Cross-platform, code sharing</td>\n</tr>\n<tr>\n<td>UI</td>\n<td>Tailwind + Shadcn/ui</td>\n<td>Speed, consistency, accessibility</td>\n</tr>\n<tr>\n<td>State</td>\n<td>Zustand + React Query</td>\n<td>Simple, powerful, performant</td>\n</tr>\n<tr>\n<td>AI Coding</td>\n<td>Claude Code, Cursor, Copilot</td>\n<td>3-5x development speed</td>\n</tr>\n<tr>\n<td>Deployment</td>\n<td>Vercel + Railway</td>\n<td>Zero-config, instant deploys</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Sentry + LogRocket</td>\n<td>Catch issues before users complain</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Vitest + Playwright</td>\n<td>Fast, reliable, AI-friendly</td>\n</tr>\n</tbody></table>\n<h2>ROI: What This Stack Delivers</h2>\n<p><strong>Traditional stack (2018)</strong>:</p>\n<ul>\n<li>3 months to MVP</li>\n<li>Complex configuration</li>\n<li>Slow feedback loops</li>\n<li>Manual deployment processes</li>\n</ul>\n<p><strong>My 2025 stack</strong>:</p>\n<ul>\n<li>✅ 6-8 weeks to MVP</li>\n<li>✅ Zero-config defaults</li>\n<li>✅ Instant preview deployments</li>\n<li>✅ AI-accelerated development</li>\n<li>✅ End-to-end type safety</li>\n<li>✅ Enterprise-grade from day one</li>\n</ul>\n<h2>Not Just Tools—Patterns</h2>\n<p>Here&#39;s the secret: <strong>The tools matter less than the patterns</strong>.</p>\n<p>I&#39;ve established patterns for:</p>\n<ul>\n<li>Authentication &amp; authorization</li>\n<li>Error handling</li>\n<li>Validation</li>\n<li>Database access</li>\n<li>API design</li>\n<li>Testing strategies</li>\n</ul>\n<p>Once these patterns are established, AI tools can replicate them across the entire codebase. That&#39;s where the speed comes from.</p>\n<h2>For Startups: The Bottom Line</h2>\n<p>Using this stack, I can deliver:</p>\n<ul>\n<li><strong>Week 1</strong>: Project setup, authentication, basic CRUD</li>\n<li><strong>Week 2-3</strong>: Core features, database design</li>\n<li><strong>Week 4-5</strong>: Advanced features, integrations</li>\n<li><strong>Week 6</strong>: Testing, polish, deployment</li>\n<li><strong>Week 7-8</strong>: Iteration based on feedback</li>\n</ul>\n<p>This is production-ready code, not a prototype that needs to be rewritten.</p>\n<h2>Want to Move This Fast?</h2>\n<p>This stack isn&#39;t theoretical—it&#39;s what I use daily to ship real applications for real clients. The combination of 27 years of architectural experience and cutting-edge AI tooling is a force multiplier.</p>\n<p>If you&#39;re a startup looking to validate your idea quickly or an established company needing to move faster, <a href=\"/contact\">let&#39;s talk about your project</a>.</p>\n<hr>\n<p><strong>Complete Tool List</strong>:</p>\n<p><strong>Development</strong>:</p>\n<ul>\n<li>TypeScript</li>\n<li>Next.js 15</li>\n<li>NestJS</li>\n<li>React Native + Expo</li>\n<li>Drizzle ORM + PostgreSQL</li>\n</ul>\n<p><strong>AI Tools</strong>:</p>\n<ul>\n<li>Claude Code</li>\n<li>Cursor</li>\n<li>GitHub Copilot</li>\n</ul>\n<p><strong>Deployment</strong>:</p>\n<ul>\n<li>Vercel</li>\n<li>Railway</li>\n<li>AWS (enterprise)</li>\n</ul>\n<p><strong>Monitoring</strong>:</p>\n<ul>\n<li>Sentry</li>\n<li>LogRocket</li>\n<li>Vercel Analytics</li>\n</ul>\n<p><strong>Testing</strong>:</p>\n<ul>\n<li>Vitest</li>\n<li>Playwright</li>\n<li>Storybook</li>\n</ul>\n",
            "feature_image": "https://jasoncochran.io/blog/modern-development-stack-rapid-prototyping/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-11-13 00:00:00",
            "updated_at": "2025-11-13 00:00:00",
            "published_at": "2025-11-13 00:00:00",
            "custom_excerpt": "The exact tools and frameworks I use to ship enterprise-grade MVPs in weeks. From Next.js to NestJS, from Claude Code to Cursor—here's my battle-tested rapid prototyping stack."
          },
          {
            "id": "7",
            "title": "Why Permian Basin Operators Need Modern SCADA Systems",
            "slug": "permian-basin-modern-scada-systems",
            "html": "<p>After working with multiple oil &amp; gas companies in the Permian Basin—from Key Energy Services to Big D Companies to building WellOS from scratch—I&#39;ve seen firsthand the operational pain caused by outdated legacy systems.</p>\n<p>Let&#39;s talk about why 2025 is the year Permian Basin operators should modernize their SCADA and field data systems.</p>\n<h2>The Problem: Legacy Systems Built for a Different Era</h2>\n<p>Most Permian Basin operators are running systems built 10-20 years ago:</p>\n<ul>\n<li><strong>Excel spreadsheets</strong> for production tracking (error-prone, delayed entry)</li>\n<li><strong>Paper forms</strong> in the field (data entry happens 24-48 hours later)</li>\n<li><strong>Expensive vendor SCADA</strong> with high licensing costs and vendor lock-in</li>\n<li><strong>No mobile access</strong> for field operators</li>\n<li><strong>Manual compliance reporting</strong> consuming hours of staff time monthly</li>\n<li><strong>5-10 disconnected systems</strong> that don&#39;t talk to each other</li>\n</ul>\n<h3>The Real Cost</h3>\n<p>This isn&#39;t just inconvenient—it&#39;s expensive:</p>\n<ul>\n<li><strong>Data entry errors</strong> leading to incorrect production reports</li>\n<li><strong>Delayed visibility</strong> into equipment failures (costly downtime)</li>\n<li><strong>Regulatory compliance risk</strong> from manual reporting errors</li>\n<li><strong>Operator frustration</strong> with clunky, outdated interfaces</li>\n<li><strong>Lost productivity</strong> from fragmented systems</li>\n</ul>\n<h2>What Modern SCADA Systems Deliver</h2>\n<h3>1. Real-Time Monitoring with Sub-Second Latency</h3>\n<p>Modern systems use <strong>OPC UA</strong>, <strong>Modbus TCP</strong>, and <strong>MQTT</strong> protocols to deliver:</p>\n<pre><code class=\"language-rust\">// Example: OPC UA subscription in Rust\nasync fn subscribe_to_tags(client: &amp;Client) -&gt; Result&lt;()&gt; {\n    let subscription = client\n        .create_subscription(Duration::from_millis(500))\n        .await?;\n\n    subscription\n        .monitor_items(&amp;[&quot;PumpSpeed&quot;, &quot;TankLevel&quot;, &quot;Pressure&quot;])\n        .await?;\n\n    Ok(())\n}\n</code></pre>\n<p><strong>Results</strong>:</p>\n<ul>\n<li>&lt;1 second data latency from field to dashboard</li>\n<li>Instant alarm notifications via Microsoft Teams</li>\n<li>Heat maps showing production across all wells</li>\n<li>Historical trends with TimescaleDB (20x faster than vanilla PostgreSQL)</li>\n</ul>\n<h3>2. Offline-First Field Data Capture</h3>\n<p>Here&#39;s the reality: <strong>Cellular connectivity in the Permian Basin is unreliable</strong>.</p>\n<p>Modern systems are built offline-first using <strong>event sourcing</strong>:</p>\n<pre><code class=\"language-typescript\">// Offline production entry\nasync function saveProductionEntry(entry: ProductionEntry) {\n  // Save locally first\n  await db.productionEntries.add(entry);\n\n  // Queue for sync when online\n  await syncQueue.add({\n    type: &#39;PRODUCTION_ENTRY_CREATED&#39;,\n    payload: entry,\n    timestamp: Date.now()\n  });\n\n  // Sync automatically when connection available\n  if (navigator.onLine) {\n    await syncQueue.process();\n  }\n}\n</code></pre>\n<p><strong>Benefits</strong>:</p>\n<ul>\n<li>Field operators work 100% offline</li>\n<li>Production data captured on-site, not hours later</li>\n<li>Photo documentation with GPS tagging</li>\n<li>Automatic sync when connectivity returns</li>\n<li>No lost data, no duplicate entries</li>\n</ul>\n<h3>3. Automated Texas RRC Compliance</h3>\n<p>Manual compliance reporting is time-consuming and error-prone.</p>\n<p>Modern systems <strong>automate RRC reporting</strong>:</p>\n<pre><code class=\"language-typescript\">// Automated W-10 report generation\nasync function generateW10Report(wellId: string, month: string) {\n  const production = await getMonthlyProduction(wellId, month);\n  const disposal = await getDisposalData(wellId, month);\n\n  return {\n    operator: production.operator,\n    lease: production.lease,\n    wellNumber: production.apiNumber,\n    oil: production.oilBarrels,\n    gas: production.gasMcf,\n    water: production.waterBarrels,\n    disposal: {\n      volume: disposal.volumeBarrels,\n      disposalWell: disposal.disposalWellId,\n    },\n    // Automated calculations\n    gasOilRatio: production.gasMcf / production.oilBarrels,\n    waterCut: production.waterBarrels / production.totalLiquid,\n  };\n}\n</code></pre>\n<p><strong>Time savings</strong>:</p>\n<ul>\n<li>W-10, G-1, G-5 reports generated in &lt;30 seconds</li>\n<li>Emissions calculations (flaring, venting, methane) automated</li>\n<li>Complete audit trail for regulatory inspections</li>\n<li>Export directly to RRC submission portal</li>\n</ul>\n<h3>4. Predictive Maintenance with Machine Learning</h3>\n<p>Instead of reactive maintenance, modern systems predict failures <strong>7-30 days in advance</strong>:</p>\n<pre><code class=\"language-python\"># ML-powered equipment failure prediction\ndef predict_pump_failure(well_data: WellData) -&gt; Prediction:\n    features = extract_features(well_data)\n\n    # Trained model detects anomalies\n    failure_probability = model.predict_proba(features)\n\n    if failure_probability &gt; 0.75:\n        days_until_failure = estimate_time_to_failure(well_data)\n        return Prediction(\n            risk=&quot;HIGH&quot;,\n            days_until_failure=days_until_failure,\n            recommended_action=&quot;Schedule preventive maintenance&quot;\n        )\n</code></pre>\n<p><strong>ROI</strong>:</p>\n<ul>\n<li>Reduce unplanned downtime by 40%</li>\n<li>Optimize maintenance schedules</li>\n<li>Extend equipment lifespan</li>\n<li>Lower repair costs through early intervention</li>\n</ul>\n<h2>The Technology Stack That Powers This</h2>\n<p>Building WellOS taught me what works for Permian Basin operations:</p>\n<h3>Backend: Rust + Axum</h3>\n<p><strong>Why Rust over Node.js/NestJS?</strong></p>\n<ul>\n<li><strong>10x faster</strong> API response times</li>\n<li><strong>Memory safety</strong> without garbage collection</li>\n<li>Perfect for SCADA real-time processing</li>\n<li>Compile-time guarantees prevent production bugs</li>\n</ul>\n<pre><code class=\"language-rust\">#[derive(Debug, Serialize)]\nstruct ScadaReading {\n    tag: String,\n    value: f64,\n    timestamp: DateTime&lt;Utc&gt;,\n    quality: QualityCode,\n}\n\nasync fn handle_scada_data(\n    State(db): State&lt;DatabasePool&gt;,\n    Json(reading): Json&lt;ScadaReading&gt;\n) -&gt; Result&lt;StatusCode, AppError&gt; {\n    // Insert with circuit breaker pattern\n    db.insert_scada_reading(&amp;reading)\n        .circuit_breaker()\n        .await?;\n\n    Ok(StatusCode::CREATED)\n}\n</code></pre>\n<h3>Time-Series Database: TimescaleDB</h3>\n<p><strong>Why TimescaleDB?</strong></p>\n<ul>\n<li><strong>20x faster</strong> time-series ingestion vs PostgreSQL</li>\n<li>Automatic data compression (12x storage savings)</li>\n<li>Continuous aggregates for real-time dashboards</li>\n<li>Perfect for production and SCADA data</li>\n</ul>\n<h3>Mobile: React Native + Expo</h3>\n<p><strong>Offline-first with event sourcing</strong>:</p>\n<pre><code class=\"language-typescript\">// Offline-capable mobile app\nconst ProductionEntryScreen = () =&gt; {\n  const [syncStatus, setSyncStatus] = useState&lt;SyncStatus&gt;(&#39;synced&#39;);\n\n  const saveEntry = async (entry: ProductionEntry) =&gt; {\n    // Works offline\n    await localDb.save(entry);\n    setSyncStatus(&#39;pending&#39;);\n\n    // Auto-sync when online\n    NetInfo.addEventListener(state =&gt; {\n      if (state.isConnected) {\n        syncToCloud();\n      }\n    });\n  };\n};\n</code></pre>\n<h2>Real-World Impact: Big D Companies Case Study</h2>\n<p>When I worked with Big D Companies (Mar 2024 - Sep 2024), I modernized their legacy PHP SCADA system:</p>\n<p><strong>Before</strong>:</p>\n<ul>\n<li>PHP-based system from early 2000s</li>\n<li>No mobile access</li>\n<li>24-hour data lag</li>\n<li>Manual MySQL queries for reports</li>\n</ul>\n<p><strong>After (React/Next.js modernization)</strong>:</p>\n<ul>\n<li>Real-time web dashboard</li>\n<li>Mobile-responsive</li>\n<li>Sub-second data updates</li>\n<li>Automated report generation</li>\n</ul>\n<p><strong>Results</strong>:</p>\n<ul>\n<li>70% reduction in data entry time</li>\n<li>90% decrease in data errors</li>\n<li>Real-time visibility (vs 24-hour lag)</li>\n<li>Operators actually enjoyed using the system</li>\n</ul>\n<h2>Building WellOS: Lessons from the Permian Basin</h2>\n<p>WellOS is my answer to the fragmented, outdated systems plaguing Permian Basin operators.</p>\n<h3>The &quot;Operating System&quot; Concept</h3>\n<p>Just like Windows or macOS, WellOS provides a <strong>unified platform</strong> for all operations:</p>\n<ul>\n<li><strong>Real-time SCADA</strong> monitoring (&lt;1s latency)</li>\n<li><strong>Offline mobile</strong> apps for field operators</li>\n<li><strong>Production tracking</strong> with automated decline curve analysis</li>\n<li><strong>Compliance automation</strong> (RRC, EPA reporting)</li>\n<li><strong>Predictive maintenance</strong> (ML-powered failure prediction)</li>\n<li><strong>QuickBooks integration</strong> (automated invoicing)</li>\n</ul>\n<h3>Six Integrated Applications</h3>\n<ol>\n<li><strong>Rust API Backend</strong>: Axum + PostgreSQL + TimescaleDB</li>\n<li><strong>Next.js Web Dashboard</strong>: Real-time charts, heat maps</li>\n<li><strong>React Native Mobile</strong>: Offline production entry</li>\n<li><strong>Electron Desktop</strong>: Rugged laptop support</li>\n<li><strong>Admin Portal</strong>: Multi-tenant management</li>\n<li><strong>ML Service</strong>: Python FastAPI for predictions</li>\n</ol>\n<h3>Key Metrics</h3>\n<ul>\n<li><strong>&lt;1 second</strong> SCADA data latency</li>\n<li><strong>100% functional</strong> offline</li>\n<li><strong>&lt;30 seconds</strong> compliance report generation</li>\n<li><strong>7-30 days</strong> advance equipment failure warnings</li>\n<li><strong>$50/well/month</strong> pricing (vs $500+ for legacy systems)</li>\n</ul>\n<h2>Why Permian Basin Operators Need This Now</h2>\n<h3>1. Regulatory Pressure Increasing</h3>\n<p>Texas RRC and EPA are <strong>tightening compliance requirements</strong>:</p>\n<ul>\n<li>More frequent reporting</li>\n<li>Stricter emissions tracking</li>\n<li>Higher fines for violations</li>\n</ul>\n<p><strong>Manual processes won&#39;t scale</strong>.</p>\n<h3>2. Labor Shortage</h3>\n<p>Permian Basin faces <strong>significant operator turnover</strong>:</p>\n<ul>\n<li>Younger workers expect modern mobile apps</li>\n<li>Paper forms drive frustration</li>\n<li>Delayed data entry causes overtime</li>\n</ul>\n<p><strong>Modern UX improves retention</strong>.</p>\n<h3>3. Thin Margins Require Efficiency</h3>\n<p>With volatile oil prices, <strong>every dollar matters</strong>:</p>\n<ul>\n<li>Predictive maintenance reduces costly downtime</li>\n<li>Automated compliance saves staff hours</li>\n<li>Real-time data enables faster decisions</li>\n</ul>\n<h3>4. Technology Debt Compounds</h3>\n<p>Legacy systems get <strong>harder to maintain</strong> over time:</p>\n<ul>\n<li>Vendor support ends</li>\n<li>Integration costs increase</li>\n<li>Security vulnerabilities grow</li>\n</ul>\n<p><strong>Modernize now before migration becomes impossible</strong>.</p>\n<h2>How to Modernize: Phased Approach</h2>\n<p>You don&#39;t have to replace everything at once. Here&#39;s a <strong>proven migration path</strong>:</p>\n<h3>Phase 1: Real-Time Monitoring (Weeks 1-4)</h3>\n<ul>\n<li>Integrate SCADA protocols (OPC UA/Modbus/MQTT)</li>\n<li>Build basic real-time dashboard</li>\n<li>Set up alarm notifications</li>\n<li><strong>Quick win</strong>: Instant visibility</li>\n</ul>\n<h3>Phase 2: Mobile Field Data (Weeks 5-8)</h3>\n<ul>\n<li>Deploy offline-capable mobile apps</li>\n<li>Production entry, photos, notes</li>\n<li>Automatic sync to cloud</li>\n<li><strong>Quick win</strong>: Eliminate paper forms</li>\n</ul>\n<h3>Phase 3: Compliance Automation (Weeks 9-12)</h3>\n<ul>\n<li>Automated RRC report generation</li>\n<li>Emissions calculations</li>\n<li>Export to submission portals</li>\n<li><strong>Quick win</strong>: Save 10+ hours/month</li>\n</ul>\n<h3>Phase 4: Advanced Features (Weeks 13+)</h3>\n<ul>\n<li>ML-powered predictive maintenance</li>\n<li>Decline curve forecasting</li>\n<li>QuickBooks integration</li>\n<li>Heat maps and advanced analytics</li>\n</ul>\n<h2>The ROI is Clear</h2>\n<p><strong>Investment</strong>:</p>\n<ul>\n<li>6-12 weeks development time</li>\n<li>Modern SaaS pricing ($50-200/well/month)</li>\n<li>Training and onboarding</li>\n</ul>\n<p><strong>Returns</strong>:</p>\n<ul>\n<li><strong>70% reduction</strong> in data entry time</li>\n<li><strong>90% fewer</strong> data entry errors</li>\n<li><strong>40% less</strong> unplanned downtime</li>\n<li><strong>10+ hours/month</strong> saved on compliance</li>\n<li><strong>Better operator retention</strong> through modern UX</li>\n</ul>\n<h3>Payback Period: 6-12 Months</h3>\n<p>For a 50-well operator:</p>\n<ul>\n<li>Monthly savings: $5,000-10,000 (labor + downtime reduction)</li>\n<li>Monthly cost: $2,500-10,000 (SaaS + development)</li>\n<li><strong>Breakeven in 6-12 months, then pure profit</strong></li>\n</ul>\n<h2>Getting Started</h2>\n<p>If you&#39;re a Permian Basin operator frustrated with legacy systems, here&#39;s how I can help:</p>\n<h3>Consulting Services</h3>\n<ul>\n<li><strong>SCADA Integration</strong>: OPC UA, Modbus TCP, MQTT</li>\n<li><strong>Mobile Field Apps</strong>: Offline-first React Native</li>\n<li><strong>Compliance Automation</strong>: RRC and EPA reporting</li>\n<li><strong>Legacy Modernization</strong>: PHP/ASP.NET → React/Next.js</li>\n<li><strong>Custom Development</strong>: Tailored to your operations</li>\n</ul>\n<h3>WellOS Platform</h3>\n<ul>\n<li><strong>Full SaaS solution</strong> launching January 2026</li>\n<li><strong>Database-per-tenant</strong> isolation for security</li>\n<li><strong>$50/well/month</strong> starter pricing</li>\n<li><strong>Free trial</strong> for Permian Basin operators</li>\n</ul>\n<h2>Conclusion</h2>\n<p>The Permian Basin deserves better than Excel spreadsheets and 20-year-old SCADA systems.</p>\n<p>Modern technology—<strong>real-time monitoring, offline mobile apps, automated compliance, ML-powered insights</strong>—is no longer optional. It&#39;s essential for staying competitive.</p>\n<p>I&#39;ve built these systems for Key Energy, Big D Companies, and now WellOS. I know what works in the field and what doesn&#39;t.</p>\n<p><strong>If you&#39;re ready to modernize your operations</strong>, let&#39;s talk.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/permian-basin-modern-scada-systems/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-11-13 00:00:00",
            "updated_at": "2025-11-13 00:00:00",
            "published_at": "2025-11-13 00:00:00",
            "custom_excerpt": "The case for modernizing oil & gas SCADA systems in the Permian Basin. How real-time monitoring, offline field data, and AI-powered insights are transforming upstream operations."
          },
          {
            "id": "8",
            "title": "Getting Started with Next.js 14",
            "slug": "getting-started-with-nextjs",
            "html": "<h1>Getting Started with Next.js 14</h1>\n<p>After 27 years of building web applications, I&#39;ve seen frameworks come and go. Next.js, however, has fundamentally changed how I approach building modern web applications. The introduction of the App Router in Next.js 13+ represents one of the most significant paradigm shifts I&#39;ve experienced since transitioning from server-side rendering to single-page applications over a decade ago.</p>\n<p>Having built this personal website and numerous production applications with Next.js, I want to share practical insights that go beyond the basics and help you avoid common pitfalls.</p>\n<h2>Why Next.js?</h2>\n<p>Next.js has become my go-to framework for several compelling reasons, backed by real production experience:</p>\n<ul>\n<li><strong>Server Components</strong>: Render components on the server for better performance and reduced JavaScript bundle sizes</li>\n<li><strong>File-based Routing</strong>: Intuitive routing based on your file structure—no more route configuration files</li>\n<li><strong>Built-in Optimization</strong>: Automatic image, font, and script optimization that actually works</li>\n<li><strong>TypeScript Support</strong>: First-class TypeScript integration with excellent type inference</li>\n<li><strong>Developer Experience</strong>: Fast refresh, helpful error messages, and a great development workflow</li>\n</ul>\n<p>The App Router, introduced in Next.js 13, has fundamentally changed the framework&#39;s architecture, moving from a purely client-centric model to a more balanced approach that leverages both server and client capabilities.</p>\n<h2>Setting Up Your First Project</h2>\n<p>Getting started is straightforward:</p>\n<pre><code class=\"language-bash\">npx create-next-app@latest my-app\n</code></pre>\n<p>During setup, you&#39;ll be prompted with several options. Here&#39;s what I recommend for most projects:</p>\n<ul>\n<li><strong>TypeScript</strong>: Yes (always)</li>\n<li><strong>ESLint</strong>: Yes</li>\n<li><strong>Tailwind CSS</strong>: Yes (unless you have strong opinions about CSS-in-JS)</li>\n<li><strong>App Router</strong>: Yes (the future of Next.js)</li>\n<li><strong>Import alias</strong>: Yes (@/*)</li>\n</ul>\n<p>This creates a solid foundation with TypeScript, ESLint, and the modern App Router structure.</p>\n<h2>Understanding the App Router Architecture</h2>\n<p>The App Router represents a fundamental shift in how Next.js applications are structured. Unlike the Pages Router, which was purely client-side focused, the App Router embraces a hybrid model.</p>\n<h3>Server Components vs Client Components</h3>\n<p>This is perhaps the most important concept to understand. By default, all components in the <code>app</code> directory are Server Components. This means they:</p>\n<ul>\n<li>Run only on the server</li>\n<li>Can directly access backend resources (databases, file systems)</li>\n<li>Don&#39;t add to the client JavaScript bundle</li>\n<li>Can&#39;t use browser APIs or React hooks like <code>useState</code> or <code>useEffect</code></li>\n</ul>\n<p>Here&#39;s a practical example from a blog listing page:</p>\n<pre><code class=\"language-tsx\">// app/blog/page.tsx - Server Component (default)\nimport { getPosts } from &#39;@/lib/posts&#39;\n\nexport default async function BlogPage() {\n  // This runs on the server - can directly access the database\n  const posts = await getPosts()\n\n  return (\n    &lt;div className=&quot;grid gap-8&quot;&gt;\n      {posts.map(post =&gt; (\n        &lt;article key={post.id}&gt;\n          &lt;h2&gt;{post.title}&lt;/h2&gt;\n          &lt;p&gt;{post.description}&lt;/p&gt;\n          &lt;time&gt;{post.date}&lt;/time&gt;\n        &lt;/article&gt;\n      ))}\n    &lt;/div&gt;\n  )\n}\n</code></pre>\n<p>Client Components, marked with <code>&#39;use client&#39;</code>, are needed when you require:</p>\n<ul>\n<li>Interactive features (onClick, onChange, etc.)</li>\n<li>React hooks (useState, useEffect, useContext)</li>\n<li>Browser APIs (localStorage, window, etc.)</li>\n<li>Third-party libraries that depend on browser APIs</li>\n</ul>\n<pre><code class=\"language-tsx\">// app/components/theme-toggle.tsx - Client Component\n&#39;use client&#39;\n\nimport { useState, useEffect } from &#39;react&#39;\n\nexport function ThemeToggle() {\n  const [theme, setTheme] = useState&lt;&#39;light&#39; | &#39;dark&#39;&gt;(&#39;light&#39;)\n\n  useEffect(() =&gt; {\n    const stored = localStorage.getItem(&#39;theme&#39;)\n    if (stored) setTheme(stored as &#39;light&#39; | &#39;dark&#39;)\n  }, [])\n\n  const toggleTheme = () =&gt; {\n    const newTheme = theme === &#39;light&#39; ? &#39;dark&#39; : &#39;light&#39;\n    setTheme(newTheme)\n    localStorage.setItem(&#39;theme&#39;, newTheme)\n    document.documentElement.classList.toggle(&#39;dark&#39;)\n  }\n\n  return (\n    &lt;button onClick={toggleTheme} className=&quot;px-4 py-2 rounded&quot;&gt;\n      {theme === &#39;light&#39; ? &#39;🌙&#39; : &#39;☀️&#39;}\n    &lt;/button&gt;\n  )\n}\n</code></pre>\n<p><strong>Common Pitfall</strong>: Don&#39;t make everything a Client Component just because you&#39;re used to it. Start with Server Components and only add <code>&#39;use client&#39;</code> when you need interactivity. This keeps your bundle size small and improves performance.</p>\n<h2>Data Fetching Patterns</h2>\n<p>Next.js 14 introduces several powerful data fetching patterns that I&#39;ve found incredibly useful in production.</p>\n<h3>Server Component Data Fetching</h3>\n<p>Server Components can be async and fetch data directly:</p>\n<pre><code class=\"language-tsx\">// app/projects/[id]/page.tsx\nimport { getProject } from &#39;@/lib/projects&#39;\nimport { notFound } from &#39;next/navigation&#39;\n\nexport default async function ProjectPage({\n  params,\n}: {\n  params: { id: string }\n}) {\n  const project = await getProject(params.id)\n\n  if (!project) {\n    notFound() // Shows 404 page\n  }\n\n  return (\n    &lt;div&gt;\n      &lt;h1&gt;{project.title}&lt;/h1&gt;\n      &lt;p&gt;{project.description}&lt;/p&gt;\n    &lt;/div&gt;\n  )\n}\n</code></pre>\n<h3>Parallel Data Fetching</h3>\n<p>One powerful pattern is fetching multiple data sources in parallel:</p>\n<pre><code class=\"language-tsx\">// app/dashboard/page.tsx\nexport default async function DashboardPage() {\n  // These fetch in parallel\n  const [user, stats, activities] = await Promise.all([\n    getUser(),\n    getStats(),\n    getRecentActivities(),\n  ])\n\n  return (\n    &lt;div className=&quot;space-y-8&quot;&gt;\n      \n      \n      \n    &lt;/div&gt;\n  )\n}\n</code></pre>\n<h3>Streaming with Suspense</h3>\n<p>For slower data fetching, you can stream content to the client:</p>\n<pre><code class=\"language-tsx\">// app/blog/page.tsx\nimport { Suspense } from &#39;react&#39;\n\nfunction PostList() {\n  // This might be slow\n  const posts = await getPosts()\n  return &lt;div&gt;{/* render posts */}&lt;/div&gt;\n}\n\nexport default function BlogPage() {\n  return (\n    &lt;div&gt;\n      &lt;h1&gt;Blog&lt;/h1&gt;\n      \n    &lt;/div&gt;\n  )\n}\n</code></pre>\n<h2>Routing Deep Dive</h2>\n<p>The file-based routing in Next.js is intuitive once you understand the conventions.</p>\n<h3>Dynamic Routes</h3>\n<p>Create dynamic segments using square brackets:</p>\n<pre><code>app/\n  blog/\n    [slug]/\n      page.tsx  # /blog/my-post\n  projects/\n    [id]/\n      page.tsx  # /projects/123\n</code></pre>\n<pre><code class=\"language-tsx\">// app/blog/[slug]/page.tsx\nexport default async function BlogPost({\n  params,\n}: {\n  params: { slug: string }\n}) {\n  const post = await getPostBySlug(params.slug)\n  return &lt;article&gt;{/* render post */}&lt;/article&gt;\n}\n\n// Generate static paths at build time\nexport async function generateStaticParams() {\n  const posts = await getAllPosts()\n  return posts.map(post =&gt; ({ slug: post.slug }))\n}\n</code></pre>\n<h3>Route Groups</h3>\n<p>Route groups let you organize routes without affecting the URL structure:</p>\n<pre><code>app/\n  (marketing)/\n    about/\n      page.tsx    # /about\n    contact/\n      page.tsx    # /contact\n  (app)/\n    dashboard/\n      page.tsx    # /dashboard\n    settings/\n      page.tsx    # /settings\n</code></pre>\n<p>Each group can have its own layout:</p>\n<pre><code class=\"language-tsx\">// app/(marketing)/layout.tsx\nexport default function MarketingLayout({\n  children,\n}: {\n  children: React.ReactNode\n}) {\n  return (\n    &lt;&gt;\n      \n      {children}\n      \n    &lt;/&gt;\n  )\n}\n</code></pre>\n<h3>Parallel Routes</h3>\n<p>This advanced pattern lets you render multiple pages in the same layout:</p>\n<pre><code>app/\n  dashboard/\n    @analytics/\n      page.tsx\n    @team/\n      page.tsx\n    layout.tsx\n    page.tsx\n</code></pre>\n<pre><code class=\"language-tsx\">// app/dashboard/layout.tsx\nexport default function DashboardLayout({\n  children,\n  analytics,\n  team,\n}: {\n  children: React.ReactNode\n  analytics: React.ReactNode\n  team: React.ReactNode\n}) {\n  return (\n    &lt;div className=&quot;grid grid-cols-2 gap-4&quot;&gt;\n      &lt;div className=&quot;col-span-2&quot;&gt;{children}&lt;/div&gt;\n      &lt;div&gt;{analytics}&lt;/div&gt;\n      &lt;div&gt;{team}&lt;/div&gt;\n    &lt;/div&gt;\n  )\n}\n</code></pre>\n<h2>Loading States and Error Handling</h2>\n<p>Next.js provides built-in conventions for loading and error states.</p>\n<h3>Loading UI</h3>\n<p>Create a <code>loading.tsx</code> file to show while the page loads:</p>\n<pre><code class=\"language-tsx\">// app/blog/loading.tsx\nexport default function Loading() {\n  return (\n    &lt;div className=&quot;space-y-4&quot;&gt;\n      &lt;div className=&quot;h-8 bg-gray-200 rounded animate-pulse&quot; /&gt;\n      &lt;div className=&quot;h-4 bg-gray-200 rounded animate-pulse w-3/4&quot; /&gt;\n      &lt;div className=&quot;h-4 bg-gray-200 rounded animate-pulse w-1/2&quot; /&gt;\n    &lt;/div&gt;\n  )\n}\n</code></pre>\n<h3>Error Boundaries</h3>\n<p>Create an <code>error.tsx</code> file to handle errors gracefully:</p>\n<pre><code class=\"language-tsx\">// app/blog/error.tsx\n&#39;use client&#39; // Error components must be Client Components\n\nimport { useEffect } from &#39;react&#39;\n\nexport default function Error({\n  error,\n  reset,\n}: {\n  error: Error &amp; { digest?: string }\n  reset: () =&gt; void\n}) {\n  useEffect(() =&gt; {\n    // Log to error reporting service\n    console.error(error)\n  }, [error])\n\n  return (\n    &lt;div className=&quot;text-center py-12&quot;&gt;\n      &lt;h2 className=&quot;text-2xl font-bold mb-4&quot;&gt;Something went wrong!&lt;/h2&gt;\n      &lt;button\n        onClick={reset}\n        className=&quot;px-4 py-2 bg-blue-500 text-white rounded&quot;\n      &gt;\n        Try again\n      &lt;/button&gt;\n    &lt;/div&gt;\n  )\n}\n</code></pre>\n<h2>Metadata and SEO</h2>\n<p>Next.js 14 provides excellent SEO support through the Metadata API.</p>\n<h3>Static Metadata</h3>\n<pre><code class=\"language-tsx\">// app/about/page.tsx\nimport { Metadata } from &#39;next&#39;\n\nexport const metadata: Metadata = {\n  title: &#39;About Jason Cochran&#39;,\n  description: &#39;Learn about my 27 years of experience in software development&#39;,\n  openGraph: {\n    title: &#39;About Jason Cochran&#39;,\n    description: &#39;Learn about my 27 years of experience in software development&#39;,\n    images: [&#39;/og-image.jpg&#39;],\n  },\n}\n\nexport default function AboutPage() {\n  return &lt;div&gt;{/* content */}&lt;/div&gt;\n}\n</code></pre>\n<h3>Dynamic Metadata</h3>\n<p>For dynamic pages, use <code>generateMetadata</code>:</p>\n<pre><code class=\"language-tsx\">// app/blog/[slug]/page.tsx\nexport async function generateMetadata({\n  params,\n}: {\n  params: { slug: string }\n}): Promise&lt;Metadata&gt; {\n  const post = await getPostBySlug(params.slug)\n\n  return {\n    title: post.title,\n    description: post.description,\n    openGraph: {\n      title: post.title,\n      description: post.description,\n      type: &#39;article&#39;,\n      publishedTime: post.date,\n      authors: [&#39;Jason Cochran&#39;],\n    },\n  }\n}\n</code></pre>\n<h2>Performance Optimization</h2>\n<h3>Image Optimization</h3>\n<p>Next.js&#39;s Image component is fantastic for performance:</p>\n<pre><code class=\"language-tsx\">import Image from &#39;next/image&#39;\n\nexport function ProjectCard({ project }) {\n  return (\n    &lt;div&gt;\n      \n      &lt;h3&gt;{project.title}&lt;/h3&gt;\n    &lt;/div&gt;\n  )\n}\n</code></pre>\n<h3>Font Optimization</h3>\n<p>Use <code>next/font</code> for optimized font loading:</p>\n<pre><code class=\"language-tsx\">// app/layout.tsx\nimport { Inter, JetBrains_Mono } from &#39;next/font/google&#39;\n\nconst inter = Inter({\n  subsets: [&#39;latin&#39;],\n  variable: &#39;--font-inter&#39;,\n})\n\nconst jetbrainsMono = JetBrains_Mono({\n  subsets: [&#39;latin&#39;],\n  variable: &#39;--font-mono&#39;,\n})\n\nexport default function RootLayout({ children }) {\n  return (\n    &lt;html lang=&quot;en&quot; className={`${inter.variable} ${jetbrainsMono.variable}`}&gt;\n      &lt;body&gt;{children}&lt;/body&gt;\n    &lt;/html&gt;\n  )\n}\n</code></pre>\n<h3>Static Generation</h3>\n<p>Use <code>generateStaticParams</code> to pre-render pages at build time:</p>\n<pre><code class=\"language-tsx\">export async function generateStaticParams() {\n  const posts = await getAllPosts()\n\n  return posts.map(post =&gt; ({\n    slug: post.slug,\n  }))\n}\n</code></pre>\n<h2>Real-World Example: Building This Website</h2>\n<p>When building my personal website with Next.js 14, I used these patterns:</p>\n<ol>\n<li><strong>Server Components</strong> for all static pages (about, projects)</li>\n<li><strong>Client Components</strong> only for interactive features (theme toggle, contact form)</li>\n<li><strong>Dynamic routes</strong> for blog posts and project details</li>\n<li><strong>Route groups</strong> to separate marketing and app sections</li>\n<li><strong>Metadata API</strong> for comprehensive SEO</li>\n<li><strong>Static generation</strong> for all blog posts at build time</li>\n</ol>\n<p>This resulted in:</p>\n<ul>\n<li>Excellent Lighthouse scores (95+ across all metrics)</li>\n<li>Fast Time to First Byte (TTFB)</li>\n<li>Small JavaScript bundle size</li>\n<li>Great SEO performance</li>\n</ul>\n<h2>Common Pitfalls to Avoid</h2>\n<ol>\n<li><strong>Making everything a Client Component</strong>: Start with Server Components and only use Client Components when needed</li>\n<li><strong>Not using Suspense boundaries</strong>: This causes entire pages to wait for slow data</li>\n<li><strong>Ignoring loading states</strong>: Users need feedback while content loads</li>\n<li><strong>Not leveraging static generation</strong>: Pre-render what you can at build time</li>\n<li><strong>Overusing client-side data fetching</strong>: Fetch on the server when possible</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Next.js 14 with the App Router represents a mature, production-ready framework that fundamentally improves how we build web applications. The combination of Server Components, streaming, and excellent developer experience makes it my framework of choice for new projects.</p>\n<p>The key is understanding when to use Server Components versus Client Components, leveraging static generation where possible, and providing excellent loading and error states. Master these patterns, and you&#39;ll build fast, maintainable applications that scale.</p>\n<p>After 27 years of web development, I can confidently say that Next.js is one of the most significant advances in the React ecosystem. Start with these patterns, and you&#39;ll be well on your way to building production-grade applications.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/getting-started-with-nextjs/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-10-14 00:00:00",
            "updated_at": "2025-10-14 00:00:00",
            "published_at": "2025-10-14 00:00:00",
            "custom_excerpt": "Practical Next.js 14 insights from 27 years of building web applications. Learn Server Components, App Router patterns, and production-tested strategies beyond the basics."
          },
          {
            "id": "9",
            "title": "TypeScript Best Practices for 2025",
            "slug": "typescript-best-practices",
            "html": "<h1>TypeScript Best Practices for 2025</h1>\n<p>In the decade since I started using TypeScript, it has evolved from a niche Microsoft project to the industry standard for building robust JavaScript applications. Working on enterprise codebases like Catalyst PSA (320K+ lines) and WellOS, I&#39;ve learned that TypeScript&#39;s value goes far beyond catching typos—it&#39;s about encoding business logic and domain knowledge directly into your type system.</p>\n<p>Here are the patterns and practices that have proven most valuable across years of production TypeScript development.</p>\n<h2>Enable Strict Mode (No Exceptions)</h2>\n<p>This isn&#39;t optional. Strict mode is the foundation of TypeScript&#39;s type safety guarantees.</p>\n<pre><code class=\"language-json\">{\n  &quot;compilerOptions&quot;: {\n    &quot;strict&quot;: true,\n    &quot;noUncheckedIndexedAccess&quot;: true,\n    &quot;noImplicitOverride&quot;: true,\n    &quot;exactOptionalPropertyTypes&quot;: true\n  }\n}\n</code></pre>\n<p>These settings enable:</p>\n<ul>\n<li><strong>strict</strong>: All strict type-checking options</li>\n<li><strong>noUncheckedIndexedAccess</strong>: Array/object index access returns <code>T | undefined</code></li>\n<li><strong>noImplicitOverride</strong>: Requires explicit <code>override</code> keyword</li>\n<li><strong>exactOptionalPropertyTypes</strong>: Prevents assigning <code>undefined</code> to optional properties</li>\n</ul>\n<p>When I inherited codebases without strict mode, the first step was always enabling it. Yes, it creates hundreds of errors initially, but fixing them reveals real bugs.</p>\n<h2>Avoid <code>any</code> Like the Plague</h2>\n<p>Every <code>any</code> is a hole in your type safety. Here&#39;s what to use instead:</p>\n<h3>Use <code>unknown</code> for Truly Unknown Data</h3>\n<pre><code class=\"language-typescript\">// Bad: any defeats type checking\nfunction parseJSON(json: string): any {\n  return JSON.parse(json)\n}\n\nconst data = parseJSON(&#39;{&quot;name&quot;: &quot;Jason&quot;}&#39;)\nconsole.log(data.name.toUpperCase()) // No type checking - runtime error if structure is wrong\n\n// Good: unknown requires type checking\nfunction parseJSON(json: string): unknown {\n  return JSON.parse(json)\n}\n\nconst data = parseJSON(&#39;{&quot;name&quot;: &quot;Jason&quot;}&#39;)\n\n// TypeScript forces you to validate\nif (isUserData(data)) {\n  console.log(data.name.toUpperCase()) // Safe!\n}\n\n// Type guard\nfunction isUserData(data: unknown): data is { name: string } {\n  return (\n    typeof data === &#39;object&#39; &amp;&amp;\n    data !== null &amp;&amp;\n    &#39;name&#39; in data &amp;&amp;\n    typeof data.name === &#39;string&#39;\n  )\n}\n</code></pre>\n<h3>Use Generics for Reusable Code</h3>\n<pre><code class=\"language-typescript\">// Generic API response handler\ninterface APIResponse&lt;T&gt; {\n  data: T\n  status: number\n  message: string\n}\n\nasync function fetchAPI&lt;T&gt;(url: string): Promise&lt;APIResponse&lt;T&gt;&gt; {\n  const response = await fetch(url)\n  return response.json()\n}\n\n// Type-safe usage\ninterface User {\n  id: string\n  name: string\n  email: string\n}\n\nconst userResponse = await fetchAPI&lt;User&gt;(&#39;/api/user/123&#39;)\n// userResponse.data is typed as User\nconsole.log(userResponse.data.name)\n</code></pre>\n<h2>Advanced Type Patterns</h2>\n<h3>Discriminated Unions for State Management</h3>\n<p>This pattern has saved me countless hours debugging state-related bugs:</p>\n<pre><code class=\"language-typescript\">// Bad: Optional properties lead to invalid states\ninterface LoadingState {\n  status: &#39;loading&#39; | &#39;success&#39; | &#39;error&#39;\n  data?: User\n  error?: Error\n}\n\n// This is a valid state, but makes no sense!\nconst badState: LoadingState = {\n  status: &#39;loading&#39;,\n  data: user,\n  error: new Error()\n}\n\n// Good: Discriminated union makes invalid states unrepresentable\ntype RequestState =\n  | { status: &#39;idle&#39; }\n  | { status: &#39;loading&#39; }\n  | { status: &#39;success&#39;; data: User }\n  | { status: &#39;error&#39;; error: Error }\n\nfunction handleState(state: RequestState) {\n  switch (state.status) {\n    case &#39;idle&#39;:\n      return &lt;div&gt;Click to load&lt;/div&gt;\n\n    case &#39;loading&#39;:\n      return \n\n    case &#39;success&#39;:\n      // TypeScript knows state.data exists here\n      return \n\n    case &#39;error&#39;:\n      // TypeScript knows state.error exists here\n      return \n  }\n}\n</code></pre>\n<p>In Catalyst PSA, we use this pattern extensively for async operations, form states, and API calls. It eliminates entire classes of bugs.</p>\n<h3>Conditional Types for Advanced Patterns</h3>\n<pre><code class=\"language-typescript\">// Extract the return type of a promise\ntype Awaited&lt;T&gt; = T extends Promise&lt;infer U&gt; ? U : T\n\ntype UserPromise = Promise&lt;User&gt;\ntype UserType = Awaited&lt;UserPromise&gt; // User\n\n// Make specific properties required\ntype RequireFields&lt;T, K extends keyof T&gt; = T &amp; Required&lt;Pick&lt;T, K&gt;&gt;\n\ninterface PartialUser {\n  id?: string\n  name?: string\n  email?: string\n}\n\n// Require id and email\ntype ValidUser = RequireFields&lt;PartialUser, &#39;id&#39; | &#39;email&#39;&gt;\n// Result: { id: string; email: string; name?: string }\n\n// Extract function parameters\ntype Parameters&lt;T extends (...args: any) =&gt; any&gt; = T extends (...args: infer P) =&gt; any ? P : never\n\nfunction createUser(name: string, email: string, age: number) {\n  // ...\n}\n\ntype CreateUserParams = Parameters&lt;typeof createUser&gt;\n// [string, string, number]\n</code></pre>\n<h3>Mapped Types for Transformations</h3>\n<pre><code class=\"language-typescript\">// Make all properties readonly\ntype Readonly&lt;T&gt; = {\n  readonly [P in keyof T]: T[P]\n}\n\n// Make all properties optional\ntype Partial&lt;T&gt; = {\n  [P in keyof T]?: T[P]\n}\n\n// Create a type with only specific properties\ntype Pick&lt;T, K extends keyof T&gt; = {\n  [P in K]: T[P]\n}\n\n// Custom: Create update types (all fields optional except id)\ntype UpdateType&lt;T extends { id: string }&gt; = Partial&lt;T&gt; &amp; { id: string }\n\ninterface Project {\n  id: string\n  title: string\n  description: string\n  startDate: Date\n}\n\ntype ProjectUpdate = UpdateType&lt;Project&gt;\n// Result: { id: string; title?: string; description?: string; startDate?: Date }\n\n// Real usage from Catalyst PSA\nasync function updateProject(update: ProjectUpdate) {\n  // id is required, everything else is optional\n  const { id, ...changes } = update\n  return await db.projects.update(id, changes)\n}\n</code></pre>\n<h2>Domain Modeling with Types</h2>\n<p>TypeScript shines when you model your business domain:</p>\n<pre><code class=\"language-typescript\">// Instead of primitive obsession\ntype UserId = string\ntype ProjectId = string\ntype Timestamp = number\n\n// Create branded types for type safety\ntype Brand&lt;K, T&gt; = K &amp; { __brand: T }\n\ntype UserId = Brand&lt;string, &#39;UserId&#39;&gt;\ntype ProjectId = Brand&lt;string, &#39;ProjectId&#39;&gt;\n\nfunction createUserId(id: string): UserId {\n  return id as UserId\n}\n\nfunction createProjectId(id: string): ProjectId {\n  return id as ProjectId\n}\n\n// Now these can&#39;t be mixed up\nfunction assignUserToProject(userId: UserId, projectId: ProjectId) {\n  // ...\n}\n\nconst userId = createUserId(&#39;user-123&#39;)\nconst projectId = createProjectId(&#39;proj-456&#39;)\n\nassignUserToProject(userId, projectId) // OK\nassignUserToProject(projectId, userId) // Type error!\n\n// Model business rules with types\ntype EmailAddress = Brand&lt;string, &#39;EmailAddress&#39;&gt;\n\nfunction createEmail(email: string): EmailAddress | null {\n  const emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/\n  return emailRegex.test(email) ? (email as EmailAddress) : null\n}\n\n// Email is guaranteed to be valid\nfunction sendEmail(to: EmailAddress, subject: string, body: string) {\n  // ...\n}\n</code></pre>\n<h2>Type-Safe API Calls</h2>\n<p>One of the most valuable patterns from enterprise work:</p>\n<pre><code class=\"language-typescript\">// Define API schema\ninterface APISchema {\n  &#39;/users&#39;: {\n    GET: {\n      response: User[]\n    }\n    POST: {\n      request: { name: string; email: string }\n      response: User\n    }\n  }\n  &#39;/users/:id&#39;: {\n    GET: {\n      response: User\n    }\n    PUT: {\n      request: Partial&lt;User&gt;\n      response: User\n    }\n    DELETE: {\n      response: { success: boolean }\n    }\n  }\n  &#39;/projects&#39;: {\n    GET: {\n      params: { status?: &#39;active&#39; | &#39;completed&#39; }\n      response: Project[]\n    }\n  }\n}\n\n// Type-safe API client\nclass APIClient {\n  async get&lt;\n    Path extends keyof APISchema,\n    Method extends keyof APISchema[Path] = &#39;GET&#39;\n  &gt;(\n    path: Path,\n    ...args: &#39;params&#39; extends keyof APISchema[Path][Method]\n      ? [params: APISchema[Path][Method][&#39;params&#39;]]\n      : []\n  ): Promise&lt;APISchema[Path][Method][&#39;response&#39;]&gt; {\n    const [params] = args\n    const url = new URL(path as string, this.baseURL)\n\n    if (params) {\n      Object.entries(params).forEach(([key, value]) =&gt; {\n        url.searchParams.set(key, String(value))\n      })\n    }\n\n    const response = await fetch(url.toString())\n    return response.json()\n  }\n\n  async post&lt;\n    Path extends keyof APISchema,\n    Method extends &#39;POST&#39; = &#39;POST&#39;\n  &gt;(\n    path: Path,\n    data: APISchema[Path][Method][&#39;request&#39;]\n  ): Promise&lt;APISchema[Path][Method][&#39;response&#39;]&gt; {\n    const response = await fetch(path as string, {\n      method: &#39;POST&#39;,\n      headers: { &#39;Content-Type&#39;: &#39;application/json&#39; },\n      body: JSON.stringify(data),\n    })\n    return response.json()\n  }\n}\n\n// Usage is completely type-safe\nconst api = new APIClient()\n\n// TypeScript knows the return type\nconst users = await api.get(&#39;/users&#39;) // User[]\n\n// TypeScript enforces correct request body\nconst newUser = await api.post(&#39;/users&#39;, {\n  name: &#39;Jason&#39;,\n  email: &#39;jason@example.com&#39;\n}) // User\n\n// TypeScript requires params when needed\nconst activeProjects = await api.get(&#39;/projects&#39;, {\n  status: &#39;active&#39;\n}) // Project[]\n\n// TypeScript catches errors\nawait api.get(&#39;/users&#39;, { invalid: true }) // Type error!\n</code></pre>\n<h2>TypeScript in React: Common Patterns and Pitfalls</h2>\n<h3>Props with Generics</h3>\n<pre><code class=\"language-typescript\">// Generic list component\ninterface ListProps&lt;T&gt; {\n  items: T[]\n  renderItem: (item: T) =&gt; React.ReactNode\n  keyExtractor: (item: T) =&gt; string\n  emptyMessage?: string\n}\n\nfunction List&lt;T&gt;({ items, renderItem, keyExtractor, emptyMessage }: ListProps&lt;T&gt;) {\n  if (items.length === 0) {\n    return &lt;div&gt;{emptyMessage ?? &#39;No items&#39;}&lt;/div&gt;\n  }\n\n  return (\n    &lt;div&gt;\n      {items.map(item =&gt; (\n        &lt;div key={keyExtractor(item)}&gt;\n          {renderItem(item)}\n        &lt;/div&gt;\n      ))}\n    &lt;/div&gt;\n  )\n}\n\n// Usage\n&lt;List\n  items={projects}\n  renderItem={project =&gt; }\n  keyExtractor={project =&gt; project.id}\n/&gt;\n</code></pre>\n<h3>Event Handlers</h3>\n<pre><code class=\"language-typescript\">// Don&#39;t use any for event types\nfunction handleChange(e: any) { // Bad\n  console.log(e.target.value)\n}\n\n// Use specific event types\nfunction handleInputChange(e: React.ChangeEvent&lt;HTMLInputElement&gt;) {\n  console.log(e.target.value)\n}\n\nfunction handleFormSubmit(e: React.FormEvent&lt;HTMLFormElement&gt;) {\n  e.preventDefault()\n  // ...\n}\n\nfunction handleButtonClick(e: React.MouseEvent&lt;HTMLButtonElement&gt;) {\n  // ...\n}\n\n// For ref callbacks\nfunction handleRef(node: HTMLDivElement | null) {\n  if (node) {\n    // node is HTMLDivElement\n  }\n}\n</code></pre>\n<h3>Children Types</h3>\n<pre><code class=\"language-typescript\">// Use React.ReactNode for children\ninterface CardProps {\n  children: React.ReactNode // Accepts anything renderable\n  title: string\n}\n\n// Use React.ReactElement when you need a specific component\ninterface TabsProps {\n  children: React.ReactElement&lt;TabProps&gt; | React.ReactElement&lt;TabProps&gt;[]\n}\n\n// Use function type for render props\ninterface RenderPropProps&lt;T&gt; {\n  data: T\n  children: (data: T) =&gt; React.ReactNode\n}\n</code></pre>\n<h2>Utility Types in Practice</h2>\n<p>TypeScript&#39;s built-in utility types are incredibly powerful:</p>\n<pre><code class=\"language-typescript\">interface User {\n  id: string\n  name: string\n  email: string\n  age: number\n  role: &#39;admin&#39; | &#39;user&#39;\n}\n\n// Pick - Select specific properties\ntype UserPreview = Pick&lt;User, &#39;id&#39; | &#39;name&#39;&gt;\n// { id: string; name: string }\n\n// Omit - Exclude specific properties\ntype UserWithoutEmail = Omit&lt;User, &#39;email&#39;&gt;\n// { id: string; name: string; age: number; role: &#39;admin&#39; | &#39;user&#39; }\n\n// Partial - Make all properties optional\ntype PartialUser = Partial&lt;User&gt;\n// { id?: string; name?: string; email?: string; age?: number; role?: &#39;admin&#39; | &#39;user&#39; }\n\n// Required - Make all properties required\ntype RequiredUser = Required&lt;PartialUser&gt;\n// Back to original User type\n\n// Record - Create object type with specific keys\ntype UserRoles = Record&lt;&#39;admin&#39; | &#39;user&#39; | &#39;guest&#39;, string[]&gt;\n// { admin: string[]; user: string[]; guest: string[] }\n\n// ReturnType - Extract return type\nfunction getUser() {\n  return { id: &#39;1&#39;, name: &#39;Jason&#39;, email: &#39;jason@example.com&#39; }\n}\n\ntype UserType = ReturnType&lt;typeof getUser&gt;\n// { id: string; name: string; email: string }\n\n// Awaited - Unwrap Promise type\ntype AsyncUser = Awaited&lt;Promise&lt;User&gt;&gt;\n// User\n</code></pre>\n<h2>Type Inference Strategies</h2>\n<p>Let TypeScript do the work when possible:</p>\n<pre><code class=\"language-typescript\">// Let TypeScript infer simple types\nconst name = &#39;Jason&#39; // string\nconst age = 38 // number\nconst active = true // boolean\n\n// Let TypeScript infer array types\nconst numbers = [1, 2, 3] // number[]\nconst mixed = [1, &#39;two&#39;, true] // (string | number | boolean)[]\n\n// Let TypeScript infer return types\nfunction getFullName(first: string, last: string) {\n  return `${first} ${last}` // Returns string (inferred)\n}\n\n// Use const assertions for literal types\nconst config = {\n  api: &#39;https://api.example.com&#39;,\n  timeout: 5000\n} as const\n\n// config.api is &#39;https://api.example.com&#39;, not string\n// config is readonly\n\n// Infer generic types from usage\nfunction identity&lt;T&gt;(value: T): T {\n  return value\n}\n\nconst num = identity(42) // T inferred as number\nconst str = identity(&#39;hello&#39;) // T inferred as string\n\n// Type parameter inference with constraints\nfunction getProperty&lt;T, K extends keyof T&gt;(obj: T, key: K): T[K] {\n  return obj[key]\n}\n\nconst user = { id: &#39;1&#39;, name: &#39;Jason&#39; }\nconst name = getProperty(user, &#39;name&#39;) // string (inferred)\nconst id = getProperty(user, &#39;id&#39;) // string (inferred)\n</code></pre>\n<h2>Common TypeScript Pitfalls and Solutions</h2>\n<h3>Pitfall 1: Index Signature Gotchas</h3>\n<pre><code class=\"language-typescript\">// Problem: Object.keys returns string[], not (keyof T)[]\nfunction printValues&lt;T extends object&gt;(obj: T) {\n  Object.keys(obj).forEach(key =&gt; {\n    // Type error: key is string, not keyof T\n    console.log(obj[key])\n  })\n}\n\n// Solution: Type assertion with runtime safety\nfunction printValues&lt;T extends object&gt;(obj: T) {\n  (Object.keys(obj) as Array&lt;keyof T&gt;).forEach(key =&gt; {\n    console.log(obj[key]) // OK\n  })\n}\n\n// Better: Use a helper\nfunction typedKeys&lt;T extends object&gt;(obj: T): Array&lt;keyof T&gt; {\n  return Object.keys(obj) as Array&lt;keyof T&gt;\n}\n</code></pre>\n<h3>Pitfall 2: Async Function Return Types</h3>\n<pre><code class=\"language-typescript\">// Problem: Forgetting async functions always return Promise\nasync function getUser(id: string): User { // Type error!\n  return await fetchUser(id)\n}\n\n// Solution: Return Promise&lt;T&gt;\nasync function getUser(id: string): Promise&lt;User&gt; {\n  return await fetchUser(id)\n}\n\n// Or let TypeScript infer it\nasync function getUser(id: string) {\n  return await fetchUser(id) // Promise&lt;User&gt; inferred\n}\n</code></pre>\n<h3>Pitfall 3: Type Guards with typeof</h3>\n<pre><code class=\"language-typescript\">// Problem: typeof doesn&#39;t work for null\nfunction processValue(value: string | null) {\n  if (typeof value === &#39;string&#39;) {\n    // value is string, but also could be null!\n  }\n}\n\n// Solution: Check for null explicitly\nfunction processValue(value: string | null) {\n  if (value !== null &amp;&amp; typeof value === &#39;string&#39;) {\n    // Now value is definitely string\n  }\n}\n\n// Or use truthiness for simpler cases\nfunction processValue(value: string | null) {\n  if (value) {\n    // value is string (null is falsy)\n  }\n}\n</code></pre>\n<h2>Real-World Example: Type-Safe Form Builder</h2>\n<p>Here&#39;s a pattern I use extensively in enterprise applications:</p>\n<pre><code class=\"language-typescript\">// Define form schema with types\ninterface FormSchema {\n  username: string\n  email: string\n  age: number\n  newsletter: boolean\n}\n\n// Form field configuration\ntype FieldConfig&lt;T&gt; = {\n  [K in keyof T]: {\n    label: string\n    type: &#39;text&#39; | &#39;email&#39; | &#39;number&#39; | &#39;checkbox&#39;\n    required: boolean\n    validate?: (value: T[K]) =&gt; string | undefined\n  }\n}\n\n// Type-safe form builder\nclass FormBuilder&lt;T extends Record&lt;string, any&gt;&gt; {\n  constructor(private config: FieldConfig&lt;T&gt;) {}\n\n  validate(values: T): Partial&lt;Record&lt;keyof T, string&gt;&gt; {\n    const errors: Partial&lt;Record&lt;keyof T, string&gt;&gt; = {}\n\n    for (const key in this.config) {\n      const field = this.config[key]\n      const value = values[key]\n\n      if (field.required &amp;&amp; !value) {\n        errors[key] = `${field.label} is required`\n      }\n\n      if (field.validate) {\n        const error = field.validate(value)\n        if (error) errors[key] = error\n      }\n    }\n\n    return errors\n  }\n\n  getInitialValues(): T {\n    const values = {} as T\n\n    for (const key in this.config) {\n      const field = this.config[key]\n      values[key] = field.type === &#39;checkbox&#39; ? false : &#39;&#39; as any\n    }\n\n    return values\n  }\n}\n\n// Usage\nconst userForm = new FormBuilder&lt;FormSchema&gt;({\n  username: {\n    label: &#39;Username&#39;,\n    type: &#39;text&#39;,\n    required: true,\n    validate: (value) =&gt; value.length &lt; 3 ? &#39;Too short&#39; : undefined\n  },\n  email: {\n    label: &#39;Email&#39;,\n    type: &#39;email&#39;,\n    required: true,\n    validate: (value) =&gt; !value.includes(&#39;@&#39;) ? &#39;Invalid email&#39; : undefined\n  },\n  age: {\n    label: &#39;Age&#39;,\n    type: &#39;number&#39;,\n    required: false,\n    validate: (value) =&gt; value &lt; 18 ? &#39;Must be 18+&#39; : undefined\n  },\n  newsletter: {\n    label: &#39;Subscribe to newsletter&#39;,\n    type: &#39;checkbox&#39;,\n    required: false,\n  }\n})\n\nconst errors = userForm.validate({\n  username: &#39;jc&#39;,\n  email: &#39;invalid&#39;,\n  age: 16,\n  newsletter: false\n})\n\n// errors is type-safe: Partial&lt;Record&lt;keyof FormSchema, string&gt;&gt;\n</code></pre>\n<h2>Conclusion</h2>\n<p>TypeScript&#39;s true power emerges when you move beyond basic type annotations and start encoding your domain knowledge, business rules, and invariants directly into the type system. After years of building enterprise applications, I&#39;ve found that investing time in proper type design pays dividends in reduced bugs, better developer experience, and more maintainable code.</p>\n<p>Key takeaways:</p>\n<ul>\n<li><strong>Enable strict mode always</strong> - It&#39;s the foundation of type safety</li>\n<li><strong>Avoid <code>any</code></strong> - Use <code>unknown</code>, generics, or proper types instead</li>\n<li><strong>Use discriminated unions</strong> - Make invalid states unrepresentable</li>\n<li><strong>Model your domain</strong> - Branded types and type constraints encode business rules</li>\n<li><strong>Leverage type inference</strong> - Let TypeScript do the work when possible</li>\n<li><strong>Use utility types</strong> - They&#39;re built-in for a reason</li>\n<li><strong>Type your APIs</strong> - End-to-end type safety prevents entire classes of bugs</li>\n</ul>\n<p>After 27 years of development and a decade with TypeScript, I can confidently say it&#39;s one of the most impactful tools for building maintainable JavaScript applications. These patterns have proven themselves across hundreds of thousands of lines of production code—use them wisely, and your codebase will thank you.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/typescript-best-practices/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-10-09 00:00:00",
            "updated_at": "2025-10-09 00:00:00",
            "published_at": "2025-10-09 00:00:00",
            "custom_excerpt": "TypeScript patterns proven across 320,000+ lines of production code in Catalyst PSA and WellOS. What actually matters after 27 years of building enterprise applications."
          },
          {
            "id": "10",
            "title": "Building Scalable React Applications",
            "slug": "building-scalable-react-apps",
            "html": "<h1>Building Scalable React Applications</h1>\n<p>Over the past decade, I&#39;ve built React applications ranging from small marketing sites to enterprise platforms. The difference between a codebase that scales gracefully and one that becomes unmaintainable often comes down to early architectural decisions.</p>\n<p>When I started building Catalyst PSA—a comprehensive professional services automation platform—I had to make critical decisions about architecture that would support a growing team and an expanding feature set. Here&#39;s what I learned from building large-scale React applications that actually work in production.</p>\n<h2>Component Architecture Principles</h2>\n<p>The foundation of a scalable React application is a well-designed component architecture. This goes far beyond just &quot;breaking things into small components.&quot;</p>\n<h3>1. The Single Responsibility Principle</h3>\n<p>Each component should have one clear purpose. When a component starts doing too much, it&#39;s time to break it down.</p>\n<p>Here&#39;s an example from a real project management system:</p>\n<pre><code class=\"language-tsx\">// Bad: Too many responsibilities\nfunction ProjectDashboard() {\n  const [projects, setProjects] = useState([])\n  const [filters, setFilters] = useState({})\n  const [user, setUser] = useState(null)\n  const [notifications, setNotifications] = useState([])\n\n  useEffect(() =&gt; {\n    // Fetch projects\n    // Fetch user\n    // Fetch notifications\n    // Apply filters\n    // Set up WebSocket\n  }, [])\n\n  return (\n    &lt;div&gt;\n      {/* Hundreds of lines of JSX */}\n    &lt;/div&gt;\n  )\n}\n\n// Good: Single responsibilities\nfunction ProjectDashboard() {\n  return (\n    \n  )\n}\n</code></pre>\n<h3>2. Component Composition Patterns</h3>\n<p>Composition is React&#39;s superpower. Here&#39;s a pattern I use extensively in enterprise applications:</p>\n<pre><code class=\"language-tsx\">// Container/Presenter pattern for complex features\n// Container handles data and business logic\nfunction ProjectListContainer() {\n  const { projects, loading, error } = useProjects()\n  const { filters, setFilters } = useProjectFilters()\n\n  if (loading) return \n  if (error) return \n\n  return (\n    \n  )\n}\n\n// Presenter focuses purely on rendering\nfunction ProjectList({ projects, filters, onFilterChange }) {\n  return (\n    &lt;div className=&quot;space-y-4&quot;&gt;\n      \n      &lt;div className=&quot;grid grid-cols-3 gap-4&quot;&gt;\n        {projects.map(project =&gt; (\n          \n        ))}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  )\n}\n</code></pre>\n<h3>3. Compound Components for Flexibility</h3>\n<p>For components that need to work together, the compound component pattern provides excellent flexibility:</p>\n<pre><code class=\"language-tsx\">// Flexible, composable card component\nfunction Card({ children, className }) {\n  return &lt;div className={`card ${className}`}&gt;{children}&lt;/div&gt;\n}\n\nCard.Header = function CardHeader({ children }) {\n  return &lt;div className=&quot;card-header&quot;&gt;{children}&lt;/div&gt;\n}\n\nCard.Body = function CardBody({ children }) {\n  return &lt;div className=&quot;card-body&quot;&gt;{children}&lt;/div&gt;\n}\n\nCard.Footer = function CardFooter({ children }) {\n  return &lt;div className=&quot;card-footer&quot;&gt;{children}&lt;/div&gt;\n}\n\n// Usage - consumers can compose as needed\nfunction ProjectCard({ project }) {\n  return (\n    \n      &lt;/Card.Footer&gt;\n    &lt;/Card&gt;\n  )\n}\n</code></pre>\n<h2>State Management at Scale</h2>\n<p>Choosing the right state management approach is critical. I&#39;ve used all the major solutions in production, and here&#39;s when to use each.</p>\n<h3>Local State: useState</h3>\n<p>Start here. Most state should be local to the component that needs it.</p>\n<pre><code class=\"language-tsx\">function SearchInput() {\n  const [query, setQuery] = useState(&#39;&#39;)\n  const [results, setResults] = useState([])\n\n  const handleSearch = async (value: string) =&gt; {\n    setQuery(value)\n    const data = await searchAPI(value)\n    setResults(data)\n  }\n\n  return (\n    &lt;div&gt;\n      &lt;input\n        value={query}\n        onChange={(e) =&gt; handleSearch(e.target.value)}\n      /&gt;\n      \n    &lt;/div&gt;\n  )\n}\n</code></pre>\n<h3>Context: For Widely-Shared State</h3>\n<p>Context is perfect for theme, authentication, and feature flags—things that many components need but don&#39;t change often.</p>\n<pre><code class=\"language-tsx\">// auth-context.tsx\ninterface AuthContext {\n  user: User | null\n  login: (credentials: Credentials) =&gt; Promise&lt;void&gt;\n  logout: () =&gt; Promise&lt;void&gt;\n  isAuthenticated: boolean\n}\n\nconst AuthContext = createContext\n          }\n        /&gt;\n        }&gt;\n              \n            &lt;/Suspense&gt;\n          }\n        /&gt;\n      &lt;/Routes&gt;\n    &lt;/Router&gt;\n  )\n}\n</code></pre>\n<h3>2. Memoization: React.memo, useMemo, useCallback</h3>\n<p>Prevent unnecessary re-renders:</p>\n<pre><code class=\"language-tsx\">// Memoize expensive computations\nfunction ProjectAnalytics({ projects }: { projects: Project[] }) {\n  const statistics = useMemo(() =&gt; {\n    // Expensive calculation\n    return calculateProjectStatistics(projects)\n  }, [projects]) // Only recalculate when projects change\n\n  return \n}\n\n// Memoize callback functions\nfunction ProjectList({ projects }: { projects: Project[] }) {\n  const [selectedId, setSelectedId] = useState&lt;string | null&gt;(null)\n\n  // Without useCallback, this creates a new function on every render\n  // causing ProjectCard to re-render even if the project hasn&#39;t changed\n  const handleSelect = useCallback((id: string) =&gt; {\n    setSelectedId(id)\n    analytics.track(&#39;project_selected&#39;, { id })\n  }, []) // Empty deps - function never changes\n\n  return (\n    &lt;div&gt;\n      {projects.map(project =&gt; (\n        \n      ))}\n    &lt;/div&gt;\n  )\n}\n\n// Memoize components that receive stable props\nconst ProjectCard = memo(function ProjectCard({\n  project,\n  onSelect,\n  isSelected\n}: {\n  project: Project\n  onSelect: (id: string) =&gt; void\n  isSelected: boolean\n}) {\n  return (\n    &lt;div\n      className={isSelected ? &#39;selected&#39; : &#39;&#39;}\n      onClick={() =&gt; onSelect(project.id)}\n    &gt;\n      &lt;h3&gt;{project.title}&lt;/h3&gt;\n      &lt;p&gt;{project.description}&lt;/p&gt;\n    &lt;/div&gt;\n  )\n})\n</code></pre>\n<h3>3. Virtual Lists for Large Datasets</h3>\n<p>For lists with thousands of items, use virtualization:</p>\n<pre><code class=\"language-tsx\">import { useVirtualizer } from &#39;@tanstack/react-virtual&#39;\n\nfunction LargeProjectList({ projects }: { projects: Project[] }) {\n  const parentRef = useRef\n  )\n}\n</code></pre>\n<h2>Testing Strategy for Scale</h2>\n<p>A robust testing strategy is non-negotiable for large applications.</p>\n<h3>Unit Tests for Hooks and Utilities</h3>\n<pre><code class=\"language-tsx\">import { renderHook, waitFor } from &#39;@testing-library/react&#39;\nimport { useQuery } from &#39;./useQuery&#39;\n\ndescribe(&#39;useQuery&#39;, () =&gt; {\n  it(&#39;fetches data successfully&#39;, async () =&gt; {\n    const fetcher = jest.fn().mockResolvedValue({ id: 1, name: &#39;Test&#39; })\n\n    const { result } = renderHook(() =&gt;\n      useQuery(&#39;test&#39;, fetcher)\n    )\n\n    expect(result.current.loading).toBe(true)\n\n    await waitFor(() =&gt; {\n      expect(result.current.loading).toBe(false)\n    })\n\n    expect(result.current.data).toEqual({ id: 1, name: &#39;Test&#39; })\n    expect(result.current.error).toBeNull()\n  })\n\n  it(&#39;handles errors&#39;, async () =&gt; {\n    const error = new Error(&#39;Failed to fetch&#39;)\n    const fetcher = jest.fn().mockRejectedValue(error)\n\n    const { result } = renderHook(() =&gt;\n      useQuery(&#39;test&#39;, fetcher)\n    )\n\n    await waitFor(() =&gt; {\n      expect(result.current.loading).toBe(false)\n    })\n\n    expect(result.current.error).toEqual(error)\n    expect(result.current.data).toBeNull()\n  })\n})\n</code></pre>\n<h3>Integration Tests for Features</h3>\n<pre><code class=\"language-tsx\">import { render, screen, fireEvent, waitFor } from &#39;@testing-library/react&#39;\nimport { CreateProjectForm } from &#39;./CreateProjectForm&#39;\nimport * as projectAPI from &#39;@/api/projects&#39;\n\njest.mock(&#39;@/api/projects&#39;)\n\ndescribe(&#39;CreateProjectForm&#39;, () =&gt; {\n  it(&#39;creates project with valid data&#39;, async () =&gt; {\n    const mockCreate = jest.spyOn(projectAPI, &#39;create&#39;).mockResolvedValue({\n      id: &#39;123&#39;,\n      title: &#39;New Project&#39;,\n      description: &#39;Test project&#39;,\n    })\n\n    render()\n\n    fireEvent.change(screen.getByLabelText(/title/i), {\n      target: { value: &#39;New Project&#39; }\n    })\n\n    fireEvent.change(screen.getByLabelText(/description/i), {\n      target: { value: &#39;Test project&#39; }\n    })\n\n    fireEvent.click(screen.getByRole(&#39;button&#39;, { name: /create/i }))\n\n    await waitFor(() =&gt; {\n      expect(mockCreate).toHaveBeenCalledWith({\n        title: &#39;New Project&#39;,\n        description: &#39;Test project&#39;,\n      })\n    })\n\n    expect(screen.getByText(/project created/i)).toBeInTheDocument()\n  })\n})\n</code></pre>\n<h2>Folder Structure for Large Applications</h2>\n<p>Here&#39;s the structure I use for enterprise applications:</p>\n<pre><code>src/\n  app/                    # Next.js app directory or routing\n    (marketing)/\n    (app)/\n    api/\n  components/\n    common/               # Shared UI components\n      Button/\n        Button.tsx\n        Button.test.tsx\n        Button.stories.tsx\n      Card/\n      Modal/\n    features/             # Feature-specific components\n      projects/\n      timesheet/\n      reporting/\n  hooks/                  # Custom hooks\n    useAuth.ts\n    useQuery.ts\n    useForm.ts\n  lib/                    # Utilities and configurations\n    api/                  # API client\n    utils/                # Helper functions\n    constants/\n  stores/                 # State management\n    project-store.ts\n    user-store.ts\n  types/                  # TypeScript types\n    models/\n    api/\n    global.d.ts\n  styles/                 # Global styles\n</code></pre>\n<h2>Conclusion</h2>\n<p>Building scalable React applications is about making the right architectural decisions early and maintaining discipline as the codebase grows. The patterns I&#39;ve shared here—from component composition to state management to performance optimization—have proven themselves across multiple enterprise applications with hundreds of thousands of lines of code.</p>\n<p>Remember:</p>\n<ul>\n<li>Start with simple patterns and add complexity only when needed</li>\n<li>Keep components focused with single responsibilities</li>\n<li>Choose state management based on actual needs, not hype</li>\n<li>Optimize performance proactively, not reactively</li>\n<li>Write tests for critical functionality</li>\n<li>Organize code logically from day one</li>\n</ul>\n<p>After building applications that have grown from 10K to millions of lines of code, I can confidently say that following these patterns will set you up for success as your application scales.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/building-scalable-react-apps/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-09-30 00:00:00",
            "updated_at": "2025-09-30 00:00:00",
            "published_at": "2025-09-30 00:00:00",
            "custom_excerpt": "Architectural decisions for scaling React applications from my experience building Catalyst PSA. Real-world patterns that worked in production."
          },
          {
            "id": "11",
            "title": "Mobile-First Design for Field Operations",
            "slug": "mobile-first-design-field-operations",
            "html": "<p>Building WellOS for oil field workers taught me that mobile-first isn&#39;t just about responsive design—it&#39;s about designing for hands wearing gloves, screens in direct sunlight, limited connectivity, and workers who don&#39;t have time for complexity.</p>\n<p>Here&#39;s everything I learned about designing mobile apps for real-world field operations.</p>\n<h2>Understanding Field Workers&#39; Constraints</h2>\n<p>Field workers face unique challenges:</p>\n<ul>\n<li><strong>Environment</strong>: Bright sunlight, rain, dust, extreme temperatures</li>\n<li><strong>Connectivity</strong>: Spotty or non-existent cellular coverage</li>\n<li><strong>Time pressure</strong>: Quick data entry between tasks</li>\n<li><strong>Safety gear</strong>: Gloves, hard hats limiting dexterity</li>\n<li><strong>Distraction</strong>: Noisy, dangerous environments requiring focus</li>\n<li><strong>Devices</strong>: Older phones, cracked screens, low battery</li>\n</ul>\n<p>Design must accommodate these constraints.</p>\n<h2>Large Touch Targets</h2>\n<p>Minimum 44x44 points (Apple) or 48x48dp (Android). For field workers with gloves: 56x56 minimum.</p>\n<pre><code class=\"language-typescript\">// styles/touchable.ts\nexport const touchableStyles = StyleSheet.create({\n  // Standard\n  standard: {\n    minWidth: 44,\n    minHeight: 44,\n    padding: 12,\n  },\n\n  // Field-optimized (larger)\n  fieldOptimized: {\n    minWidth: 56,\n    minHeight: 56,\n    padding: 16,\n  },\n\n  // Critical actions (even larger)\n  critical: {\n    minWidth: 64,\n    minHeight: 64,\n    padding: 20,\n  },\n});\n\n// Component\nconst InspectionButton = ({ onPress }) =&gt; {\n  return (\n    \n    &lt;/TouchableOpacity&gt;\n  );\n};\n</code></pre>\n<p>Spacing between targets: minimum 8px, preferably 16px.</p>\n<h2>High Contrast for Sunlight Readability</h2>\n<p>Screens are hard to read in direct sunlight. Use high contrast:</p>\n<pre><code class=\"language-typescript\">// theme/colors.ts\nexport const fieldTheme = {\n  // High contrast\n  text: &#39;#000000&#39;,\n  background: &#39;#FFFFFF&#39;,\n\n  // Status colors (bold, distinct)\n  success: &#39;#047857&#39;, // Dark green\n  warning: &#39;#DC2626&#39;, // Dark red\n  info: &#39;#1E40AF&#39;,    // Dark blue\n\n  // Touch states (clear feedback)\n  pressed: &#39;#E5E7EB&#39;,\n  disabled: &#39;#D1D5DB&#39;,\n\n  // Avoid light colors\n  // ❌ &#39;#F3F4F6&#39; - too light\n  // ❌ &#39;#FEF3C7&#39; - too light\n};\n\n// Component\nconst InspectionCard = ({ inspection }) =&gt; {\n  const getStatusColor = (status: InspectionStatus) =&gt; {\n    switch (status) {\n      case &#39;completed&#39;: return fieldTheme.success;\n      case &#39;pending&#39;: return fieldTheme.warning;\n      case &#39;in-progress&#39;: return fieldTheme.info;\n    }\n  };\n\n  return (\n    \n      &lt;/View&gt;\n    &lt;/View&gt;\n  );\n};\n\nconst styles = StyleSheet.create({\n  card: {\n    backgroundColor: fieldTheme.background,\n    borderWidth: 2, // Bold border for visibility\n    borderColor: &#39;#000&#39;,\n  },\n  statusBadge: {\n    padding: 8,\n    borderRadius: 4,\n  },\n  statusText: {\n    color: &#39;#FFF&#39;,\n    fontWeight: &#39;bold&#39;,\n    fontSize: 16, // Large text\n  },\n});\n</code></pre>\n<h2>Large, Bold Typography</h2>\n<p>Small text is unreadable in field conditions:</p>\n<pre><code class=\"language-typescript\">// theme/typography.ts\nexport const fieldTypography = {\n  // Minimum sizes for field use\n  heading1: {\n    fontSize: 32,\n    fontWeight: &#39;700&#39;,\n    lineHeight: 40,\n  },\n  heading2: {\n    fontSize: 24,\n    fontWeight: &#39;700&#39;,\n    lineHeight: 32,\n  },\n  body: {\n    fontSize: 18, // Larger than typical 16px\n    fontWeight: &#39;400&#39;,\n    lineHeight: 26,\n  },\n  label: {\n    fontSize: 16,\n    fontWeight: &#39;600&#39;,\n    lineHeight: 24,\n  },\n  button: {\n    fontSize: 18,\n    fontWeight: &#39;700&#39;,\n    letterSpacing: 0.5,\n  },\n};\n\n// Component\nconst InspectionDetail = ({ inspection }) =&gt; {\n  return (\n    \n\n      \n    &lt;/ScrollView&gt;\n  );\n};\n</code></pre>\n<h2>Minimize Text Input</h2>\n<p>Text input is slow with gloves. Use selection, not typing:</p>\n<pre><code class=\"language-typescript\">// ❌ Bad: Requires typing\nconst InspectionForm = () =&gt; {\n  return (\n    \n  );\n};\n\n// ✅ Good: Buttons and selections\nconst InspectionForm = () =&gt; {\n  const [pressure, setPressure] = useState&lt;number | null&gt;(null);\n  const [temperature, setTemperature] = useState&lt;string&gt;(&#39;normal&#39;);\n\n  return (\n    \n      \n          &lt;/TouchableOpacity&gt;\n        ))}\n      &lt;/View&gt;\n\n      \n      \n          &lt;/TouchableOpacity&gt;\n        ))}\n      &lt;/View&gt;\n\n      {/* Voice input for notes */}\n      \n    &lt;/View&gt;\n  );\n};\n</code></pre>\n<h2>Voice Input for Notes</h2>\n<p>Voice is faster than typing:</p>\n<pre><code class=\"language-typescript\">import Voice from &#39;@react-native-voice/voice&#39;;\n\nexport const VoiceNoteRecorder = ({ onRecordingComplete }) =&gt; {\n  const [isRecording, setIsRecording] = useState(false);\n  const [transcript, setTranscript] = useState(&#39;&#39;);\n\n  useEffect(() =&gt; {\n    Voice.onSpeechResults = (e) =&gt; {\n      setTranscript(e.value[0]);\n    };\n\n    return () =&gt; {\n      Voice.destroy().then(Voice.removeAllListeners);\n    };\n  }, []);\n\n  const startRecording = async () =&gt; {\n    try {\n      await Voice.start(&#39;en-US&#39;);\n      setIsRecording(true);\n    } catch (error) {\n      console.error(error);\n    }\n  };\n\n  const stopRecording = async () =&gt; {\n    try {\n      await Voice.stop();\n      setIsRecording(false);\n      onRecordingComplete(transcript);\n    } catch (error) {\n      console.error(error);\n    }\n  };\n\n  return (\n    \n      &lt;/TouchableOpacity&gt;\n\n      {transcript &amp;&amp; (\n        \n      )}\n    &lt;/View&gt;\n  );\n};\n</code></pre>\n<h2>Offline-First Architecture</h2>\n<p>Field workers often have no connectivity. App must work offline:</p>\n<pre><code class=\"language-typescript\">export const InspectionList = () =&gt; {\n  const [inspections, setInspections] = useState\n  );\n};\n</code></pre>\n<h2>Clear Sync Status</h2>\n<p>Users need to know what&#39;s synced:</p>\n<pre><code class=\"language-typescript\">const SyncStatusBanner = ({ status }) =&gt; {\n  return (\n    \n        &lt;/&gt;\n      )}\n\n      {status === &#39;pending&#39; &amp;&amp; (\n        &lt;&gt;\n          \n          \n        &lt;/&gt;\n      )}\n\n      {status === &#39;error&#39; &amp;&amp; (\n        &lt;&gt;\n          \n          \n        &lt;/&gt;\n      )}\n    &lt;/View&gt;\n  );\n};\n</code></pre>\n<h2>Photo Capture Optimized</h2>\n<p>Camera is essential for field documentation:</p>\n<pre><code class=\"language-typescript\">import { Camera, useCameraDevices } from &#39;react-native-vision-camera&#39;;\n\nexport const InspectionPhotoCapture = ({ onPhotoTaken }) =&gt; {\n  const camera = useRef\n      &lt;/View&gt;\n    &lt;/View&gt;\n  );\n};\n\nconst styles = StyleSheet.create({\n  captureButton: {\n    width: 80,\n    height: 80,\n    borderRadius: 40,\n    backgroundColor: &#39;#FFF&#39;,\n    padding: 4,\n    alignSelf: &#39;center&#39;,\n  },\n  captureButtonInner: {\n    flex: 1,\n    borderRadius: 36,\n    backgroundColor: &#39;#DC2626&#39;,\n  },\n});\n</code></pre>\n<h2>GPS Tagging</h2>\n<p>Automatically tag location:</p>\n<pre><code class=\"language-typescript\">import Geolocation from &#39;@react-native-community/geolocation&#39;;\n\nexport const useLocationTagging = () =&gt; {\n  const [location, setLocation] = useState&lt;{\n    latitude: number;\n    longitude: number;\n  } | null&gt;(null);\n\n  useEffect(() =&gt; {\n    Geolocation.getCurrentPosition(\n      (position) =&gt; {\n        setLocation({\n          latitude: position.coords.latitude,\n          longitude: position.coords.longitude,\n        });\n      },\n      (error) =&gt; {\n        console.error(&#39;Location error:&#39;, error);\n      },\n      {\n        enableHighAccuracy: true,\n        timeout: 20000,\n        maximumAge: 1000,\n      }\n    );\n  }, []);\n\n  return location;\n};\n\n// Usage\nconst CreateInspection = () =&gt; {\n  const location = useLocationTagging();\n\n  const handleSubmit = async (data) =&gt; {\n    await db.insert(&#39;inspections&#39;).values({\n      ...data,\n      latitude: location?.latitude,\n      longitude: location?.longitude,\n      timestamp: new Date(),\n    });\n  };\n};\n</code></pre>\n<h2>Battery Optimization</h2>\n<p>Field workers can&#39;t charge frequently. Optimize battery:</p>\n<pre><code class=\"language-typescript\">// Reduce GPS polling\nGeolocation.getCurrentPosition(\n  callback,\n  errorCallback,\n  {\n    enableHighAccuracy: false, // Use network location (less battery)\n    timeout: 20000,\n    maximumAge: 300000, // Cache for 5 minutes\n  }\n);\n\n// Throttle API calls\nconst throttledSync = useCallback(\n  throttle(async () =&gt; {\n    await syncData();\n  }, 60000), // Max once per minute\n  []\n);\n\n// Batch operations\nconst batchSaveInspections = async (inspections: Inspection[]) =&gt; {\n  // Single transaction instead of multiple\n  await db.transaction(async (tx) =&gt; {\n    for (const inspection of inspections) {\n      await tx.insert(&#39;inspections&#39;).values(inspection);\n    }\n  });\n};\n\n// Reduce animations\nconst fieldAnimationConfig = {\n  useNativeDriver: true, // Faster, less battery\n  duration: 150, // Shorter = less CPU\n};\n</code></pre>\n<h2>Progressive Disclosure</h2>\n<p>Don&#39;t overwhelm with complexity. Show basics first:</p>\n<pre><code class=\"language-typescript\">const InspectionDetail = ({ inspection }) =&gt; {\n  const [showAdvanced, setShowAdvanced] = useState(false);\n\n  return (\n    \n        \n        \n      &lt;/View&gt;\n\n      {/* Advanced info hidden by default */}\n      {showAdvanced &amp;&amp; (\n        \n          \n          \n          {/* More details... */}\n        &lt;/View&gt;\n      )}\n\n      \n      &lt;/TouchableOpacity&gt;\n    &lt;/ScrollView&gt;\n  );\n};\n</code></pre>\n<h2>Haptic Feedback</h2>\n<p>Confirm actions with haptic feedback (especially with gloves):</p>\n<pre><code class=\"language-typescript\">import ReactNativeHapticFeedback from &#39;react-native-haptic-feedback&#39;;\n\nconst InspectionButton = ({ onPress, label }) =&gt; {\n  const handlePress = () =&gt; {\n    ReactNativeHapticFeedback.trigger(&#39;impactMedium&#39;);\n    onPress();\n  };\n\n  return (\n    \n    &lt;/TouchableOpacity&gt;\n  );\n};\n\n// Different feedback for different actions\nconst saveInspection = () =&gt; {\n  ReactNativeHapticFeedback.trigger(&#39;notificationSuccess&#39;);\n  // Save logic\n};\n\nconst deleteInspection = () =&gt; {\n  ReactNativeHapticFeedback.trigger(&#39;notificationWarning&#39;);\n  // Delete logic\n};\n</code></pre>\n<h2>Error Handling</h2>\n<p>Clear, actionable errors:</p>\n<pre><code class=\"language-typescript\">const ErrorMessage = ({ error, onRetry }) =&gt; {\n  return (\n    \n\n      \n\n      {onRetry &amp;&amp; (\n        \n        &lt;/TouchableOpacity&gt;\n      )}\n    &lt;/View&gt;\n  );\n};\n\n// Usage\nconst InspectionsList = () =&gt; {\n  const [error, setError] = useState(null);\n\n  const loadInspections = async () =&gt; {\n    try {\n      const data = await api.getInspections();\n      setInspections(data);\n    } catch (err) {\n      setError({\n        title: &#39;Cannot load inspections&#39;,\n        message: &#39;Check your connection and try again.&#39;,\n      });\n    }\n  };\n\n  if (error) {\n    return ;\n  }\n\n  // ...\n};\n</code></pre>\n<h2>Results</h2>\n<p>After implementing these field-optimized designs in WellOS:</p>\n<ul>\n<li><strong>Task completion time</strong>: 40% faster</li>\n<li><strong>Error rate</strong>: 60% reduction</li>\n<li><strong>User satisfaction</strong>: 4.2 → 4.8 stars</li>\n<li><strong>Adoption rate</strong>: 95% of field workers using daily</li>\n</ul>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Test in real conditions</strong>: Office testing doesn&#39;t reveal sunlight issues</li>\n<li><strong>Gloves change everything</strong>: Touch targets must be larger</li>\n<li><strong>Offline is non-negotiable</strong>: Field workers have no connectivity</li>\n<li><strong>Voice &gt; typing</strong>: Much faster for notes</li>\n<li><strong>Battery matters</strong>: Field workers can&#39;t charge mid-shift</li>\n<li><strong>Simplicity wins</strong>: Complex features go unused</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Designing for field operations requires rethinking standard mobile UX. Large touch targets, high contrast, minimal text input, offline-first architecture, and optimized battery usage are essential.</p>\n<p>After 27 years of building software and two years building field operation apps, I&#39;ve learned that the best mobile apps for workers are the ones that get out of their way—fast, simple, and reliable when it matters most.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/mobile-first-design-field-operations/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-03-14 00:00:00",
            "updated_at": "2025-03-14 00:00:00",
            "published_at": "2025-03-14 00:00:00",
            "custom_excerpt": "Designing mobile applications for field workers in challenging environments, based on building WellOS for oil & gas operations with limited connectivity and harsh conditions."
          },
          {
            "id": "12",
            "title": "Database Query Optimization for Large Datasets",
            "slug": "database-query-optimization-large-datasets",
            "html": "<p>When Catalyst PSA hit 10 million time entries and queries started taking 30+ seconds, we had to get serious about database optimization. After 27 years of database work and optimizing queries across PostgreSQL, SQL Server, MySQL, and MongoDB, here are the techniques that made the biggest impact.</p>\n<h2>Start with EXPLAIN: Understand Before Optimizing</h2>\n<p>Never optimize blind. Always start with query execution plans.</p>\n<h3>PostgreSQL EXPLAIN ANALYZE</h3>\n<pre><code class=\"language-sql\">EXPLAIN ANALYZE\nSELECT p.id, p.name, COUNT(te.id) as entry_count\nFROM projects p\nLEFT JOIN time_entries te ON te.project_id = p.id\nWHERE p.status = &#39;active&#39;\nGROUP BY p.id, p.name;\n</code></pre>\n<p>Output shows:</p>\n<pre><code>GroupAggregate  (cost=125.45..130.12 rows=150 width=520) (actual time=2543.234..2544.012 rows=148 loops=1)\n  -&gt;  Hash Join  (cost=12.50..100.00 rows=500 width=512) (actual time=0.234..2500.123 rows=500000 loops=1)\n        Hash Cond: (te.project_id = p.id)\n        -&gt;  Seq Scan on time_entries te  (cost=0.00..50000.00 rows=10000000 width=16) (actual time=0.012..1500.456 rows=10000000 loops=1)\n        -&gt;  Hash  (cost=10.00..10.00 rows=150 width=512) (actual time=0.123..0.123 rows=148 loops=1)\n              -&gt;  Seq Scan on projects p  (cost=0.00..10.00 rows=150 width=512) (actual time=0.001..0.050 rows=148 loops=1)\n                    Filter: (status = &#39;active&#39;::text)\nPlanning Time: 0.234 ms\nExecution Time: 2544.123 ms\n</code></pre>\n<p>Key insights:</p>\n<ul>\n<li><strong>Seq Scan</strong>: Full table scan (bad for large tables)</li>\n<li><strong>actual time</strong>: Real execution time</li>\n<li><strong>rows</strong>: Estimated vs actual rows</li>\n<li><strong>loops</strong>: How many times operation ran</li>\n</ul>\n<p>The sequential scan on 10M time_entries is killing performance.</p>\n<h2>Indexing: The Most Important Optimization</h2>\n<p>Indexes are to databases what table of contents is to books.</p>\n<h3>Basic Single-Column Index</h3>\n<pre><code class=\"language-sql\">-- Problem: Slow filtering by status\nSELECT * FROM projects WHERE status = &#39;active&#39;;\n\n-- Solution: Index on status\nCREATE INDEX idx_projects_status ON projects(status);\n\n-- Result: 2500ms -&gt; 15ms\n</code></pre>\n<h3>Composite Indexes</h3>\n<p>Order matters! Most selective column first:</p>\n<pre><code class=\"language-sql\">-- Query filtering by tenant and status\nSELECT * FROM projects\nWHERE tenant_id = &#39;123&#39; AND status = &#39;active&#39;;\n\n-- Good: tenant_id first (more selective)\nCREATE INDEX idx_projects_tenant_status ON projects(tenant_id, status);\n\n-- Bad: status first (less selective)\nCREATE INDEX idx_projects_status_tenant ON projects(status, tenant_id);\n</code></pre>\n<p>With good index: 0.5ms. With bad index: 50ms.</p>\n<h3>Covering Indexes</h3>\n<p>Include all columns needed by query:</p>\n<pre><code class=\"language-sql\">-- Query needs id, name, status\nSELECT id, name, status FROM projects\nWHERE tenant_id = &#39;123&#39; AND status = &#39;active&#39;;\n\n-- Covering index - no table lookup needed\nCREATE INDEX idx_projects_covering ON projects(tenant_id, status) INCLUDE (id, name);\n\n-- Result: 50ms -&gt; 2ms\n</code></pre>\n<p>PostgreSQL fetches everything from index without touching table.</p>\n<h3>Partial Indexes</h3>\n<p>Index only relevant rows:</p>\n<pre><code class=\"language-sql\">-- Only query active projects\nSELECT * FROM projects WHERE status = &#39;active&#39;;\n\n-- Partial index - smaller, faster\nCREATE INDEX idx_projects_active ON projects(tenant_id)\nWHERE status = &#39;active&#39;;\n\n-- Index is 80% smaller, queries are faster\n</code></pre>\n<h3>Index on Computed Columns</h3>\n<pre><code class=\"language-sql\">-- Query on lowercase email\nSELECT * FROM users WHERE LOWER(email) = &#39;john@example.com&#39;;\n\n-- Index on expression\nCREATE INDEX idx_users_email_lower ON users(LOWER(email));\n\n-- Now query uses index\n</code></pre>\n<h2>Query Optimization Techniques</h2>\n<h3>Avoid SELECT *</h3>\n<p>Select only needed columns:</p>\n<pre><code class=\"language-sql\">-- Bad: Fetches all columns (wasteful)\nSELECT * FROM projects;\n\n-- Good: Fetch only what you need\nSELECT id, name, status FROM projects;\n\n-- Result: 500ms -&gt; 100ms (5x faster)\n</code></pre>\n<p>With 50 columns and millions of rows, this matters.</p>\n<h3>Use LIMIT</h3>\n<p>Always limit results:</p>\n<pre><code class=\"language-sql\">-- Bad: Returns all 10 million records\nSELECT * FROM time_entries ORDER BY created_at DESC;\n\n-- Good: Return first page\nSELECT * FROM time_entries ORDER BY created_at DESC LIMIT 20 OFFSET 0;\n\n-- For pagination, use cursor-based:\nSELECT * FROM time_entries\nWHERE created_at &lt; &#39;2024-01-01T00:00:00Z&#39;\nORDER BY created_at DESC\nLIMIT 20;\n</code></pre>\n<p>Cursor-based pagination is faster than OFFSET for large datasets.</p>\n<h3>Avoid N+1 Queries</h3>\n<p>Classic ORM problem:</p>\n<pre><code class=\"language-typescript\">// Bad: N+1 queries (1 + 100 queries)\nconst projects = await db.select().from(projects);\n\nfor (const project of projects) {\n  const client = await db.select().from(clients).where(eq(clients.id, project.clientId));\n  project.client = client;\n}\n\n// Good: Single query with join\nconst projectsWithClients = await db\n  .select({\n    project: projects,\n    client: clients,\n  })\n  .from(projects)\n  .leftJoin(clients, eq(projects.clientId, clients.id));\n</code></pre>\n<p>Result: 100 queries -&gt; 1 query. 5000ms -&gt; 50ms.</p>\n<h3>Batch Operations</h3>\n<p>Insert/update in batches:</p>\n<pre><code class=\"language-sql\">-- Bad: 1000 separate inserts\nINSERT INTO time_entries (project_id, hours) VALUES (&#39;123&#39;, 8);\nINSERT INTO time_entries (project_id, hours) VALUES (&#39;124&#39;, 6);\n-- ... 998 more\n\n-- Good: Single batch insert\nINSERT INTO time_entries (project_id, hours)\nVALUES\n  (&#39;123&#39;, 8),\n  (&#39;124&#39;, 6),\n  (&#39;125&#39;, 7),\n  -- ... all 1000 rows\n;\n\n-- Result: 30 seconds -&gt; 0.5 seconds\n</code></pre>\n<p>With ORMs:</p>\n<pre><code class=\"language-typescript\">// Drizzle batch insert\nawait db.insert(timeEntries).values(\n  entries.map(e =&gt; ({\n    projectId: e.projectId,\n    hours: e.hours,\n  }))\n);\n</code></pre>\n<h3>Avoid Subqueries When Possible</h3>\n<pre><code class=\"language-sql\">-- Bad: Correlated subquery (runs for each row)\nSELECT p.id, p.name,\n  (SELECT COUNT(*) FROM time_entries WHERE project_id = p.id) as entry_count\nFROM projects p;\n\n-- Good: Join and aggregate\nSELECT p.id, p.name, COUNT(te.id) as entry_count\nFROM projects p\nLEFT JOIN time_entries te ON te.project_id = p.id\nGROUP BY p.id, p.name;\n\n-- Result: 8000ms -&gt; 200ms\n</code></pre>\n<h2>JOIN Optimization</h2>\n<h3>Choose the Right JOIN Type</h3>\n<pre><code class=\"language-sql\">-- INNER JOIN: Only matching rows (fastest)\nSELECT * FROM projects p\nINNER JOIN clients c ON p.client_id = c.id;\n\n-- LEFT JOIN: All left rows + matching right (slower)\nSELECT * FROM projects p\nLEFT JOIN clients c ON p.client_id = c.id;\n\n-- Use INNER JOIN when possible\n</code></pre>\n<h3>Join Order Matters</h3>\n<p>Smaller table first:</p>\n<pre><code class=\"language-sql\">-- Bad: 10M rows joined with 1K rows\nSELECT * FROM time_entries te\nLEFT JOIN projects p ON te.project_id = p.id;\n\n-- Good: 1K rows joined with 10M rows\nSELECT * FROM projects p\nLEFT JOIN time_entries te ON p.id = te.project_id;\n</code></pre>\n<p>Modern query planners optimize this, but explicit ordering helps.</p>\n<h3>Index Foreign Keys</h3>\n<p>Always index columns used in joins:</p>\n<pre><code class=\"language-sql\">CREATE INDEX idx_time_entries_project_id ON time_entries(project_id);\nCREATE INDEX idx_time_entries_user_id ON time_entries(user_id);\n</code></pre>\n<p>Without these indexes, joins do full table scans.</p>\n<h2>Aggregation Optimization</h2>\n<h3>Use Indexed Columns in GROUP BY</h3>\n<pre><code class=\"language-sql\">-- Query\nSELECT client_id, COUNT(*) FROM projects GROUP BY client_id;\n\n-- Index helps grouping\nCREATE INDEX idx_projects_client_id ON projects(client_id);\n\n-- Result: 5000ms -&gt; 50ms\n</code></pre>\n<h3>Materialized Views for Complex Aggregations</h3>\n<p>Pre-compute expensive aggregations:</p>\n<pre><code class=\"language-sql\">-- Create materialized view\nCREATE MATERIALIZED VIEW project_stats AS\nSELECT\n  p.id,\n  p.name,\n  COUNT(DISTINCT te.id) as total_entries,\n  SUM(te.hours) as total_hours,\n  COUNT(DISTINCT te.user_id) as team_size\nFROM projects p\nLEFT JOIN time_entries te ON te.project_id = p.id\nGROUP BY p.id, p.name;\n\nCREATE INDEX idx_project_stats_id ON project_stats(id);\n\n-- Query materialized view (instant)\nSELECT * FROM project_stats WHERE id = &#39;123&#39;;\n\n-- Refresh periodically\nREFRESH MATERIALIZED VIEW CONCURRENTLY project_stats;\n</code></pre>\n<p>Complex aggregation: 30s -&gt; 0.01s.</p>\n<h2>Date Range Queries</h2>\n<h3>Use Date Indexes</h3>\n<pre><code class=\"language-sql\">-- Query by date range\nSELECT * FROM time_entries\nWHERE entry_date BETWEEN &#39;2024-01-01&#39; AND &#39;2024-12-31&#39;;\n\n-- Index on date column\nCREATE INDEX idx_time_entries_date ON time_entries(entry_date);\n\n-- For timestamp queries, use BRIN index for large tables\nCREATE INDEX idx_time_entries_created_brin ON time_entries USING BRIN (created_at);\n</code></pre>\n<p>BRIN indexes are tiny and fast for chronological data.</p>\n<h3>Partition by Date</h3>\n<p>For massive tables, partition by date:</p>\n<pre><code class=\"language-sql\">CREATE TABLE time_entries (\n  id UUID,\n  project_id UUID,\n  entry_date DATE,\n  hours DECIMAL\n) PARTITION BY RANGE (entry_date);\n\nCREATE TABLE time_entries_2024 PARTITION OF time_entries\nFOR VALUES FROM (&#39;2024-01-01&#39;) TO (&#39;2025-01-01&#39;);\n\nCREATE TABLE time_entries_2023 PARTITION OF time_entries\nFOR VALUES FROM (&#39;2023-01-01&#39;) TO (&#39;2024-01-01&#39;);\n</code></pre>\n<p>Queries only scan relevant partitions. 10M row scan -&gt; 1M row scan.</p>\n<h2>Full-Text Search</h2>\n<h3>Use PostgreSQL&#39;s Built-in FTS</h3>\n<pre><code class=\"language-sql\">-- Add tsvector column\nALTER TABLE projects ADD COLUMN search_vector tsvector;\n\n-- Generate search vector\nUPDATE projects SET search_vector = to_tsvector(&#39;english&#39;, name || &#39; &#39; || description);\n\n-- Index it\nCREATE INDEX idx_projects_search ON projects USING GIN(search_vector);\n\n-- Query\nSELECT * FROM projects\nWHERE search_vector @@ to_tsquery(&#39;english&#39;, &#39;website &amp; redesign&#39;);\n\n-- Automatic updates\nCREATE TRIGGER projects_search_update\nBEFORE INSERT OR UPDATE ON projects\nFOR EACH ROW EXECUTE FUNCTION\ntsvector_update_trigger(search_vector, &#39;pg_catalog.english&#39;, name, description);\n</code></pre>\n<p>Full-text search: 5000ms -&gt; 20ms.</p>\n<h2>Connection Pooling</h2>\n<p>Don&#39;t create new connections for each query:</p>\n<pre><code class=\"language-typescript\">// Bad: New connection per query\nexport async function query(sql: string) {\n  const client = new Client({ connectionString: process.env.DATABASE_URL });\n  await client.connect();\n  const result = await client.query(sql);\n  await client.end();\n  return result;\n}\n\n// Good: Connection pool\nimport { Pool } from &#39;pg&#39;;\n\nconst pool = new Pool({\n  connectionString: process.env.DATABASE_URL,\n  max: 20, // max connections\n  idleTimeoutMillis: 30000,\n  connectionTimeoutMillis: 2000,\n});\n\nexport async function query(sql: string) {\n  return await pool.query(sql);\n}\n</code></pre>\n<p>With Drizzle:</p>\n<pre><code class=\"language-typescript\">import { drizzle } from &#39;drizzle-orm/node-postgres&#39;;\nimport { Pool } from &#39;pg&#39;;\n\nconst pool = new Pool({ max: 20 });\nconst db = drizzle(pool);\n</code></pre>\n<h2>Analyze and Vacuum</h2>\n<p>PostgreSQL needs maintenance:</p>\n<pre><code class=\"language-sql\">-- Update statistics for query planner\nANALYZE projects;\nANALYZE time_entries;\n\n-- Or all tables\nANALYZE;\n\n-- Clean up dead rows\nVACUUM time_entries;\n\n-- Full vacuum (locks table)\nVACUUM FULL time_entries;\n\n-- Enable auto-vacuum\nALTER TABLE time_entries SET (autovacuum_enabled = true);\n</code></pre>\n<p>We run <code>ANALYZE</code> after bulk imports. Without it, query planner makes bad decisions.</p>\n<h2>Monitoring and Profiling</h2>\n<h3>Find Slow Queries</h3>\n<pre><code class=\"language-sql\">-- Enable slow query log (postgresql.conf)\nlog_min_duration_statement = 1000  # Log queries &gt; 1s\n\n-- Or use pg_stat_statements extension\nCREATE EXTENSION pg_stat_statements;\n\n-- Find slowest queries\nSELECT\n  query,\n  calls,\n  total_exec_time,\n  mean_exec_time,\n  max_exec_time\nFROM pg_stat_statements\nORDER BY mean_exec_time DESC\nLIMIT 20;\n</code></pre>\n<p>This showed us that a single report query was taking 80% of database time.</p>\n<h3>Monitor Index Usage</h3>\n<pre><code class=\"language-sql\">-- Find unused indexes\nSELECT\n  schemaname,\n  tablename,\n  indexname,\n  idx_scan\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\nORDER BY pg_relation_size(indexrelid) DESC;\n</code></pre>\n<p>We found 15 unused indexes consuming 2GB. Dropped them.</p>\n<h2>Results</h2>\n<p>After applying these optimizations to Catalyst PSA:</p>\n<ul>\n<li><strong>Dashboard load</strong>: 30s -&gt; 1.5s (20x faster)</li>\n<li><strong>Report generation</strong>: 45s -&gt; 3s (15x faster)</li>\n<li><strong>API average response time</strong>: 500ms -&gt; 50ms (10x faster)</li>\n<li><strong>Database size</strong>: Reduced 20% by removing unused indexes</li>\n<li><strong>Query throughput</strong>: 500 qps -&gt; 2000 qps</li>\n</ul>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Index everything used in WHERE/JOIN</strong>: Most important optimization</li>\n<li><strong>EXPLAIN is your best friend</strong>: Always check execution plans</li>\n<li>**Avoid SELECT ***: Select only needed columns</li>\n<li><strong>Batch operations</strong>: Never insert/update one row at a time</li>\n<li><strong>Monitor in production</strong>: Find slow queries with logging</li>\n<li><strong>Partition large tables</strong>: Especially time-series data</li>\n<li><strong>Connection pooling is essential</strong>: Never create connections per query</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Database optimization is about understanding how databases work: indexes, query plans, joins, and aggregations. With the right indexes and query patterns, even databases with hundreds of millions of rows can be fast.</p>\n<p>After 27 years of database work, I&#39;ve learned that most performance problems come from missing indexes or N+1 queries. Fix those, and you&#39;re 90% of the way there.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/database-query-optimization-large-datasets/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-02-28 00:00:00",
            "updated_at": "2025-02-28 00:00:00",
            "published_at": "2025-02-28 00:00:00",
            "custom_excerpt": "Proven techniques for optimizing database queries in enterprise applications with millions of records, based on experience with PostgreSQL, SQL Server, and MongoDB at scale."
          },
          {
            "id": "13",
            "title": "Building Enterprise Forms: Complex Validation and UX",
            "slug": "building-enterprise-forms-validation-ux",
            "html": "<p>Enterprise forms are deceptively complex. A simple project creation form in Catalyst PSA grew to 47 fields across 4 steps with 15 conditional validation rules. Building forms that handle this complexity while remaining user-friendly taught me that great form UX is an art.</p>\n<p>Here&#39;s everything I&#39;ve learned about building enterprise forms over 27 years.</p>\n<h2>The Stack: React Hook Form + Zod</h2>\n<p>After trying Formik, Redux Form, and custom solutions, React Hook Form + Zod became our standard:</p>\n<pre><code class=\"language-typescript\">import { useForm } from &#39;react-hook-form&#39;;\nimport { zodResolver } from &#39;@hookform/resolvers/zod&#39;;\nimport { z } from &#39;zod&#39;;\n\nconst projectSchema = z.object({\n  name: z.string().min(1, &#39;Project name is required&#39;).max(255),\n  clientId: z.string().uuid(&#39;Invalid client&#39;),\n  budget: z.number().min(0).max(10000000).optional(),\n  startDate: z.date(),\n  endDate: z.date().optional(),\n  description: z.string().max(5000).optional(),\n}).refine((data) =&gt; {\n  // Custom validation: endDate must be after startDate\n  if (data.endDate &amp;&amp; data.endDate &lt; data.startDate) {\n    return false;\n  }\n  return true;\n}, {\n  message: &#39;End date must be after start date&#39;,\n  path: [&#39;endDate&#39;],\n});\n\ntype ProjectFormData = z.infer&lt;typeof projectSchema&gt;;\n\nexport const ProjectForm = () =&gt; {\n  const {\n    register,\n    handleSubmit,\n    formState: { errors, isSubmitting },\n  } = useForm\n        )}\n\n        \n      &lt;/div&gt;\n    &lt;/form&gt;\n  );\n};\n</code></pre>\n<h3>Persist Form State Between Steps</h3>\n<p>Don&#39;t lose data when users navigate back:</p>\n<pre><code class=\"language-typescript\">import { useLocalStorage } from &#39;@/hooks/useLocalStorage&#39;;\n\nexport const ProjectWizard = () =&gt; {\n  const [step, setStep] = useState(1);\n\n  const { register, handleSubmit, watch } = useForm({\n    defaultValues: useLocalStorage(&#39;projectWizard&#39;, {}),\n  });\n\n  // Save to localStorage on every change\n  const formData = watch();\n\n  useEffect(() =&gt; {\n    localStorage.setItem(&#39;projectWizard&#39;, JSON.stringify(formData));\n  }, [formData]);\n\n  const onSubmit = async (data: ProjectFormData) =&gt; {\n    await api.createProject(data);\n\n    // Clear saved data after successful submit\n    localStorage.removeItem(&#39;projectWizard&#39;);\n  };\n};\n</code></pre>\n<h2>Conditional Validation</h2>\n<p>Validation rules change based on other fields:</p>\n<pre><code class=\"language-typescript\">const projectSchema = z.discriminatedUnion(&#39;billingType&#39;, [\n  z.object({\n    billingType: z.literal(&#39;fixed-price&#39;),\n    fixedPrice: z.number().min(1, &#39;Fixed price required&#39;),\n    // hourlyRate not needed\n  }),\n  z.object({\n    billingType: z.literal(&#39;time-materials&#39;),\n    hourlyRate: z.number().min(1, &#39;Hourly rate required&#39;),\n    // fixedPrice not needed\n  }),\n]);\n\n// Or with custom validation\nconst projectSchema = z.object({\n  billingType: z.enum([&#39;fixed-price&#39;, &#39;time-materials&#39;]),\n  fixedPrice: z.number().optional(),\n  hourlyRate: z.number().optional(),\n}).superRefine((data, ctx) =&gt; {\n  if (data.billingType === &#39;fixed-price&#39; &amp;&amp; !data.fixedPrice) {\n    ctx.addIssue({\n      code: z.ZodIssueCode.custom,\n      message: &#39;Fixed price is required&#39;,\n      path: [&#39;fixedPrice&#39;],\n    });\n  }\n\n  if (data.billingType === &#39;time-materials&#39; &amp;&amp; !data.hourlyRate) {\n    ctx.addIssue({\n      code: z.ZodIssueCode.custom,\n      message: &#39;Hourly rate is required&#39;,\n      path: [&#39;hourlyRate&#39;],\n    });\n  }\n});\n</code></pre>\n<h2>Autosave for Long Forms</h2>\n<p>Save progress automatically to prevent data loss:</p>\n<pre><code class=\"language-typescript\">export const ProjectForm = () =&gt; {\n  const { register, watch } = useForm();\n  const formData = watch();\n\n  // Debounced autosave\n  useEffect(() =&gt; {\n    const timeoutId = setTimeout(() =&gt; {\n      api.saveProjectDraft(formData);\n    }, 2000); // Save 2 seconds after user stops typing\n\n    return () =&gt; clearTimeout(timeoutId);\n  }, [formData]);\n\n  return (\n    &lt;form&gt;\n      \n      {/* form fields */}\n    &lt;/form&gt;\n  );\n};\n\nconst AutosaveIndicator = () =&gt; {\n  const [saving, setSaving] = useState(false);\n  const [lastSaved, setLastSaved] = useState\n        &lt;/div&gt;\n      ))}\n\n      \n    &lt;/form&gt;\n  );\n};\n</code></pre>\n<h2>File Uploads</h2>\n<p>Handle files in forms:</p>\n<pre><code class=\"language-typescript\">export const ProjectForm = () =&gt; {\n  const [files, setFiles] = useState\n      )}\n\n      {/* fields */}\n    &lt;/form&gt;\n  );\n};\n</code></pre>\n<h2>Accessibility</h2>\n<p>Forms must be accessible:</p>\n<pre><code class=\"language-typescript\">export const AccessibleInput = ({ label, name, error, ...props }) =&gt; {\n  const inputId = `input-${name}`;\n  const errorId = `error-${name}`;\n\n  return (\n    &lt;div&gt;\n      &lt;label htmlFor={inputId}&gt;\n        {label}\n        {props.required &amp;&amp; &lt;span aria-label=&quot;required&quot;&gt;*&lt;/span&gt;}\n      &lt;/label&gt;\n\n      &lt;input\n        id={inputId}\n        name={name}\n        aria-required={props.required}\n        aria-invalid={!!error}\n        aria-describedby={error ? errorId : undefined}\n        {...props}\n      /&gt;\n\n      {error &amp;&amp; (\n        &lt;span id={errorId} role=&quot;alert&quot; className=&quot;text-red-500&quot;&gt;\n          {error.message}\n        &lt;/span&gt;\n      )}\n    &lt;/div&gt;\n  );\n};\n</code></pre>\n<h2>UX Best Practices</h2>\n<h3>1. Inline Validation</h3>\n<p>Validate as user types (after first blur):</p>\n<pre><code class=\"language-typescript\">const { register, formState: { errors, touchedFields } } = useForm({\n  mode: &#39;onTouched&#39;, // Validate after first blur, then on change\n});\n</code></pre>\n<h3>2. Clear Error Messages</h3>\n<pre><code>❌ &quot;Invalid input&quot;\n✅ &quot;Project name must be between 1 and 255 characters&quot;\n\n❌ &quot;Required&quot;\n✅ &quot;Client is required&quot;\n\n❌ &quot;Error&quot;\n✅ &quot;End date must be after start date&quot;\n</code></pre>\n<h3>3. Prevent Duplicate Submissions</h3>\n<pre><code class=\"language-typescript\">const [submitting, setSubmitting] = useState(false);\n\nconst handleSubmit = async (data: ProjectFormData) =&gt; {\n  if (submitting) return;\n\n  setSubmitting(true);\n\n  try {\n    await api.createProject(data);\n  } finally {\n    setSubmitting(false);\n  }\n};\n\nreturn (\n  &lt;button type=&quot;submit&quot; disabled={submitting}&gt;\n    {submitting ? &#39;Creating...&#39; : &#39;Create Project&#39;}\n  &lt;/button&gt;\n);\n</code></pre>\n<h3>4. Preserve Data on Error</h3>\n<p>Don&#39;t clear form when submission fails:</p>\n<pre><code class=\"language-typescript\">const handleSubmit = async (data: ProjectFormData) =&gt; {\n  try {\n    await api.createProject(data);\n    reset(); // Clear form only on success\n  } catch (error) {\n    // Form data preserved, user can fix and retry\n    setError(&#39;root&#39;, { message: error.message });\n  }\n};\n</code></pre>\n<h3>5. Loading States</h3>\n<p>Show progress for long operations:</p>\n<pre><code class=\"language-typescript\">\n</code></pre>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Break complex forms into steps</strong>: Multi-step wizards reduce cognitive load</li>\n<li><strong>Autosave frequently</strong>: Prevent data loss on crashes or accidental navigation</li>\n<li><strong>Validate early, not eagerly</strong>: After first blur, then on change</li>\n<li><strong>Clear error messages</strong>: Help users fix problems</li>\n<li><strong>Conditional validation</strong>: Rules change based on other fields</li>\n<li><strong>Test accessibility</strong>: Screen readers, keyboard navigation</li>\n<li><strong>Preserve state</strong>: Don&#39;t lose data on errors or navigation</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Building great enterprise forms requires balancing complexity with usability. Multi-step wizards, conditional validation, autosave, and clear error messages make complex forms manageable.</p>\n<p>After 27 years of building forms, I&#39;ve learned that the best forms are invisible—users complete them without thinking about the form itself.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/building-enterprise-forms-validation-ux/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-02-27 00:00:00",
            "updated_at": "2025-02-27 00:00:00",
            "published_at": "2025-02-27 00:00:00",
            "custom_excerpt": "Best practices for building complex enterprise forms with multi-step wizards, conditional validation, autosave, and excellent UX, from building forms in Catalyst PSA and WellOS."
          },
          {
            "id": "14",
            "title": "Monorepo Management for Multi-App Projects",
            "slug": "monorepo-management-multi-app-projects",
            "html": "<p>When we built WellOS—a platform with 6 integrated apps (3 React Native mobile apps, 2 Next.js web apps, and 1 Node.js backend)—we needed a way to share code, manage dependencies, and build efficiently. A monorepo was the answer.</p>\n<p>After two years managing a monorepo across multiple apps in a single repository, here&#39;s everything I learned about monorepo management.</p>\n<h2>Why Monorepo?</h2>\n<p>WellOS consists of:</p>\n<ul>\n<li><strong>Field Worker Mobile App</strong> (React Native)</li>\n<li><strong>Manager Mobile App</strong> (React Native)</li>\n<li><strong>Admin Mobile App</strong> (React Native)</li>\n<li><strong>Customer Portal</strong> (Next.js)</li>\n<li><strong>Internal Dashboard</strong> (Next.js)</li>\n<li><strong>Backend API</strong> (NestJS)</li>\n</ul>\n<p>These apps share:</p>\n<ul>\n<li>Business logic (domain models, validation)</li>\n<li>UI components (design system)</li>\n<li>Type definitions (TypeScript interfaces)</li>\n<li>Utilities (date formatting, calculations)</li>\n<li>Configuration (ESLint, TypeScript, environment)</li>\n</ul>\n<p>Options:</p>\n<ol>\n<li><strong>Separate repos</strong>: Duplicate code, version hell, hard to coordinate changes</li>\n<li><strong>NPM packages</strong>: Publish shared code as packages (slow, version management)</li>\n<li><strong>Monorepo</strong>: Single repo, shared code, atomic changes</li>\n</ol>\n<p>We chose monorepo, and it transformed our development workflow.</p>\n<h2>Monorepo Structure</h2>\n<pre><code>wellos/\n├── apps/\n│   ├── mobile-field/          # React Native - field workers\n│   ├── mobile-manager/        # React Native - managers\n│   ├── mobile-admin/          # React Native - admins\n│   ├── web-customer/          # Next.js - customer portal\n│   ├── web-dashboard/         # Next.js - internal dashboard\n│   └── api/                   # NestJS - backend\n├── packages/\n│   ├── domain/                # Domain models, business logic\n│   ├── ui/                    # Shared React components\n│   ├── ui-native/             # React Native specific components\n│   ├── config/                # Shared config (ESLint, TS, etc.)\n│   ├── utils/                 # Utility functions\n│   └── types/                 # TypeScript type definitions\n├── package.json\n├── turbo.json\n└── tsconfig.json\n</code></pre>\n<h2>Setting Up with Turborepo</h2>\n<p>We use Turborepo for fast, cached builds:</p>\n<pre><code class=\"language-json\">// package.json (root)\n{\n  &quot;name&quot;: &quot;wellos-monorepo&quot;,\n  &quot;private&quot;: true,\n  &quot;workspaces&quot;: [\n    &quot;apps/*&quot;,\n    &quot;packages/*&quot;\n  ],\n  &quot;scripts&quot;: {\n    &quot;dev&quot;: &quot;turbo run dev&quot;,\n    &quot;build&quot;: &quot;turbo run build&quot;,\n    &quot;test&quot;: &quot;turbo run test&quot;,\n    &quot;lint&quot;: &quot;turbo run lint&quot;,\n    &quot;type-check&quot;: &quot;turbo run type-check&quot;\n  },\n  &quot;devDependencies&quot;: {\n    &quot;turbo&quot;: &quot;^1.10.0&quot;,\n    &quot;typescript&quot;: &quot;^5.0.0&quot;\n  }\n}\n</code></pre>\n<p>Turborepo configuration:</p>\n<pre><code class=\"language-json\">// turbo.json\n{\n  &quot;pipeline&quot;: {\n    &quot;build&quot;: {\n      &quot;dependsOn&quot;: [&quot;^build&quot;],\n      &quot;outputs&quot;: [&quot;dist/**&quot;, &quot;.next/**&quot;, &quot;build/**&quot;]\n    },\n    &quot;dev&quot;: {\n      &quot;cache&quot;: false,\n      &quot;persistent&quot;: true\n    },\n    &quot;test&quot;: {\n      &quot;dependsOn&quot;: [&quot;^build&quot;],\n      &quot;outputs&quot;: [&quot;coverage/**&quot;]\n    },\n    &quot;lint&quot;: {\n      &quot;outputs&quot;: []\n    },\n    &quot;type-check&quot;: {\n      &quot;dependsOn&quot;: [&quot;^build&quot;],\n      &quot;outputs&quot;: []\n    }\n  }\n}\n</code></pre>\n<p>The <code>dependsOn: [&quot;^build&quot;]</code> means &quot;build dependencies first.&quot; Turborepo handles the ordering automatically.</p>\n<h2>Shared Packages</h2>\n<h3>Domain Package</h3>\n<p>Business logic shared across all apps:</p>\n<pre><code class=\"language-typescript\">// packages/domain/src/entities/well-inspection.ts\nexport class WellInspection {\n  constructor(\n    public readonly id: string,\n    public wellId: string,\n    public inspectorId: string,\n    public inspectionDate: Date,\n    public status: InspectionStatus,\n    public findings: InspectionFinding[]\n  ) {}\n\n  addFinding(finding: InspectionFinding): void {\n    if (this.status === InspectionStatus.COMPLETED) {\n      throw new Error(&#39;Cannot add finding to completed inspection&#39;);\n    }\n    this.findings.push(finding);\n  }\n\n  // Same business logic in mobile, web, and API\n}\n</code></pre>\n<pre><code class=\"language-json\">// packages/domain/package.json\n{\n  &quot;name&quot;: &quot;@wellos/domain&quot;,\n  &quot;version&quot;: &quot;1.0.0&quot;,\n  &quot;main&quot;: &quot;./dist/index.js&quot;,\n  &quot;types&quot;: &quot;./dist/index.d.ts&quot;,\n  &quot;scripts&quot;: {\n    &quot;build&quot;: &quot;tsc&quot;,\n    &quot;dev&quot;: &quot;tsc --watch&quot;\n  },\n  &quot;dependencies&quot;: {\n    &quot;date-fns&quot;: &quot;^2.30.0&quot;\n  },\n  &quot;devDependencies&quot;: {\n    &quot;@wellos/tsconfig&quot;: &quot;*&quot;,\n    &quot;typescript&quot;: &quot;^5.0.0&quot;\n  }\n}\n</code></pre>\n<h3>UI Components Package</h3>\n<p>Shared React components:</p>\n<pre><code class=\"language-typescript\">// packages/ui/src/Button/Button.tsx\nexport interface ButtonProps {\n  variant?: &#39;primary&#39; | &#39;secondary&#39; | &#39;danger&#39;;\n  size?: &#39;sm&#39; | &#39;md&#39; | &#39;lg&#39;;\n  children: React.ReactNode;\n  onClick?: () =&gt; void;\n  disabled?: boolean;\n}\n\nexport const Button: React.FC\n    &lt;/TouchableOpacity&gt;\n  );\n};\n\nconst styles = StyleSheet.create({\n  button: { borderRadius: 8, padding: 12 },\n  primary: { backgroundColor: &#39;#007AFF&#39; },\n  secondary: { backgroundColor: &#39;#6B7280&#39; },\n  danger: { backgroundColor: &#39;#EF4444&#39; },\n  sm: { padding: 8 },\n  md: { padding: 12 },\n  lg: { padding: 16 },\n  text: { color: &#39;#fff&#39;, fontWeight: &#39;600&#39; },\n});\n</code></pre>\n<h3>Config Packages</h3>\n<p>Shared configuration:</p>\n<pre><code class=\"language-json\">// packages/tsconfig/base.json\n{\n  &quot;compilerOptions&quot;: {\n    &quot;target&quot;: &quot;ES2020&quot;,\n    &quot;lib&quot;: [&quot;ES2020&quot;],\n    &quot;module&quot;: &quot;commonjs&quot;,\n    &quot;moduleResolution&quot;: &quot;node&quot;,\n    &quot;esModuleInterop&quot;: true,\n    &quot;strict&quot;: true,\n    &quot;skipLibCheck&quot;: true,\n    &quot;forceConsistentCasingInFileNames&quot;: true,\n    &quot;declaration&quot;: true,\n    &quot;declarationMap&quot;: true,\n    &quot;sourceMap&quot;: true\n  }\n}\n\n// packages/tsconfig/react.json\n{\n  &quot;extends&quot;: &quot;./base.json&quot;,\n  &quot;compilerOptions&quot;: {\n    &quot;jsx&quot;: &quot;react-jsx&quot;,\n    &quot;lib&quot;: [&quot;ES2020&quot;, &quot;DOM&quot;, &quot;DOM.Iterable&quot;]\n  }\n}\n\n// packages/tsconfig/nextjs.json\n{\n  &quot;extends&quot;: &quot;./react.json&quot;,\n  &quot;compilerOptions&quot;: {\n    &quot;target&quot;: &quot;ES2017&quot;,\n    &quot;lib&quot;: [&quot;ES2020&quot;, &quot;DOM&quot;],\n    &quot;allowJs&quot;: true,\n    &quot;noEmit&quot;: true,\n    &quot;incremental&quot;: true,\n    &quot;moduleResolution&quot;: &quot;bundler&quot;,\n    &quot;resolveJsonModule&quot;: true,\n    &quot;isolatedModules&quot;: true\n  },\n  &quot;include&quot;: [&quot;next-env.d.ts&quot;, &quot;**/*.ts&quot;, &quot;**/*.tsx&quot;],\n  &quot;exclude&quot;: [&quot;node_modules&quot;]\n}\n</code></pre>\n<p>Apps extend these configs:</p>\n<pre><code class=\"language-json\">// apps/web-dashboard/tsconfig.json\n{\n  &quot;extends&quot;: &quot;@wellos/tsconfig/nextjs.json&quot;\n}\n</code></pre>\n<h2>Using Shared Packages in Apps</h2>\n<h3>In Next.js Apps</h3>\n<pre><code class=\"language-json\">// apps/web-dashboard/package.json\n{\n  &quot;name&quot;: &quot;@wellos/web-dashboard&quot;,\n  &quot;dependencies&quot;: {\n    &quot;@wellos/domain&quot;: &quot;*&quot;,\n    &quot;@wellos/ui&quot;: &quot;*&quot;,\n    &quot;@wellos/types&quot;: &quot;*&quot;,\n    &quot;@wellos/utils&quot;: &quot;*&quot;,\n    &quot;next&quot;: &quot;14.0.0&quot;,\n    &quot;react&quot;: &quot;^18.2.0&quot;\n  }\n}\n</code></pre>\n<pre><code class=\"language-typescript\">// apps/web-dashboard/components/InspectionList.tsx\nimport { Button } from &#39;@wellos/ui&#39;;\nimport { WellInspection } from &#39;@wellos/domain&#39;;\nimport { formatDate } from &#39;@wellos/utils&#39;;\n\nexport const InspectionList = ({ inspections }: { inspections: WellInspection[] }) =&gt; {\n  return (\n    &lt;div&gt;\n      {inspections.map((inspection) =&gt; (\n        &lt;div key={inspection.id}&gt;\n          &lt;h3&gt;{inspection.wellId}&lt;/h3&gt;\n          &lt;p&gt;{formatDate(inspection.inspectionDate)}&lt;/p&gt;\n          \n        &lt;/div&gt;\n      ))}\n    &lt;/div&gt;\n  );\n};\n</code></pre>\n<h3>In React Native Apps</h3>\n<pre><code class=\"language-json\">// apps/mobile-field/package.json\n{\n  &quot;name&quot;: &quot;@wellos/mobile-field&quot;,\n  &quot;dependencies&quot;: {\n    &quot;@wellos/domain&quot;: &quot;*&quot;,\n    &quot;@wellos/ui-native&quot;: &quot;*&quot;,\n    &quot;@wellos/types&quot;: &quot;*&quot;,\n    &quot;@wellos/utils&quot;: &quot;*&quot;,\n    &quot;react-native&quot;: &quot;0.72.0&quot;\n  }\n}\n</code></pre>\n<pre><code class=\"language-typescript\">// apps/mobile-field/screens/InspectionListScreen.tsx\nimport { Button } from &#39;@wellos/ui-native&#39;;\nimport { WellInspection } from &#39;@wellos/domain&#39;;\nimport { formatDate } from &#39;@wellos/utils&#39;;\n\nexport const InspectionListScreen = () =&gt; {\n  const inspections = useInspections();\n\n  return (\n    \n          \n          \n        &lt;/View&gt;\n      ))}\n    &lt;/View&gt;\n  );\n};\n</code></pre>\n<h2>Build Pipeline with Turborepo</h2>\n<p>Turborepo caches build outputs. If nothing changed, builds are instant:</p>\n<pre><code class=\"language-bash\">$ turbo run build\n\n# First build\npackages/domain:build: cache miss, executing\npackages/ui:build: cache miss, executing\napps/web-dashboard:build: cache miss, executing\n# ... takes 2 minutes\n\n$ turbo run build\n\n# Second build (nothing changed)\npackages/domain:build: cache hit, replaying output\npackages/ui:build: cache hit, replaying output\napps/web-dashboard:build: cache hit, replaying output\n# ... takes 2 seconds!\n</code></pre>\n<p>This works locally and in CI. Turborepo uses content-based hashing to detect changes.</p>\n<h3>Selective Builds</h3>\n<p>Build only what changed:</p>\n<pre><code class=\"language-bash\"># Build everything\nturbo run build\n\n# Build only affected apps\nturbo run build --filter=...@wellos/mobile-field\n\n# Build a single app and its dependencies\nturbo run build --filter=@wellos/web-dashboard\n</code></pre>\n<p>In CI, we use Git to build only changed apps:</p>\n<pre><code class=\"language-yaml\"># .github/workflows/build.yml\nname: Build\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-node@v3\n\n      - name: Install dependencies\n        run: npm install\n\n      - name: Build affected apps\n        run: npx turbo run build --filter=...[origin/main]\n</code></pre>\n<p><code>--filter=...[origin/main]</code> builds only apps that changed since <code>main</code>.</p>\n<h2>Development Workflow</h2>\n<h3>Running Multiple Apps</h3>\n<pre><code class=\"language-bash\"># Run all apps in dev mode\nturbo run dev\n\n# Run specific apps\nturbo run dev --filter=@wellos/mobile-field\nturbo run dev --filter=@wellos/web-dashboard --filter=@wellos/api\n</code></pre>\n<p>Turborepo runs them in parallel with streaming output.</p>\n<h3>Watch Mode for Packages</h3>\n<p>When developing shared packages, run in watch mode:</p>\n<pre><code class=\"language-bash\"># Terminal 1: Watch shared packages\ncd packages/domain\nnpm run dev\n\n# Terminal 2: Run the app\ncd apps/mobile-field\nnpm run dev\n</code></pre>\n<p>Changes to <code>packages/domain</code> automatically rebuild and hot reload in the app.</p>\n<h2>Version Management</h2>\n<h3>Internal Packages</h3>\n<p>Use workspace protocol:</p>\n<pre><code class=\"language-json\">{\n  &quot;dependencies&quot;: {\n    &quot;@wellos/domain&quot;: &quot;*&quot;  // Always use latest local version\n  }\n}\n</code></pre>\n<h3>External Dependencies</h3>\n<p>Keep versions synchronized with root package.json:</p>\n<pre><code class=\"language-json\">// package.json (root)\n{\n  &quot;devDependencies&quot;: {\n    &quot;typescript&quot;: &quot;^5.0.0&quot;,\n    &quot;eslint&quot;: &quot;^8.45.0&quot;\n  }\n}\n</code></pre>\n<p>Apps inherit these versions:</p>\n<pre><code class=\"language-json\">// apps/web-dashboard/package.json\n{\n  &quot;devDependencies&quot;: {\n    &quot;@wellos/tsconfig&quot;: &quot;*&quot;,\n    // typescript inherited from root\n  }\n}\n</code></pre>\n<h2>Testing in Monorepo</h2>\n<p>Run tests across all packages:</p>\n<pre><code class=\"language-bash\">turbo run test\n\n# With coverage\nturbo run test -- --coverage\n\n# Watch mode\nturbo run test -- --watch\n</code></pre>\n<p>Test shared packages independently:</p>\n<pre><code class=\"language-typescript\">// packages/domain/src/entities/__tests__/well-inspection.test.ts\nimport { WellInspection } from &#39;../well-inspection&#39;;\n\ndescribe(&#39;WellInspection&#39;, () =&gt; {\n  it(&#39;should not allow adding findings after completion&#39;, () =&gt; {\n    const inspection = new WellInspection(/*...*/);\n    inspection.complete();\n\n    expect(() =&gt; {\n      inspection.addFinding(/*...*/);\n    }).toThrow(&#39;Cannot add finding to completed inspection&#39;);\n  });\n});\n</code></pre>\n<p>Tests run in CI for affected packages only, saving time.</p>\n<h2>Challenges and Solutions</h2>\n<h3>Challenge 1: React Native Metro Bundler</h3>\n<p>Metro doesn&#39;t support monorepos out of the box. We needed custom configuration:</p>\n<pre><code class=\"language-javascript\">// apps/mobile-field/metro.config.js\nconst path = require(&#39;path&#39;);\n\nmodule.exports = {\n  projectRoot: __dirname,\n  watchFolders: [\n    path.resolve(__dirname, &#39;../../node_modules&#39;),\n    path.resolve(__dirname, &#39;../../packages&#39;),\n  ],\n  resolver: {\n    nodeModulesPaths: [\n      path.resolve(__dirname, &#39;node_modules&#39;),\n      path.resolve(__dirname, &#39;../../node_modules&#39;),\n    ],\n  },\n};\n</code></pre>\n<h3>Challenge 2: Build Order</h3>\n<p>Apps must build after packages. Turborepo handles this with <code>dependsOn</code>:</p>\n<pre><code class=\"language-json\">{\n  &quot;pipeline&quot;: {\n    &quot;build&quot;: {\n      &quot;dependsOn&quot;: [&quot;^build&quot;]  // ^ means dependencies\n    }\n  }\n}\n</code></pre>\n<h3>Challenge 3: Large node_modules</h3>\n<p>Hoisting dependencies to the root reduces duplication:</p>\n<pre><code># Before (without hoisting)\napps/mobile-field/node_modules/  # 500MB\napps/web-dashboard/node_modules/ # 500MB\n# Total: 1GB+\n\n# After (with hoisting)\nnode_modules/  # 600MB\napps/*/node_modules/  # ~50MB each\n# Total: ~800MB\n</code></pre>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Start with packages, not apps</strong>: Define shared packages first, then build apps on top</li>\n<li><strong>Keep packages small</strong>: Many small packages &gt; few large packages</li>\n<li><strong>Use Turborepo caching</strong>: Saves massive CI time</li>\n<li><strong>Watch mode is essential</strong>: Make package changes feel instant</li>\n<li><strong>Document internal APIs</strong>: Shared packages need docs just like external ones</li>\n</ol>\n<h2>When NOT to Use Monorepo</h2>\n<p>Monorepos aren&#39;t always the answer:</p>\n<ul>\n<li><strong>Independent release cycles</strong>: If apps deploy independently, separate repos might be better</li>\n<li><strong>Different teams</strong>: Cross-team monorepos require coordination</li>\n<li><strong>Massive scale</strong>: Google&#39;s monorepo works for Google, but 100K+ developers requires special tooling</li>\n</ul>\n<p>For WellOS (6 apps, 10 developers), monorepo was perfect.</p>\n<h2>Conclusion</h2>\n<p>Managing WellOS as a monorepo with Turborepo transformed our development experience. Code sharing is trivial, atomic changes across apps are possible, and builds are blazingly fast with caching.</p>\n<p>The key is structure: organize into apps and packages, use Turborepo for builds, and leverage workspace dependencies.</p>\n<p>After 27 years of managing codebases, monorepos are my preferred approach for multi-app projects with shared code. The initial setup investment pays dividends in developer productivity.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/monorepo-management-multi-app-projects/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-02-13 00:00:00",
            "updated_at": "2025-02-13 00:00:00",
            "published_at": "2025-02-13 00:00:00",
            "custom_excerpt": "Managing a monorepo with 6 integrated applications in the WellOS platform using Turborepo, shared packages, and effective build strategies for mobile and web apps."
          },
          {
            "id": "15",
            "title": "React Native Performance Optimization Techniques",
            "slug": "react-native-performance-optimization",
            "html": "<p>Building three React Native apps for WellOS taught me that mobile performance is unforgiving. A janky web app is annoying; a janky mobile app is unusable. Field workers in remote locations with older devices don&#39;t tolerate lag.</p>\n<p>After two years of optimizing React Native apps to run smoothly on devices from 2018, here are the techniques that made the biggest difference.</p>\n<h2>Profiling First: Measure Before Optimizing</h2>\n<p>Never optimize blindly. Use React Native&#39;s built-in profiler:</p>\n<pre><code class=\"language-javascript\">// Enable Flipper for debugging\n// Install: npm install --save-dev react-native-flipper\n\n// In your app\nimport { Platform } from &#39;react-native&#39;;\n\nif (__DEV__ &amp;&amp; Platform.OS === &#39;ios&#39;) {\n  require(&#39;./ReactotronConfig&#39;);\n}\n</code></pre>\n<p>Or use the Flashlight CLI tool:</p>\n<pre><code class=\"language-bash\">npx @perf-profiler/profiler measure\n</code></pre>\n<p>This shows:</p>\n<ul>\n<li>JavaScript thread FPS</li>\n<li>UI thread FPS</li>\n<li>Component render times</li>\n<li>Bridge traffic</li>\n</ul>\n<p>Find bottlenecks before fixing them.</p>\n<h2>List Optimization: The Biggest Performance Win</h2>\n<p>Lists are where most React Native apps slow down. We had inspection lists with 1000+ items—scrolling was unusable until we optimized.</p>\n<h3>Use FlatList, Not ScrollView</h3>\n<pre><code class=\"language-typescript\">// Bad: Renders all 1000 items immediately\n\n\n// Good: Renders only visible items\n\n      \n    &lt;/View&gt;\n  );\n};\n\n// Good: Only re-renders when inspection changes\nconst InspectionCard = React.memo(({ inspection }) =&gt; {\n  return (\n    \n      \n    &lt;/View&gt;\n  );\n}, (prevProps, nextProps) =&gt; {\n  // Custom comparison\n  return prevProps.inspection.id === nextProps.inspection.id &amp;&amp;\n         prevProps.inspection.status === nextProps.inspection.status;\n});\n</code></pre>\n<p>For complex items, memoization prevents unnecessary re-renders.</p>\n<h3>Use FlashList for Even Better Performance</h3>\n<p>Shopify&#39;s FlashList outperforms FlatList:</p>\n<pre><code class=\"language-bash\">npm install @shopify/flash-list\n</code></pre>\n<pre><code class=\"language-typescript\">import { FlashList } from &#39;@shopify/flash-list&#39;;\n\n\n  );\n};\n</code></pre>\n<p>Or use react-native-visibility-sensor.</p>\n<h2>Reduce JavaScript Bundle Size</h2>\n<p>Smaller bundles = faster startup.</p>\n<h3>Use Hermes Engine</h3>\n<p>Hermes compiles JavaScript to bytecode, reducing app size and improving startup:</p>\n<pre><code class=\"language-javascript\">// android/app/build.gradle\nproject.ext.react = [\n  enableHermes: true,  // Enable Hermes\n]\n\n// ios/Podfile\nuse_react_native!(\n  :hermes_enabled =&gt; true\n)\n</code></pre>\n<p>Hermes reduced our Android APK by 30% and improved startup time by 40%.</p>\n<h3>Code Splitting with React.lazy</h3>\n<pre><code class=\"language-typescript\">// Bad: Import everything upfront\nimport InspectionDetailScreen from &#39;./screens/InspectionDetailScreen&#39;;\nimport ReportScreen from &#39;./screens/ReportScreen&#39;;\n\n// Good: Lazy load screens\nconst InspectionDetailScreen = React.lazy(() =&gt; import(&#39;./screens/InspectionDetailScreen&#39;));\nconst ReportScreen = React.lazy(() =&gt; import(&#39;./screens/ReportScreen&#39;));\n\n// Use with Suspense\n}&gt;\n  \n&lt;/Suspense&gt;\n</code></pre>\n<p>Screens users don&#39;t visit immediately don&#39;t need to be in the initial bundle.</p>\n<h3>Remove Console Logs in Production</h3>\n<p>Console logs impact performance. Remove them:</p>\n<pre><code class=\"language-bash\">npm install babel-plugin-transform-remove-console --save-dev\n</code></pre>\n<pre><code class=\"language-javascript\">// babel.config.js\nmodule.exports = {\n  presets: [&#39;module:metro-react-native-babel-preset&#39;],\n  plugins: [\n    [&#39;transform-remove-console&#39;, { exclude: [&#39;error&#39;, &#39;warn&#39;] }],\n  ],\n};\n</code></pre>\n<p>This removes <code>console.log</code> but keeps <code>console.error</code> and <code>console.warn</code>.</p>\n<h2>Optimize Re-Renders</h2>\n<p>Unnecessary re-renders kill performance. Use React DevTools Profiler to find them.</p>\n<h3>Memoize Expensive Computations</h3>\n<pre><code class=\"language-typescript\">const InspectionList = ({ inspections }) =&gt; {\n  // Bad: Recalculates on every render\n  const completedCount = inspections.filter(i =&gt; i.status === &#39;completed&#39;).length;\n\n  // Good: Only recalculates when inspections change\n  const completedCount = useMemo(\n    () =&gt; inspections.filter(i =&gt; i.status === &#39;completed&#39;).length,\n    [inspections]\n  );\n\n  return ;\n};\n</code></pre>\n<h3>Stabilize Callbacks with useCallback</h3>\n<pre><code class=\"language-typescript\">const InspectionList = ({ inspections }) =&gt; {\n  // Bad: New function on every render\n  const handlePress = (id) =&gt; {\n    navigate(&#39;InspectionDetail&#39;, { id });\n  };\n\n  // Good: Stable function reference\n  const handlePress = useCallback((id) =&gt; {\n    navigate(&#39;InspectionDetail&#39;, { id });\n  }, [navigate]);\n\n  return (\n    \n        &lt;/TouchableOpacity&gt;\n      )}\n    /&gt;\n  );\n};\n</code></pre>\n<p>Without <code>useCallback</code>, <code>FlatList</code> re-renders all items because <code>handlePress</code> reference changes.</p>\n<h3>Use Context Wisely</h3>\n<p>Context causes re-renders of all consumers. Split contexts:</p>\n<pre><code class=\"language-typescript\">// Bad: Entire app re-renders when user or settings change\nconst AppContext = createContext({ user, settings, theme });\n\n// Good: Split contexts\nconst UserContext = createContext(user);\nconst SettingsContext = createContext(settings);\nconst ThemeContext = createContext(theme);\n\n// Components only re-render when their specific context changes\n</code></pre>\n<h2>Optimize Animations</h2>\n<p>Smooth 60 FPS animations are crucial for perceived performance.</p>\n<h3>Use Reanimated Instead of Animated</h3>\n<pre><code class=\"language-typescript\">// Bad: JavaScript thread animations (janky)\nimport { Animated } from &#39;react-native&#39;;\n\nconst opacity = new Animated.Value(0);\n\nAnimated.timing(opacity, {\n  toValue: 1,\n  duration: 300,\n  useNativeDriver: true,  // Helps, but still JavaScript-based\n}).start();\n\n// Good: Native thread animations (smooth)\nimport Animated, { useSharedValue, withTiming } from &#39;react-native-reanimated&#39;;\n\nconst opacity = useSharedValue(0);\n\nopacity.value = withTiming(1, { duration: 300 });\n</code></pre>\n<p>Reanimated 2 runs animations on the UI thread, not JavaScript thread, preventing drops during heavy JS work.</p>\n<h3>Always Use Native Driver When Possible</h3>\n<pre><code class=\"language-typescript\">Animated.timing(translateY, {\n  toValue: 100,\n  duration: 300,\n  useNativeDriver: true,  // Critical!\n}).start();\n</code></pre>\n<p><code>useNativeDriver: true</code> moves animation to native thread. Only works for transform and opacity, not layout properties.</p>\n<h2>Native Module Optimization</h2>\n<p>Some operations must be native for performance.</p>\n<h3>Move Heavy Computation to Native</h3>\n<p>We needed to process large datasets from SQLite. JavaScript was too slow:</p>\n<pre><code class=\"language-objective-c\">// ios/DataProcessor.m\n@implementation DataProcessor\n\nRCT_EXPORT_MODULE();\n\nRCT_EXPORT_METHOD(processInspections:(NSArray *)inspections\n                  resolver:(RCTPromiseResolveBlock)resolve\n                  rejecter:(RCTPromiseRejectBlock)reject)\n{\n  dispatch_async(dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_DEFAULT, 0), ^{\n    // Heavy processing in background thread\n    NSArray *processed = [self doHeavyProcessing:inspections];\n\n    dispatch_async(dispatch_get_main_queue(), ^{\n      resolve(processed);\n    });\n  });\n}\n\n@end\n</code></pre>\n<p>This reduced processing time from 5 seconds to 500ms.</p>\n<h3>Batch Bridge Calls</h3>\n<p>The React Native bridge is a bottleneck. Batch calls:</p>\n<pre><code class=\"language-typescript\">// Bad: 100 bridge calls\nfor (const inspection of inspections) {\n  await NativeModules.Database.save(inspection);\n}\n\n// Good: 1 bridge call\nawait NativeModules.Database.batchSave(inspections);\n</code></pre>\n<h2>Memory Management</h2>\n<p>Memory leaks cause crashes on low-end devices.</p>\n<h3>Clean Up Listeners</h3>\n<pre><code class=\"language-typescript\">useEffect(() =&gt; {\n  const subscription = NetInfo.addEventListener(state =&gt; {\n    setIsConnected(state.isConnected);\n  });\n\n  // Clean up!\n  return () =&gt; subscription();\n}, []);\n</code></pre>\n<p>Forgetting cleanup causes memory leaks.</p>\n<h3>Limit State in Large Lists</h3>\n<p>Don&#39;t store unnecessary state for list items:</p>\n<pre><code class=\"language-typescript\">// Bad: Storing expanded state for 1000 items\nconst [expandedIds, setExpandedIds] = useState&lt;Set&lt;string&gt;&gt;(new Set());\n\n// Good: Store only expanded items\nconst [expandedIds, setExpandedIds] = useState&lt;Set&lt;string&gt;&gt;(new Set());\n// Most items aren&#39;t expanded, so Set stays small\n</code></pre>\n<h2>Startup Optimization</h2>\n<p>First impression matters. Optimize app startup:</p>\n<h3>Defer Non-Critical Initialization</h3>\n<pre><code class=\"language-typescript\">const App = () =&gt; {\n  useEffect(() =&gt; {\n    // Render UI first, then initialize analytics\n    InteractionManager.runAfterInteractions(() =&gt; {\n      Analytics.init();\n      CrashReporting.init();\n    });\n  }, []);\n\n  return ;\n};\n</code></pre>\n<p><code>InteractionManager.runAfterInteractions</code> waits until UI is interactive.</p>\n<h3>Use Splash Screen Wisely</h3>\n<p>Keep splash screen visible while loading critical data:</p>\n<pre><code class=\"language-typescript\">import SplashScreen from &#39;react-native-splash-screen&#39;;\n\nconst App = () =&gt; {\n  const [ready, setReady] = useState(false);\n\n  useEffect(() =&gt; {\n    const initialize = async () =&gt; {\n      await loadCriticalData();\n      setReady(true);\n      SplashScreen.hide();\n    };\n\n    initialize();\n  }, []);\n\n  if (!ready) return null;\n\n  return ;\n};\n</code></pre>\n<h2>Performance Monitoring in Production</h2>\n<p>Use Sentry or Firebase Performance:</p>\n<pre><code class=\"language-typescript\">import * as Sentry from &#39;@sentry/react-native&#39;;\n\nSentry.init({\n  dsn: process.env.SENTRY_DSN,\n  tracesSampleRate: 0.2,\n});\n\n// Track screen load times\nconst InspectionDetailScreen = () =&gt; {\n  useEffect(() =&gt; {\n    const transaction = Sentry.startTransaction({\n      name: &#39;InspectionDetailScreen&#39;,\n      op: &#39;screen.load&#39;,\n    });\n\n    return () =&gt; {\n      transaction.finish();\n    };\n  }, []);\n\n  // ...\n};\n</code></pre>\n<p>This identifies slow screens in production.</p>\n<h2>Results</h2>\n<p>After applying these optimizations to WellOS:</p>\n<ul>\n<li><strong>FlatList scrolling</strong>: 60 FPS on all devices (was 15-30 FPS)</li>\n<li><strong>App startup</strong>: 1.2s → 0.6s</li>\n<li><strong>Bundle size</strong>: 40MB → 28MB (Android)</li>\n<li><strong>Memory usage</strong>: Reduced by 35%</li>\n<li><strong>Crash rate</strong>: Dropped from 2.5% to 0.3%</li>\n</ul>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Profile first</strong>: Don&#39;t guess, measure</li>\n<li><strong>Lists are critical</strong>: FlatList optimization gives biggest wins</li>\n<li><strong>Images are expensive</strong>: Optimize sizes, use FastImage</li>\n<li><strong>Hermes is essential</strong>: Enable it on both platforms</li>\n<li><strong>Test on real devices</strong>: Emulators lie about performance</li>\n</ol>\n<h2>Conclusion</h2>\n<p>React Native can be performant, but it requires discipline. Optimize lists, images, bundle size, and animations. Profile regularly, especially on low-end devices.</p>\n<p>After 27 years of development and two years optimizing React Native apps, I&#39;ve learned that mobile performance isn&#39;t optional—it&#39;s the difference between an app users love and one they uninstall.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/react-native-performance-optimization/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-01-24 00:00:00",
            "updated_at": "2025-01-24 00:00:00",
            "published_at": "2025-01-24 00:00:00",
            "custom_excerpt": "Proven strategies for optimizing React Native apps based on building WellOS mobile applications, covering rendering, list optimization, bundle size, and native module integration."
          },
          {
            "id": "16",
            "title": "27 years of Web Development: Key Lessons Learned",
            "slug": "25-years-web-development-lessons-learned",
            "html": "<p>In 1999, I wrote my first professional code: Classic ASP connecting to SQL Server, building an e-commerce site for OneTravel.com. Twenty-five years later, I&#39;m building React Native apps with offline-first architecture and GraphQL APIs. The technologies changed completely, but the fundamental lessons remain constant.</p>\n<p>Here&#39;s what 27 years of building software taught me.</p>\n<h2>Technology Lessons</h2>\n<h3>Lesson 1: Fundamentals Never Change</h3>\n<p>I&#39;ve learned JavaScript three times: ES5 (2009), ES6 (2015), and TypeScript (2020). I&#39;ve learned databases four times: SQL Server (1999), MySQL (2005), MongoDB (2012), PostgreSQL (2018).</p>\n<p>But the fundamentals—data structures, algorithms, system design, HTTP, databases, security—remain unchanged. A hash table works the same in 1999 and 2025. Understanding indexes mattered in SQL Server 7.0 and matters in PostgreSQL 16.</p>\n<p><strong>Invest in fundamentals, not frameworks.</strong> Frameworks come and go (remember Backbone.js? CoffeeScript? Gulp?). Fundamentals compound over decades.</p>\n<h3>Lesson 2: Boring Technology Wins</h3>\n<p>The most successful projects I&#39;ve built used proven, &quot;boring&quot; technology:</p>\n<ul>\n<li><strong>PostgreSQL</strong> over the latest NoSQL database</li>\n<li><strong>REST APIs</strong> over the newest RPC framework</li>\n<li><strong>Server-side rendering</strong> over complex client-side hydration</li>\n<li><strong>Monorepos</strong> over microservices (for most projects)</li>\n</ul>\n<p>New technology is exciting. Boring technology ships products and keeps you employed.</p>\n<h3>Lesson 3: Premature Optimization is Real</h3>\n<p>In 2003, I spent a month optimizing a query from 500ms to 50ms. Turns out, only 5 users ran that query per day. Those 2.25 seconds saved per day weren&#39;t worth 160 hours of engineering.</p>\n<p>Meanwhile, the login page—used by 10,000 users daily—loaded in 8 seconds. I should have optimized that.</p>\n<p><strong>Measure first, optimize second.</strong> Profile production, not your assumptions.</p>\n<h3>Lesson 4: Simple is Better Than Clever</h3>\n<p>Early in my career, I wrote &quot;clever&quot; code:</p>\n<pre><code class=\"language-javascript\">// 2005: &quot;Clever&quot; code I was proud of\nconst validate = (v) =&gt; !!(v &amp;&amp; v.match(/^\\w+@\\w+\\.\\w+$/));\n</code></pre>\n<p>Today, I write &quot;boring&quot; code:</p>\n<pre><code class=\"language-typescript\">// 2025: Boring code I&#39;m proud of\nfunction isValidEmail(email: string): boolean {\n  if (!email || typeof email !== &#39;string&#39;) {\n    return false;\n  }\n\n  const emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n  return emailRegex.test(email);\n}\n</code></pre>\n<p>The second is longer but clearer. Future me (and my teammates) can understand it instantly.</p>\n<p><strong>Code is read 10x more than written. Optimize for reading.</strong></p>\n<h3>Lesson 5: Tests Are About Confidence, Not Coverage</h3>\n<p>I&#39;ve seen projects with 95% test coverage that were still buggy, and projects with 60% coverage that were rock solid.</p>\n<p>Coverage percentage doesn&#39;t matter. What matters:</p>\n<ul>\n<li>Can you refactor without fear?</li>\n<li>Can you deploy confidently?</li>\n<li>Do tests catch real bugs?</li>\n</ul>\n<p><strong>Test the critical paths users take, not every edge case.</strong></p>\n<h3>Lesson 6: Architecture Matters More as Codebases Grow</h3>\n<p>A 1,000-line project doesn&#39;t need architecture. A 100,000-line project desperately needs it.</p>\n<p>Bad architecture shows up around 10,000 lines. Everything becomes harder: adding features, onboarding developers, debugging, testing. Technical debt compounds.</p>\n<p>Good architecture (Hexagonal, CQRS, DDD) pays dividends as codebases grow. Catalyst PSA hit remained maintainable because we invested in architecture early.</p>\n<p><strong>Architecture is an investment that pays off over years, not weeks.</strong></p>\n<h2>People and Process Lessons</h2>\n<h3>Lesson 7: Code Reviews Are About Knowledge Sharing</h3>\n<p>Early in my career, code reviews felt like judgment. Am I good enough? Will they find mistakes?</p>\n<p>Now I see them as knowledge sharing:</p>\n<ul>\n<li>Junior developers learn patterns</li>\n<li>Senior developers catch edge cases</li>\n<li>Everyone understands more of the codebase</li>\n<li>Team standards emerge organically</li>\n</ul>\n<p><strong>Best code review comment: &quot;I didn&#39;t know you could do it that way! Thanks for teaching me.&quot;</strong></p>\n<h3>Lesson 8: Communication &gt; Code Quality</h3>\n<p>I&#39;ve worked with brilliant developers who couldn&#39;t communicate. They wrote perfect code that no one understood, solved problems no one asked for, and frustrated every teammate.</p>\n<p>I&#39;ve also worked with good (not great) developers who communicated constantly. They asked clarifying questions, documented decisions, explained tradeoffs, and helped everyone ship faster.</p>\n<p><strong>The second group had more impact.</strong></p>\n<p>Clear communication—in PRs, docs, Slack, and meetings—multiplies your effectiveness.</p>\n<h3>Lesson 9: Estimates Are Guesses, and That&#39;s Okay</h3>\n<p>In 2007, I estimated a feature would take 2 weeks. It took 6 weeks. I felt terrible.</p>\n<p>In 2025, I estimate a feature will take 2 weeks. It takes 6 weeks. I don&#39;t feel terrible—I update the estimate and communicate why.</p>\n<p>Software is inherently uncertain. Hidden complexity, changing requirements, integration challenges—these are inevitable.</p>\n<p><strong>Estimate with ranges (1-3 weeks), update frequently, and communicate changes early.</strong></p>\n<h3>Lesson 10: Rewrites Usually Fail</h3>\n<p>I&#39;ve participated in four major rewrites:</p>\n<ul>\n<li><strong>2004</strong>: VB6 → .NET (failed, went back to VB6)</li>\n<li><strong>2009</strong>: Classic ASP → Ruby on Rails (failed after 18 months)</li>\n<li><strong>2013</strong>: PHP monolith → Node.js microservices (succeeded but took 3 years)</li>\n<li><strong>2019</strong>: Angular → React (succeeded in 6 months with incremental migration)</li>\n</ul>\n<p>The successful rewrites were incremental—gradually replacing the old system. The failed rewrites were big-bang—rewrite everything from scratch.</p>\n<p><strong>Strangler fig pattern &gt; big rewrite.</strong></p>\n<h3>Lesson 11: Production is the Hardest Environment</h3>\n<p>All my bugs looked fine in development. Most looked fine in staging. They only appeared in production:</p>\n<ul>\n<li>When 1,000 users hit the system simultaneously</li>\n<li>When the database has 10 million rows, not 1,000</li>\n<li>When cellular connectivity drops mid-request</li>\n<li>When users do things you never imagined</li>\n</ul>\n<p><strong>Production is where software lives. Monitor it obsessively.</strong></p>\n<h3>Lesson 12: Delete More Than You Add</h3>\n<p>The best code is no code. The best feature is a deleted feature.</p>\n<p>I&#39;ve spent weeks building features users never adopted. I&#39;ve maintained complex code paths used by 5 customers. I&#39;ve debugged systems where 40% of the code was dead.</p>\n<p><strong>Before adding, ask: Can we delete something instead? Can we simplify?</strong></p>\n<h2>Career Lessons</h2>\n<h3>Lesson 13: Specialists and Generalists Both Win</h3>\n<p>I&#39;ve been both:</p>\n<ul>\n<li><strong>Specialist (2005-2010)</strong>: Deep .NET expertise</li>\n<li><strong>Generalist (2010-2020)</strong>: Full-stack JavaScript, mobile, DevOps</li>\n<li><strong>Specialist again (2020-2025)</strong>: React Native and enterprise architecture</li>\n</ul>\n<p>Both paths work. Specialists command higher rates. Generalists have more opportunities.</p>\n<p><strong>Follow your curiosity, not a career plan.</strong></p>\n<h3>Lesson 14: Teaching Compounds Your Learning</h3>\n<p>I&#39;ve learned more from writing blog posts, mentoring junior developers, and giving conference talks than from building features.</p>\n<p>Teaching forces you to:</p>\n<ul>\n<li>Understand deeply (can&#39;t explain what you don&#39;t understand)</li>\n<li>Identify patterns (teaching requires generalization)</li>\n<li>Get feedback (students ask questions you never considered)</li>\n</ul>\n<p><strong>If you want to master something, teach it.</strong></p>\n<h3>Lesson 15: Side Projects Are Overrated</h3>\n<p>I&#39;ve built dozens of side projects. Most failed. A few made money. None changed my career as much as doing great work at my day job.</p>\n<p>The developers who got promoted, who built reputation, who got interesting opportunities—they weren&#39;t the ones with the most side projects. They were the ones who shipped great products at work and helped their teammates succeed.</p>\n<p><strong>Side projects are fun, but excellence at work matters more.</strong></p>\n<h3>Lesson 16: Network &gt; Portfolio</h3>\n<p>I&#39;ve gotten exactly zero jobs from my portfolio. I&#39;ve gotten every job from my network:</p>\n<ul>\n<li>Former coworkers who recommended me</li>\n<li>Conference connections who remembered me</li>\n<li>Open source contributors I helped</li>\n</ul>\n<p><strong>Your reputation with people you&#39;ve worked with is your true portfolio.</strong></p>\n<h3>Lesson 17: Imposter Syndrome Never Ends</h3>\n<p>In 1999, I thought: &quot;Once I&#39;m a senior developer, I&#39;ll feel confident.&quot;</p>\n<p>In 2010: &quot;Once I&#39;m an architect, I&#39;ll feel confident.&quot;</p>\n<p>In 2025: &quot;Once I&#39;m a CTO, I&#39;ll feel confident.&quot;</p>\n<p>Imposter syndrome doesn&#39;t go away with experience. Everyone—even 25-year veterans—feels it.</p>\n<p><strong>Accept it. Feel it. Ship anyway.</strong></p>\n<h2>Technical Philosophy Lessons</h2>\n<h3>Lesson 18: Build for Today, Design for Tomorrow</h3>\n<p>Don&#39;t over-engineer for scale you don&#39;t have. Also don&#39;t paint yourself into corners you can&#39;t escape.</p>\n<ul>\n<li>Use PostgreSQL (scales to millions of rows) instead of SQLite</li>\n<li>Use TypeScript (refactorable) instead of JavaScript</li>\n<li>Use interfaces/abstractions (swappable) instead of tight coupling</li>\n</ul>\n<p><strong>Build the simplest thing that works, but design it so you can grow.</strong></p>\n<h3>Lesson 19: Security is Everyone&#39;s Job</h3>\n<p>I&#39;ve seen security as an afterthought lead to:</p>\n<ul>\n<li>SQL injection vulnerabilities in production</li>\n<li>Exposed API keys in client-side code</li>\n<li>Unencrypted sensitive data</li>\n<li>Cross-tenant data leakage</li>\n</ul>\n<p>Security can&#39;t be &quot;the security team&#39;s problem.&quot; It must be baked into every PR, every feature, every deployment.</p>\n<p><strong>Security review is as important as code review.</strong></p>\n<h3>Lesson 20: Users Don&#39;t Care About Your Stack</h3>\n<p>I&#39;ve been excited about GraphQL, excited about React, excited about Kubernetes.</p>\n<p>Users don&#39;t care. They care about:</p>\n<ul>\n<li>Does it work?</li>\n<li>Is it fast?</li>\n<li>Can I accomplish my task?</li>\n</ul>\n<p><strong>Your job is solving user problems, not using cool technology.</strong></p>\n<h3>Lesson 21: Automate the Boring Stuff</h3>\n<p>In 2005, I manually deployed to production: FTP files, run SQL scripts, restart IIS. It took an hour and broke regularly.</p>\n<p>In 2025, I push to <code>main</code> and CI/CD deploys automatically in 5 minutes. Database migrations run automatically. Rollbacks are one click.</p>\n<p>The time I invested in CI/CD (maybe 20 hours) has saved thousands of hours and prevented countless deployment bugs.</p>\n<p><strong>Automate anything you do more than twice.</strong></p>\n<h3>Lesson 22: Monoliths Are Fine</h3>\n<p>Microservices are trendy. But I&#39;ve built successful products as monoliths and struggled with premature microservices.</p>\n<p>Unless you have Netflix&#39;s scale and team size, start with a monolith. It&#39;s:</p>\n<ul>\n<li>Easier to develop (one codebase)</li>\n<li>Easier to deploy (one service)</li>\n<li>Easier to debug (one log)</li>\n<li>Faster to build features (no network calls between services)</li>\n</ul>\n<p><strong>Monoliths scale farther than you think. Microservices scale teams, not technology.</strong></p>\n<h3>Lesson 23: Documentation is a Love Letter to Future You</h3>\n<p>I&#39;ve inherited codebases with zero documentation. I&#39;ve spent weeks reverse-engineering business logic that could have been explained in a paragraph.</p>\n<p>I&#39;ve also inherited codebases with great docs. I was productive on day one.</p>\n<p>Documentation isn&#39;t for users—it&#39;s for:</p>\n<ul>\n<li>Future you (in 6 months)</li>\n<li>Your teammates</li>\n<li>Your replacement</li>\n</ul>\n<p><strong>Document why, not what. The code shows what. The docs explain why.</strong></p>\n<h3>Lesson 24: Burnout is Real</h3>\n<p>I&#39;ve burned out twice:</p>\n<ul>\n<li><strong>2008</strong>: Working 70-hour weeks on a failing rewrite</li>\n<li><strong>2016</strong>: Maintaining a legacy system solo while building a replacement</li>\n</ul>\n<p>Both times, I thought: &quot;I just need to push through.&quot; Both times, I crashed hard—depression, health issues, months to recover.</p>\n<p><strong>Sustainable pace &gt; heroic sprints. Career is a marathon.</strong></p>\n<h3>Lesson 25: Software is About People</h3>\n<p>The best code I&#39;ve written doesn&#39;t matter if it doesn&#39;t help people. The perfect architecture doesn&#39;t matter if it doesn&#39;t ship.</p>\n<p>Software engineering is ultimately about:</p>\n<ul>\n<li>Solving user problems</li>\n<li>Enabling business goals</li>\n<li>Collaborating with teammates</li>\n<li>Building things that last</li>\n</ul>\n<p><strong>Technology is the tool. People are the purpose.</strong></p>\n<h2>What I&#39;d Tell My Younger Self</h2>\n<p>If I could go back to 1999 and give myself advice:</p>\n<ol>\n<li>Learn fundamentals, not frameworks</li>\n<li>Write simple code, not clever code</li>\n<li>Communicate more, code less</li>\n<li>Delete more, add less</li>\n<li>Build relationships, not portfolios</li>\n<li>Ship products, not perfect code</li>\n<li>Take care of yourself—it&#39;s a marathon</li>\n</ol>\n<p>But honestly, I probably wouldn&#39;t listen. Some lessons only stick when you learn them yourself.</p>\n<h2>Conclusion</h2>\n<p>Twenty-five years is a long time in technology. I&#39;ve seen languages rise and fall, paradigms shift, entire platforms emerge and disappear.</p>\n<p>But the core lessons remain: Build for users. Keep it simple. Invest in fundamentals. Communicate clearly. Take care of yourself and your team.</p>\n<p>Technology is the easy part. The hard part is everything else—people, process, product, priorities.</p>\n<p>Here&#39;s to the next 27 years. I have no idea what technology we&#39;ll be using, but I know the lessons will be the same.</p>\n<p><strong>The best time to start learning was 27 years ago. The second best time is today.</strong></p>\n",
            "feature_image": "https://jasoncochran.io/blog/25-years-web-development-lessons-learned/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-01-14 00:00:00",
            "updated_at": "2025-01-14 00:00:00",
            "published_at": "2025-01-14 00:00:00",
            "custom_excerpt": "Reflections on 27 years of professional software development from 1999 to 2025, covering technology shifts, architectural lessons, team dynamics, and what truly matters in building software."
          },
          {
            "id": "17",
            "title": "NFT Platform Development: Ethereum Integration Best Practices",
            "slug": "nft-platform-development-ethereum",
            "html": "<p>Building an NFT platform taught me that blockchain development is fundamentally different from traditional web development. Immutability, gas costs, and decentralization introduce constraints that require completely different architectural thinking.</p>\n<p>Here&#39;s everything I learned building a production NFT platform with Ethereum, including smart contract design, wallet integration, IPFS storage, and gas optimization.</p>\n<h2>NFT Platform Architecture</h2>\n<p>A production NFT platform has several components:</p>\n<ul>\n<li><strong>Smart Contracts</strong>: ERC-721/ERC-1155 token contracts on Ethereum</li>\n<li><strong>IPFS</strong>: Decentralized storage for metadata and images</li>\n<li><strong>Backend API</strong>: Indexing, caching, user management (Node.js/NestJS)</li>\n<li><strong>Frontend</strong>: Web3 wallet integration (React/Next.js)</li>\n<li><strong>Database</strong>: Off-chain data for fast queries (PostgreSQL)</li>\n</ul>\n<p>The key insight: you can&#39;t put everything on-chain. Use blockchain for ownership and transfers; use traditional tech for everything else.</p>\n<h2>Smart Contract Development</h2>\n<p>We used ERC-721 for unique NFTs with OpenZeppelin&#39;s battle-tested contracts:</p>\n<pre><code class=\"language-solidity\">// contracts/MyNFT.sol\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.20;\n\nimport &quot;@openzeppelin/contracts/token/ERC721/ERC721.sol&quot;;\nimport &quot;@openzeppelin/contracts/token/ERC721/extensions/ERC721URIStorage.sol&quot;;\nimport &quot;@openzeppelin/contracts/access/Ownable.sol&quot;;\nimport &quot;@openzeppelin/contracts/utils/Counters.sol&quot;;\n\ncontract MyNFT is ERC721, ERC721URIStorage, Ownable {\n    using Counters for Counters.Counter;\n    Counters.Counter private _tokenIds;\n\n    // Royalty info\n    address public royaltyRecipient;\n    uint256 public royaltyPercentage; // Basis points (e.g., 500 = 5%)\n\n    // Max supply\n    uint256 public constant MAX_SUPPLY = 10000;\n\n    // Mint price\n    uint256 public mintPrice = 0.05 ether;\n\n    // Events\n    event NFTMinted(address indexed to, uint256 indexed tokenId, string tokenURI);\n    event RoyaltyUpdated(address indexed recipient, uint256 percentage);\n\n    constructor(\n        string memory name,\n        string memory symbol,\n        address _royaltyRecipient,\n        uint256 _royaltyPercentage\n    ) ERC721(name, symbol) {\n        royaltyRecipient = _royaltyRecipient;\n        royaltyPercentage = _royaltyPercentage;\n    }\n\n    function mint(address to, string memory tokenURI) public payable returns (uint256) {\n        require(_tokenIds.current() &lt; MAX_SUPPLY, &quot;Max supply reached&quot;);\n        require(msg.value &gt;= mintPrice, &quot;Insufficient payment&quot;);\n\n        _tokenIds.increment();\n        uint256 newTokenId = _tokenIds.current();\n\n        _safeMint(to, newTokenId);\n        _setTokenURI(newTokenId, tokenURI);\n\n        emit NFTMinted(to, newTokenId, tokenURI);\n\n        return newTokenId;\n    }\n\n    function batchMint(\n        address[] memory recipients,\n        string[] memory tokenURIs\n    ) public payable onlyOwner {\n        require(recipients.length == tokenURIs.length, &quot;Array length mismatch&quot;);\n        require(\n            _tokenIds.current() + recipients.length &lt;= MAX_SUPPLY,\n            &quot;Exceeds max supply&quot;\n        );\n\n        for (uint256 i = 0; i &lt; recipients.length; i++) {\n            _tokenIds.increment();\n            uint256 newTokenId = _tokenIds.current();\n\n            _safeMint(recipients[i], newTokenId);\n            _setTokenURI(newTokenId, tokenURIs[i]);\n\n            emit NFTMinted(recipients[i], newTokenId, tokenURIs[i]);\n        }\n    }\n\n    function setMintPrice(uint256 newPrice) public onlyOwner {\n        mintPrice = newPrice;\n    }\n\n    function setRoyalty(address recipient, uint256 percentage) public onlyOwner {\n        require(percentage &lt;= 1000, &quot;Royalty too high&quot;); // Max 10%\n        royaltyRecipient = recipient;\n        royaltyPercentage = percentage;\n\n        emit RoyaltyUpdated(recipient, percentage);\n    }\n\n    // ERC-2981 royalty standard\n    function royaltyInfo(\n        uint256 tokenId,\n        uint256 salePrice\n    ) public view returns (address, uint256) {\n        uint256 royaltyAmount = (salePrice * royaltyPercentage) / 10000;\n        return (royaltyRecipient, royaltyAmount);\n    }\n\n    function withdraw() public onlyOwner {\n        uint256 balance = address(this).balance;\n        payable(owner()).transfer(balance);\n    }\n\n    function totalSupply() public view returns (uint256) {\n        return _tokenIds.current();\n    }\n\n    // Required overrides\n    function tokenURI(uint256 tokenId)\n        public\n        view\n        override(ERC721, ERC721URIStorage)\n        returns (string memory)\n    {\n        return super.tokenURI(tokenId);\n    }\n\n    function supportsInterface(bytes4 interfaceId)\n        public\n        view\n        override(ERC721, ERC721URIStorage)\n        returns (bool)\n    {\n        return super.supportsInterface(interfaceId);\n    }\n\n    function _burn(uint256 tokenId) internal override(ERC721, ERC721URIStorage) {\n        super._burn(tokenId);\n    }\n}\n</code></pre>\n<h2>IPFS Integration for Metadata</h2>\n<p>NFT metadata must be stored immutably. We used IPFS (InterPlanetary File System):</p>\n<pre><code class=\"language-typescript\">// infrastructure/ipfs/ipfs.service.ts\nimport { create, IPFSHTTPClient } from &#39;ipfs-http-client&#39;;\nimport { File } from &#39;buffer&#39;;\n\n@Injectable()\nexport class IPFSService {\n  private client: IPFSHTTPClient;\n\n  constructor() {\n    // Use Infura&#39;s IPFS gateway or run your own node\n    this.client = create({\n      host: &#39;ipfs.infura.io&#39;,\n      port: 5001,\n      protocol: &#39;https&#39;,\n      headers: {\n        authorization: `Basic ${Buffer.from(\n          process.env.INFURA_PROJECT_ID + &#39;:&#39; + process.env.INFURA_SECRET\n        ).toString(&#39;base64&#39;)}`,\n      },\n    });\n  }\n\n  async uploadImage(buffer: Buffer, filename: string): Promise&lt;string&gt; {\n    const file = {\n      path: filename,\n      content: buffer,\n    };\n\n    const result = await this.client.add(file);\n\n    // Return IPFS URL\n    return `ipfs://${result.cid}`;\n  }\n\n  async uploadMetadata(metadata: NFTMetadata): Promise&lt;string&gt; {\n    const json = JSON.stringify(metadata);\n\n    const result = await this.client.add(json);\n\n    return `ipfs://${result.cid}`;\n  }\n\n  async getMetadata(ipfsUrl: string): Promise&lt;NFTMetadata&gt; {\n    // Convert ipfs:// to https://\n    const cid = ipfsUrl.replace(&#39;ipfs://&#39;, &#39;&#39;);\n    const url = `https://ipfs.io/ipfs/${cid}`;\n\n    const response = await fetch(url);\n    return await response.json();\n  }\n}\n\n// NFT Metadata standard (ERC-721)\ninterface NFTMetadata {\n  name: string;\n  description: string;\n  image: string; // IPFS URL\n  attributes?: Array&lt;{\n    trait_type: string;\n    value: string | number;\n  }&gt;;\n  external_url?: string;\n}\n</code></pre>\n<p>Minting flow with IPFS:</p>\n<pre><code class=\"language-typescript\">// application/use-cases/mint-nft.use-case.ts\n@Injectable()\nexport class MintNFTUseCase {\n  constructor(\n    private readonly ipfs: IPFSService,\n    private readonly web3: Web3Service,\n    private readonly nftRepository: NFTRepository\n  ) {}\n\n  async execute(\n    userId: string,\n    imageFile: Buffer,\n    metadata: Partial&lt;NFTMetadata&gt;\n  ): Promise&lt;{ tokenId: number; transactionHash: string }&gt; {\n    // 1. Upload image to IPFS\n    const imageUrl = await this.ipfs.uploadImage(\n      imageFile,\n      `nft-${Date.now()}.png`\n    );\n\n    // 2. Create metadata with IPFS image URL\n    const fullMetadata: NFTMetadata = {\n      name: metadata.name!,\n      description: metadata.description!,\n      image: imageUrl,\n      attributes: metadata.attributes || [],\n    };\n\n    // 3. Upload metadata to IPFS\n    const metadataUrl = await this.ipfs.uploadMetadata(fullMetadata);\n\n    // 4. Mint NFT on blockchain\n    const user = await this.userRepository.findById(userId);\n    const result = await this.web3.mintNFT(user.walletAddress, metadataUrl);\n\n    // 5. Store in database for fast queries\n    await this.nftRepository.create({\n      tokenId: result.tokenId,\n      owner: user.walletAddress,\n      metadataUrl,\n      imageUrl,\n      name: metadata.name!,\n      transactionHash: result.transactionHash,\n    });\n\n    return result;\n  }\n}\n</code></pre>\n<h2>Web3 Service for Blockchain Interaction</h2>\n<p>Interacting with smart contracts from Node.js:</p>\n<pre><code class=\"language-typescript\">// infrastructure/web3/web3.service.ts\nimport { ethers } from &#39;ethers&#39;;\nimport contractABI from &#39;./MyNFT.json&#39;;\n\n@Injectable()\nexport class Web3Service {\n  private provider: ethers.providers.Provider;\n  private contract: ethers.Contract;\n  private wallet: ethers.Wallet;\n\n  constructor() {\n    // Connect to Ethereum (Infura, Alchemy, or local node)\n    this.provider = new ethers.providers.JsonRpcProvider(\n      process.env.ETH_RPC_URL\n    );\n\n    // Wallet for signing transactions\n    this.wallet = new ethers.Wallet(\n      process.env.PRIVATE_KEY!,\n      this.provider\n    );\n\n    // Contract instance\n    this.contract = new ethers.Contract(\n      process.env.NFT_CONTRACT_ADDRESS!,\n      contractABI.abi,\n      this.wallet\n    );\n  }\n\n  async mintNFT(\n    toAddress: string,\n    tokenURI: string\n  ): Promise&lt;{ tokenId: number; transactionHash: string }&gt; {\n    // Get mint price from contract\n    const mintPrice = await this.contract.mintPrice();\n\n    // Send transaction\n    const tx = await this.contract.mint(toAddress, tokenURI, {\n      value: mintPrice,\n      gasLimit: 300000, // Set appropriate gas limit\n    });\n\n    // Wait for confirmation\n    const receipt = await tx.wait();\n\n    // Parse event to get token ID\n    const event = receipt.events?.find((e: any) =&gt; e.event === &#39;NFTMinted&#39;);\n    const tokenId = event?.args?.tokenId.toNumber();\n\n    return {\n      tokenId,\n      transactionHash: receipt.transactionHash,\n    };\n  }\n\n  async getTokenURI(tokenId: number): Promise&lt;string&gt; {\n    return await this.contract.tokenURI(tokenId);\n  }\n\n  async getOwner(tokenId: number): Promise&lt;string&gt; {\n    return await this.contract.ownerOf(tokenId);\n  }\n\n  async getTotalSupply(): Promise&lt;number&gt; {\n    const supply = await this.contract.totalSupply();\n    return supply.toNumber();\n  }\n\n  async estimateGas(toAddress: string, tokenURI: string): Promise&lt;string&gt; {\n    const mintPrice = await this.contract.mintPrice();\n\n    const gasEstimate = await this.contract.estimateGas.mint(\n      toAddress,\n      tokenURI,\n      { value: mintPrice }\n    );\n\n    const gasPrice = await this.provider.getGasPrice();\n    const gasCost = gasEstimate.mul(gasPrice);\n\n    return ethers.utils.formatEther(gasCost);\n  }\n}\n</code></pre>\n<h2>Frontend Wallet Integration</h2>\n<p>Connect to user wallets with MetaMask:</p>\n<pre><code class=\"language-typescript\">// lib/web3.ts\nimport { ethers } from &#39;ethers&#39;;\n\nexport async function connectWallet(): Promise&lt;{\n  address: string;\n  provider: ethers.providers.Web3Provider;\n}&gt; {\n  if (!window.ethereum) {\n    throw new Error(&#39;MetaMask not installed&#39;);\n  }\n\n  const provider = new ethers.providers.Web3Provider(window.ethereum);\n\n  // Request account access\n  await provider.send(&#39;eth_requestAccounts&#39;, []);\n\n  const signer = provider.getSigner();\n  const address = await signer.getAddress();\n\n  return { address, provider };\n}\n\nexport async function mintNFT(\n  contractAddress: string,\n  tokenURI: string,\n  mintPrice: string\n): Promise&lt;{ transactionHash: string }&gt; {\n  const { provider } = await connectWallet();\n  const signer = provider.getSigner();\n\n  const contract = new ethers.Contract(\n    contractAddress,\n    contractABI,\n    signer\n  );\n\n  const tx = await contract.mint(await signer.getAddress(), tokenURI, {\n    value: ethers.utils.parseEther(mintPrice),\n  });\n\n  await tx.wait();\n\n  return { transactionHash: tx.hash };\n}\n</code></pre>\n<p>React component for minting:</p>\n<pre><code class=\"language-typescript\">const MintNFT = () =&gt; {\n  const [connected, setConnected] = useState(false);\n  const [address, setAddress] = useState(&#39;&#39;);\n  const [minting, setMinting] = useState(false);\n\n  const handleConnect = async () =&gt; {\n    try {\n      const { address } = await connectWallet();\n      setAddress(address);\n      setConnected(true);\n    } catch (error) {\n      toast.error(&#39;Failed to connect wallet&#39;);\n    }\n  };\n\n  const handleMint = async (imageFile: File, metadata: NFTMetadata) =&gt; {\n    setMinting(true);\n\n    try {\n      // 1. Upload to backend (which uploads to IPFS)\n      const formData = new FormData();\n      formData.append(&#39;image&#39;, imageFile);\n      formData.append(&#39;metadata&#39;, JSON.stringify(metadata));\n\n      const response = await fetch(&#39;/api/nft/prepare-mint&#39;, {\n        method: &#39;POST&#39;,\n        body: formData,\n      });\n\n      const { tokenURI, mintPrice } = await response.json();\n\n      // 2. Mint via wallet\n      const { transactionHash } = await mintNFT(\n        process.env.NEXT_PUBLIC_CONTRACT_ADDRESS!,\n        tokenURI,\n        mintPrice\n      );\n\n      // 3. Confirm on backend\n      await fetch(&#39;/api/nft/confirm-mint&#39;, {\n        method: &#39;POST&#39;,\n        headers: { &#39;Content-Type&#39;: &#39;application/json&#39; },\n        body: JSON.stringify({ transactionHash }),\n      });\n\n      toast.success(&#39;NFT minted successfully!&#39;);\n    } catch (error) {\n      toast.error(&#39;Minting failed&#39;);\n    } finally {\n      setMinting(false);\n    }\n  };\n\n  return (\n    &lt;div&gt;\n      {!connected ? (\n        &lt;button onClick={handleConnect}&gt;Connect Wallet&lt;/button&gt;\n      ) : (\n        &lt;div&gt;\n          &lt;p&gt;Connected: {address.slice(0, 6)}...{address.slice(-4)}&lt;/p&gt;\n          \n        &lt;/div&gt;\n      )}\n    &lt;/div&gt;\n  );\n};\n</code></pre>\n<h2>Gas Optimization Strategies</h2>\n<p>Gas costs can make or break an NFT platform. Here&#39;s how we optimized:</p>\n<h3>1. Batch Operations</h3>\n<p>Minting 100 NFTs individually vs. one batch:</p>\n<pre><code class=\"language-solidity\">// Bad: 100 separate transactions = ~$500 in gas\nfor (uint i = 0; i &lt; 100; i++) {\n    mint(recipients[i], tokenURIs[i]);\n}\n\n// Good: 1 transaction = ~$50 in gas\nfunction batchMint(\n    address[] memory recipients,\n    string[] memory tokenURIs\n) public {\n    for (uint256 i = 0; i &lt; recipients.length; i++) {\n        _mint(recipients[i], _tokenIds.current());\n        _setTokenURI(_tokenIds.current(), tokenURIs[i]);\n        _tokenIds.increment();\n    }\n}\n</code></pre>\n<h3>2. Storage Optimization</h3>\n<p>Use events for data that doesn&#39;t need on-chain queries:</p>\n<pre><code class=\"language-solidity\">// Bad: Expensive storage\nmapping(uint256 =&gt; string) public tokenDescriptions;\n\n// Good: Emit event, index off-chain\nevent TokenMinted(uint256 indexed tokenId, string description);\n</code></pre>\n<h3>3. Off-Chain Metadata</h3>\n<p>Store metadata on IPFS, not on-chain:</p>\n<pre><code class=\"language-solidity\">// Bad: Storing full metadata on-chain\nstruct NFTData {\n    string name;\n    string description;\n    string imageUrl;\n    string[] attributes;\n}\n\n// Good: Store only IPFS hash\nstring private _baseTokenURI = &quot;ipfs://Qm...&quot;;\n</code></pre>\n<h3>4. Use uint256 Efficiently</h3>\n<p>Pack variables to save storage slots:</p>\n<pre><code class=\"language-solidity\">// Bad: Each takes a storage slot\nuint256 public royaltyPercentage; // 1 slot\naddress public royaltyRecipient;  // 1 slot\n\n// Good: Packed into 1 slot\nstruct RoyaltyInfo {\n    address recipient; // 20 bytes\n    uint96 percentage; // 12 bytes\n} // Total: 32 bytes = 1 slot\n</code></pre>\n<h2>Indexing with The Graph</h2>\n<p>Query blockchain data efficiently with The Graph:</p>\n<pre><code class=\"language-graphql\"># schema.graphql\ntype NFT @entity {\n  id: ID!\n  tokenId: BigInt!\n  owner: Bytes!\n  metadataURI: String!\n  mintedAt: BigInt!\n  transactionHash: Bytes!\n}\n\ntype Transfer @entity {\n  id: ID!\n  tokenId: BigInt!\n  from: Bytes!\n  to: Bytes!\n  timestamp: BigInt!\n  transactionHash: Bytes!\n}\n</code></pre>\n<p>Query from frontend:</p>\n<pre><code class=\"language-typescript\">const GET_USER_NFTS = gql`\n  query GetUserNFTs($owner: Bytes!) {\n    nfts(where: { owner: $owner }) {\n      id\n      tokenId\n      metadataURI\n      mintedAt\n    }\n  }\n`;\n\nconst { data } = useQuery(GET_USER_NFTS, {\n  variables: { owner: userAddress.toLowerCase() },\n});\n</code></pre>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Gas costs are real</strong>: Every storage write costs money. Optimize aggressively.</li>\n<li><strong>Use IPFS for storage</strong>: Never store large data on-chain.</li>\n<li><strong>Testnet extensively</strong>: Mainnet mistakes are expensive and permanent.</li>\n<li><strong>Security audits are essential</strong>: Use OpenZeppelin, get audited before mainnet.</li>\n<li><strong>Index everything off-chain</strong>: Blockchain queries are slow; index to PostgreSQL/The Graph.</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Building NFT platforms requires mastering both Web3 and traditional web development. Use blockchain for ownership and transfers, IPFS for immutable storage, and traditional databases for fast queries.</p>\n<p>The key is understanding what goes on-chain (minimal, expensive) vs. off-chain (rich data, fast queries). After 27 years in software, blockchain taught me that constraints breed creativity—gas optimization and immutability force better architectural decisions.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/nft-platform-development-ethereum/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2025-01-07 00:00:00",
            "updated_at": "2025-01-07 00:00:00",
            "published_at": "2025-01-07 00:00:00",
            "custom_excerpt": "Building production NFT platforms with Ethereum smart contracts, IPFS storage, wallet integration, and gas optimization strategies from real-world implementation experience."
          },
          {
            "id": "18",
            "title": "Role-Based Access Control in Modern Web Apps",
            "slug": "role-based-access-control-modern-web-apps",
            "html": "<p>When we built Catalyst PSA, we needed granular access control: project managers can edit their projects, team members can log time, admins can do everything, and clients can view reports. Simple role checks wouldn&#39;t cut it.</p>\n<p>After building RBAC systems for multiple enterprise applications, here&#39;s a flexible approach that scales from simple to complex authorization requirements.</p>\n<h2>RBAC Fundamentals</h2>\n<p>Role-Based Access Control has three core concepts:</p>\n<ul>\n<li><strong>Users</strong>: People using the system</li>\n<li><strong>Roles</strong>: Named collections of permissions (Admin, Manager, Employee)</li>\n<li><strong>Permissions</strong>: Specific actions (create_project, edit_timesheet, view_reports)</li>\n</ul>\n<h2>Database Schema</h2>\n<pre><code class=\"language-sql\">-- Users\nCREATE TABLE users (\n  id UUID PRIMARY KEY,\n  email VARCHAR(255) UNIQUE NOT NULL,\n  name VARCHAR(255) NOT NULL\n);\n\n-- Roles\nCREATE TABLE roles (\n  id UUID PRIMARY KEY,\n  name VARCHAR(100) UNIQUE NOT NULL,\n  description TEXT\n);\n\n-- Permissions\nCREATE TABLE permissions (\n  id UUID PRIMARY KEY,\n  name VARCHAR(100) UNIQUE NOT NULL,\n  resource VARCHAR(100),  -- e.g., &#39;project&#39;, &#39;timesheet&#39;\n  action VARCHAR(100)      -- e.g., &#39;create&#39;, &#39;read&#39;, &#39;update&#39;, &#39;delete&#39;\n);\n\n-- Role has many permissions\nCREATE TABLE role_permissions (\n  role_id UUID REFERENCES roles(id) ON DELETE CASCADE,\n  permission_id UUID REFERENCES permissions(id) ON DELETE CASCADE,\n  PRIMARY KEY (role_id, permission_id)\n);\n\n-- User has many roles (per tenant for multi-tenancy)\nCREATE TABLE user_roles (\n  user_id UUID REFERENCES users(id) ON DELETE CASCADE,\n  role_id UUID REFERENCES roles(id) ON DELETE CASCADE,\n  tenant_id UUID NOT NULL,\n  PRIMARY KEY (user_id, role_id, tenant_id)\n);\n</code></pre>\n<h2>Seed Data: Standard Roles and Permissions</h2>\n<pre><code class=\"language-typescript\">// scripts/seed-rbac.ts\nasync function seedRBAC() {\n  // Create permissions\n  const permissions = await db.insert(permissionsTable).values([\n    // Project permissions\n    { name: &#39;projects:create&#39;, resource: &#39;project&#39;, action: &#39;create&#39; },\n    { name: &#39;projects:read&#39;, resource: &#39;project&#39;, action: &#39;read&#39; },\n    { name: &#39;projects:update&#39;, resource: &#39;project&#39;, action: &#39;update&#39; },\n    { name: &#39;projects:delete&#39;, resource: &#39;project&#39;, action: &#39;delete&#39; },\n\n    // Timesheet permissions\n    { name: &#39;timesheets:create&#39;, resource: &#39;timesheet&#39;, action: &#39;create&#39; },\n    { name: &#39;timesheets:read&#39;, resource: &#39;timesheet&#39;, action: &#39;read&#39; },\n    { name: &#39;timesheets:update&#39;, resource: &#39;timesheet&#39;, action: &#39;update&#39; },\n    { name: &#39;timesheets:approve&#39;, resource: &#39;timesheet&#39;, action: &#39;approve&#39; },\n\n    // Report permissions\n    { name: &#39;reports:view&#39;, resource: &#39;report&#39;, action: &#39;view&#39; },\n    { name: &#39;reports:export&#39;, resource: &#39;report&#39;, action: &#39;export&#39; },\n\n    // User management\n    { name: &#39;users:create&#39;, resource: &#39;user&#39;, action: &#39;create&#39; },\n    { name: &#39;users:update&#39;, resource: &#39;user&#39;, action: &#39;update&#39; },\n    { name: &#39;users:delete&#39;, resource: &#39;user&#39;, action: &#39;delete&#39; },\n  ]).returning();\n\n  // Create roles\n  const adminRole = await db.insert(rolesTable).values({\n    name: &#39;admin&#39;,\n    description: &#39;Full system access&#39;,\n  }).returning();\n\n  const managerRole = await db.insert(rolesTable).values({\n    name: &#39;manager&#39;,\n    description: &#39;Manage projects and approve timesheets&#39;,\n  }).returning();\n\n  const employeeRole = await db.insert(rolesTable).values({\n    name: &#39;employee&#39;,\n    description: &#39;Log time and view assigned projects&#39;,\n  }).returning();\n\n  // Assign permissions to roles\n  await db.insert(rolePermissionsTable).values([\n    // Admin gets all permissions\n    ...permissions.map(p =&gt; ({\n      roleId: adminRole[0].id,\n      permissionId: p.id,\n    })),\n\n    // Manager gets project and timesheet permissions\n    { roleId: managerRole[0].id, permissionId: findPermission(&#39;projects:create&#39;).id },\n    { roleId: managerRole[0].id, permissionId: findPermission(&#39;projects:read&#39;).id },\n    { roleId: managerRole[0].id, permissionId: findPermission(&#39;projects:update&#39;).id },\n    { roleId: managerRole[0].id, permissionId: findPermission(&#39;timesheets:read&#39;).id },\n    { roleId: managerRole[0].id, permissionId: findPermission(&#39;timesheets:approve&#39;).id },\n    { roleId: managerRole[0].id, permissionId: findPermission(&#39;reports:view&#39;).id },\n\n    // Employee gets basic permissions\n    { roleId: employeeRole[0].id, permissionId: findPermission(&#39;projects:read&#39;).id },\n    { roleId: employeeRole[0].id, permissionId: findPermission(&#39;timesheets:create&#39;).id },\n    { roleId: employeeRole[0].id, permissionId: findPermission(&#39;timesheets:read&#39;).id },\n  ]);\n}\n</code></pre>\n<h2>Authorization Service</h2>\n<pre><code class=\"language-typescript\">// infrastructure/auth/authorization.service.ts\n@Injectable()\nexport class AuthorizationService {\n  constructor(private db: DrizzleService) {}\n\n  async getUserPermissions(userId: string, tenantId: string): Promise&lt;string[]&gt; {\n    const result = await this.db\n      .select({\n        permissionName: permissions.name,\n      })\n      .from(userRoles)\n      .innerJoin(roles, eq(userRoles.roleId, roles.id))\n      .innerJoin(rolePermissions, eq(roles.id, rolePermissions.roleId))\n      .innerJoin(permissions, eq(rolePermissions.permissionId, permissions.id))\n      .where(\n        and(\n          eq(userRoles.userId, userId),\n          eq(userRoles.tenantId, tenantId)\n        )\n      );\n\n    return result.map(r =&gt; r.permissionName);\n  }\n\n  async hasPermission(\n    userId: string,\n    tenantId: string,\n    permission: string\n  ): Promise&lt;boolean&gt; {\n    const permissions = await this.getUserPermissions(userId, tenantId);\n    return permissions.includes(permission);\n  }\n\n  async hasAnyPermission(\n    userId: string,\n    tenantId: string,\n    requiredPermissions: string[]\n  ): Promise&lt;boolean&gt; {\n    const permissions = await this.getUserPermissions(userId, tenantId);\n    return requiredPermissions.some(p =&gt; permissions.includes(p));\n  }\n\n  async hasAllPermissions(\n    userId: string,\n    tenantId: string,\n    requiredPermissions: string[]\n  ): Promise&lt;boolean&gt; {\n    const permissions = await this.getUserPermissions(userId, tenantId);\n    return requiredPermissions.every(p =&gt; permissions.includes(p));\n  }\n}\n</code></pre>\n<h2>Permission Guards (NestJS)</h2>\n<pre><code class=\"language-typescript\">// guards/permissions.guard.ts\n@Injectable()\nexport class PermissionsGuard implements CanActivate {\n  constructor(\n    private reflector: Reflector,\n    private authz: AuthorizationService\n  ) {}\n\n  async canActivate(context: ExecutionContext): Promise&lt;boolean&gt; {\n    const requiredPermissions = this.reflector.get&lt;string[]&gt;(\n      &#39;permissions&#39;,\n      context.getHandler()\n    );\n\n    if (!requiredPermissions) {\n      return true;\n    }\n\n    const request = context.switchToHttp().getRequest();\n    const { userId, tenantId } = request.user;\n\n    return await this.authz.hasAllPermissions(\n      userId,\n      tenantId,\n      requiredPermissions\n    );\n  }\n}\n\n// Decorator\nexport const RequirePermissions = (...permissions: string[]) =&gt;\n  SetMetadata(&#39;permissions&#39;, permissions);\n\n// Usage in controller\n@Controller(&#39;projects&#39;)\nexport class ProjectsController {\n  @Post()\n  @UseGuards(JwtAuthGuard, PermissionsGuard)\n  @RequirePermissions(&#39;projects:create&#39;)\n  async create(@Body() dto: CreateProjectDTO) {\n    return await this.projectsService.create(dto);\n  }\n\n  @Put(&#39;:id&#39;)\n  @UseGuards(JwtAuthGuard, PermissionsGuard)\n  @RequirePermissions(&#39;projects:update&#39;)\n  async update(@Param(&#39;id&#39;) id: string, @Body() dto: UpdateProjectDTO) {\n    return await this.projectsService.update(id, dto);\n  }\n}\n</code></pre>\n<h2>Resource-Level Authorization</h2>\n<p>Role permissions aren&#39;t enough. Users should only access resources they own or are assigned to.</p>\n<pre><code class=\"language-typescript\">// domain/policies/project.policy.ts\nexport class ProjectPolicy {\n  canView(user: User, project: Project): boolean {\n    return (\n      // Admin can view all\n      user.hasPermission(&#39;projects:read&#39;) &amp;&amp;\n      (user.isAdmin ||\n        // Project manager can view\n        project.managerId === user.id ||\n        // Team member can view\n        project.teamMemberIds.includes(user.id))\n    );\n  }\n\n  canUpdate(user: User, project: Project): boolean {\n    return (\n      user.hasPermission(&#39;projects:update&#39;) &amp;&amp;\n      (user.isAdmin || project.managerId === user.id)\n    );\n  }\n\n  canDelete(user: User, project: Project): boolean {\n    return user.hasPermission(&#39;projects:delete&#39;) &amp;&amp; user.isAdmin;\n  }\n}\n\n// Use in service\n@Injectable()\nexport class ProjectsService {\n  constructor(\n    private repository: ProjectRepository,\n    private policy: ProjectPolicy\n  ) {}\n\n  async findById(userId: string, projectId: string): Promise\n      )}\n\n      {canDelete &amp;&amp; (\n        \n      )}\n    &lt;/div&gt;\n  );\n};\n</code></pre>\n<h2>Route-Level Protection</h2>\n<pre><code class=\"language-typescript\">// components/ProtectedRoute.tsx\nexport const ProtectedRoute = ({\n  children,\n  permissions,\n}: {\n  children: React.ReactNode;\n  permissions: string[];\n}) =&gt; {\n  const { hasAllPermissions } = usePermissions();\n\n  if (!hasAllPermissions(permissions)) {\n    return ;\n  }\n\n  return &lt;&gt;{children}&lt;/&gt;;\n};\n\n// App routing\n\n    }\n  /&gt;\n\n  \n    }\n  /&gt;\n&lt;/Routes&gt;\n</code></pre>\n<h2>Hierarchical Roles</h2>\n<p>Some roles inherit from others:</p>\n<pre><code class=\"language-sql\">CREATE TABLE role_hierarchy (\n  parent_role_id UUID REFERENCES roles(id),\n  child_role_id UUID REFERENCES roles(id),\n  PRIMARY KEY (parent_role_id, child_role_id)\n);\n\n-- Admin inherits from Manager\nINSERT INTO role_hierarchy (parent_role_id, child_role_id)\nVALUES (admin_role_id, manager_role_id);\n\n-- Manager inherits from Employee\nINSERT INTO role_hierarchy (parent_role_id, child_role_id)\nVALUES (manager_role_id, employee_role_id);\n</code></pre>\n<pre><code class=\"language-typescript\">async getUserPermissions(userId: string, tenantId: string): Promise&lt;string[]&gt; {\n  const result = await this.db.execute(sql`\n    WITH RECURSIVE role_tree AS (\n      -- Direct roles\n      SELECT role_id FROM user_roles\n      WHERE user_id = ${userId} AND tenant_id = ${tenantId}\n\n      UNION\n\n      -- Inherited roles\n      SELECT rh.child_role_id\n      FROM role_hierarchy rh\n      INNER JOIN role_tree rt ON rt.role_id = rh.parent_role_id\n    )\n    SELECT DISTINCT p.name\n    FROM role_tree rt\n    INNER JOIN role_permissions rp ON rt.role_id = rp.role_id\n    INNER JOIN permissions p ON rp.permission_id = p.id\n  `);\n\n  return result.map(r =&gt; r.name);\n}\n</code></pre>\n<h2>Attribute-Based Access Control (ABAC)</h2>\n<p>Sometimes role isn&#39;t enough. Consider attributes:</p>\n<pre><code class=\"language-typescript\">export class TimeEntryPolicy {\n  canApprove(user: User, timeEntry: TimeEntry, project: Project): boolean {\n    return (\n      user.hasPermission(&#39;timesheets:approve&#39;) &amp;&amp;\n      (user.isAdmin ||\n        (project.managerId === user.id &amp;&amp; // Is project manager\n         timeEntry.userId !== user.id)) // Can&#39;t approve own entries\n    );\n  }\n\n  canEdit(user: User, timeEntry: TimeEntry): boolean {\n    const isOwner = timeEntry.userId === user.id;\n    const isNotApproved = timeEntry.status !== &#39;approved&#39;;\n    const isRecent = isWithinDays(timeEntry.date, 7);\n\n    return (\n      user.hasPermission(&#39;timesheets:update&#39;) &amp;&amp;\n      isOwner &amp;&amp;\n      isNotApproved &amp;&amp;\n      isRecent\n    );\n  }\n}\n</code></pre>\n<h2>Dynamic Permissions</h2>\n<p>Allow creating custom permissions at runtime:</p>\n<pre><code class=\"language-typescript\">@Post(&#39;permissions&#39;)\n@RequirePermissions(&#39;permissions:create&#39;)\nasync createPermission(@Body() dto: CreatePermissionDTO) {\n  const permission = await this.db.insert(permissions).values({\n    name: dto.name,\n    resource: dto.resource,\n    action: dto.action,\n    description: dto.description,\n  });\n\n  return permission;\n}\n\n// Assign to role\n@Post(&#39;roles/:roleId/permissions&#39;)\n@RequirePermissions(&#39;roles:update&#39;)\nasync assignPermission(\n  @Param(&#39;roleId&#39;) roleId: string,\n  @Body() dto: { permissionId: string }\n) {\n  await this.db.insert(rolePermissions).values({\n    roleId,\n    permissionId: dto.permissionId,\n  });\n}\n</code></pre>\n<h2>Caching Permissions</h2>\n<p>Don&#39;t query permissions on every request. Cache them in JWT or Redis:</p>\n<pre><code class=\"language-typescript\">// Store permissions in JWT\nconst token = jwt.sign(\n  {\n    userId: user.id,\n    tenantId: user.tenantId,\n    permissions: await authz.getUserPermissions(user.id, user.tenantId),\n  },\n  process.env.JWT_SECRET\n);\n\n// Or cache in Redis\nawait redis.setex(\n  `permissions:${userId}:${tenantId}`,\n  3600, // 1 hour\n  JSON.stringify(permissions)\n);\n</code></pre>\n<p>Invalidate cache when roles/permissions change:</p>\n<pre><code class=\"language-typescript\">@Put(&#39;users/:userId/roles&#39;)\nasync updateUserRoles(@Param(&#39;userId&#39;) userId: string, @Body() dto: UpdateRolesDTO) {\n  await this.userRolesService.update(userId, dto.roleIds);\n\n  // Invalidate permission cache\n  await redis.del(`permissions:${userId}:*`);\n}\n</code></pre>\n<h2>Audit Logging</h2>\n<p>Track permission changes:</p>\n<pre><code class=\"language-sql\">CREATE TABLE audit_log (\n  id UUID PRIMARY KEY,\n  user_id UUID,\n  action VARCHAR(100),\n  resource VARCHAR(100),\n  resource_id UUID,\n  timestamp TIMESTAMP DEFAULT NOW(),\n  details JSONB\n);\n</code></pre>\n<pre><code class=\"language-typescript\">async function logAudit(\n  userId: string,\n  action: string,\n  resource: string,\n  resourceId: string,\n  details: any\n) {\n  await db.insert(auditLog).values({\n    userId,\n    action,\n    resource,\n    resourceId,\n    details,\n  });\n}\n\n// Usage\nawait this.projectsService.update(projectId, dto);\n\nawait logAudit(user.id, &#39;update&#39;, &#39;project&#39;, projectId, {\n  changes: dto,\n});\n</code></pre>\n<h2>Testing RBAC</h2>\n<pre><code class=\"language-typescript\">describe(&#39;ProjectsController&#39;, () =&gt; {\n  it(&#39;should allow admin to delete any project&#39;, async () =&gt; {\n    const admin = await createUser({ roles: [&#39;admin&#39;] });\n\n    const response = await request(app)\n      .delete(`/projects/${projectId}`)\n      .set(&#39;Authorization&#39;, `Bearer ${admin.token}`);\n\n    expect(response.status).toBe(200);\n  });\n\n  it(&#39;should allow manager to delete own project&#39;, async () =&gt; {\n    const manager = await createUser({ roles: [&#39;manager&#39;] });\n    const project = await createProject({ managerId: manager.id });\n\n    const response = await request(app)\n      .delete(`/projects/${project.id}`)\n      .set(&#39;Authorization&#39;, `Bearer ${manager.token}`);\n\n    expect(response.status).toBe(200);\n  });\n\n  it(&#39;should prevent employee from deleting project&#39;, async () =&gt; {\n    const employee = await createUser({ roles: [&#39;employee&#39;] });\n\n    const response = await request(app)\n      .delete(`/projects/${projectId}`)\n      .set(&#39;Authorization&#39;, `Bearer ${employee.token}`);\n\n    expect(response.status).toBe(403);\n  });\n});\n</code></pre>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Permissions over roles</strong>: Check permissions, not role names</li>\n<li><strong>Resource-level auth</strong>: Role permissions + ownership checks</li>\n<li><strong>Cache permissions</strong>: Don&#39;t query database on every request</li>\n<li><strong>Frontend checks are UX</strong>: Backend checks are security</li>\n<li><strong>Audit everything</strong>: Track who did what</li>\n<li><strong>Test thoroughly</strong>: Authorization bugs are security bugs</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Building flexible RBAC systems requires balancing simplicity with granularity. Start with roles and permissions, add resource-level policies as needed, and always enforce authorization on the backend.</p>\n<p>After 27 years of building authorization systems, I&#39;ve learned that good RBAC is invisible—users can do what they should, can&#39;t do what they shouldn&#39;t, and never think about it.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/role-based-access-control-modern-web-apps/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-12-17 00:00:00",
            "updated_at": "2024-12-17 00:00:00",
            "published_at": "2024-12-17 00:00:00",
            "custom_excerpt": "Implementing flexible RBAC systems with roles, permissions, and resource-level access control for enterprise applications, based on building authorization for Catalyst PSA."
          },
          {
            "id": "19",
            "title": "GraphQL vs REST: When to Use Each",
            "slug": "graphql-vs-rest-when-to-use-each",
            "html": "<p>After building both GraphQL and REST APIs for the Catalyst PSA platform, I&#39;ve learned that the &quot;GraphQL vs REST&quot; debate misses the point. They&#39;re tools for different jobs. The question isn&#39;t which is better—it&#39;s which fits your specific use case.</p>\n<p>Here&#39;s what 27 years of API development and hands-on experience with both approaches taught me about when to use each.</p>\n<h2>REST for Project Management, GraphQL for Reporting</h2>\n<p>In Catalyst PSA, we used both in the same application:</p>\n<ul>\n<li><strong>REST APIs</strong>: CRUD operations on projects, tasks, time entries</li>\n<li><strong>GraphQL API</strong>: Complex reporting dashboard fetching data across multiple entities</li>\n</ul>\n<p>This hybrid approach gave us the simplicity of REST where it made sense and the flexibility of GraphQL where we needed it.</p>\n<h2>Understanding the Fundamental Difference</h2>\n<h3>REST: Resource-Oriented</h3>\n<p>REST organizes endpoints around resources:</p>\n<pre><code>GET    /api/projects\nGET    /api/projects/123\nPOST   /api/projects\nPUT    /api/projects/123\nDELETE /api/projects/123\n\nGET    /api/projects/123/tasks\nGET    /api/projects/123/time-entries\n</code></pre>\n<p>Each endpoint returns a fixed structure. Want different fields? Create a new endpoint.</p>\n<h3>GraphQL: Query-Oriented</h3>\n<p>GraphQL has one endpoint and flexible queries:</p>\n<pre><code class=\"language-graphql\">query {\n  project(id: &quot;123&quot;) {\n    id\n    name\n    client {\n      name\n      industry\n    }\n    tasks {\n      id\n      name\n      status\n    }\n  }\n}\n</code></pre>\n<p>Clients specify exactly what they need. Same endpoint, different data shapes.</p>\n<h2>When REST Wins</h2>\n<h3>1. Simple CRUD Operations</h3>\n<p>For basic create/read/update/delete, REST is simpler:</p>\n<pre><code class=\"language-typescript\">// REST endpoint - straightforward\n@Post(&#39;projects&#39;)\nasync createProject(@Body() dto: CreateProjectDTO) {\n  return await this.projectService.create(dto);\n}\n\n@Get(&#39;projects/:id&#39;)\nasync getProject(@Param(&#39;id&#39;) id: string) {\n  return await this.projectService.findById(id);\n}\n</code></pre>\n<p>Compare to GraphQL:</p>\n<pre><code class=\"language-typescript\">// GraphQL - more boilerplate\n@Resolver(() =&gt; Project)\nexport class ProjectResolver {\n  @Mutation(() =&gt; Project)\n  async createProject(@Args(&#39;input&#39;) input: CreateProjectInput) {\n    return await this.projectService.create(input);\n  }\n\n  @Query(() =&gt; Project)\n  async project(@Args(&#39;id&#39;) id: string) {\n    return await this.projectService.findById(id);\n  }\n\n  // Need resolver for every field that requires loading\n  @ResolveField(() =&gt; Client)\n  async client(@Parent() project: Project) {\n    return await this.clientService.findById(project.clientId);\n  }\n}\n</code></pre>\n<p>For CRUD, GraphQL adds complexity without benefit.</p>\n<h3>2. Caching</h3>\n<p>HTTP caching works beautifully with REST:</p>\n<pre><code class=\"language-typescript\">@Get(&#39;projects/:id&#39;)\n@Header(&#39;Cache-Control&#39;, &#39;public, max-age=300&#39;)\nasync getProject(@Param(&#39;id&#39;) id: string) {\n  return await this.projectService.findById(id);\n}\n</code></pre>\n<p>CDNs, browsers, and proxies cache GET requests automatically. GraphQL typically uses POST, bypassing HTTP caches.</p>\n<h3>3. File Uploads</h3>\n<p>REST handles file uploads naturally:</p>\n<pre><code class=\"language-typescript\">@Post(&#39;projects/:id/attachments&#39;)\n@UseInterceptors(FileInterceptor(&#39;file&#39;))\nasync uploadAttachment(\n  @Param(&#39;id&#39;) id: string,\n  @UploadedFile() file: Express.Multer.File\n) {\n  return await this.attachmentService.create(id, file);\n}\n</code></pre>\n<p>GraphQL requires multipart requests with special handling—doable but awkward.</p>\n<h3>4. Rate Limiting</h3>\n<p>REST endpoints are easy to rate limit:</p>\n<pre><code class=\"language-typescript\">@Get(&#39;projects&#39;)\n@UseGuards(RateLimitGuard)\n@RateLimit({ points: 100, duration: 60 })\nasync getProjects() {\n  return await this.projectService.findAll();\n}\n</code></pre>\n<p>With GraphQL, how do you rate limit? By operation complexity? Number of fields? It&#39;s more complex.</p>\n<h3>5. Monitoring and Debugging</h3>\n<p>REST endpoints show up clearly in logs:</p>\n<pre><code>GET /api/projects/123 - 200 - 45ms\nPOST /api/projects - 201 - 120ms\n</code></pre>\n<p>GraphQL logs all look the same:</p>\n<pre><code>POST /graphql - 200 - ???ms\n</code></pre>\n<p>You need to parse the query to know what actually happened.</p>\n<h2>When GraphQL Wins</h2>\n<h3>1. Complex Data Fetching</h3>\n<p>Our reporting dashboard needed data from 10+ entities. With REST:</p>\n<pre><code class=\"language-typescript\">// Client makes 10+ requests\nconst project = await fetch(&#39;/api/projects/123&#39;);\nconst client = await fetch(`/api/clients/${project.clientId}`);\nconst tasks = await fetch(&#39;/api/projects/123/tasks&#39;);\nconst timeEntries = await fetch(&#39;/api/projects/123/time-entries&#39;);\nconst team = await fetch(&#39;/api/projects/123/team-members&#39;);\n// ... 6 more requests\n\n// Or create a special endpoint\nconst dashboard = await fetch(&#39;/api/projects/123/dashboard&#39;);\n// But then you need different endpoints for different dashboards\n</code></pre>\n<p>With GraphQL:</p>\n<pre><code class=\"language-graphql\">query ProjectDashboard($id: ID!) {\n  project(id: $id) {\n    id\n    name\n    budget\n    client {\n      name\n      industry\n    }\n    tasks {\n      id\n      name\n      status\n      assignee {\n        name\n        avatar\n      }\n    }\n    timeEntries(last: 10) {\n      hours\n      description\n      date\n    }\n    teamMembers {\n      user {\n        name\n        role\n      }\n      allocation\n    }\n  }\n}\n</code></pre>\n<p>One request, exactly the data needed. GraphQL shines here.</p>\n<h3>2. Mobile Apps with Limited Bandwidth</h3>\n<p>Mobile apps need to minimize requests. GraphQL lets them fetch exactly what the UI needs:</p>\n<pre><code class=\"language-graphql\"># Mobile list view - just names and status\nquery ProjectList {\n  projects {\n    id\n    name\n    status\n  }\n}\n\n# Desktop detail view - full data\nquery ProjectDetail($id: ID!) {\n  project(id: $id) {\n    id\n    name\n    description\n    status\n    budget\n    # ... all fields\n  }\n}\n</code></pre>\n<p>Same endpoint, different payloads for different clients.</p>\n<h3>3. Evolving APIs</h3>\n<p>Adding fields to GraphQL is backward-compatible:</p>\n<pre><code class=\"language-graphql\">type Project {\n  id: ID!\n  name: String!\n  # Add new field - old queries still work\n  estimatedHours: Int\n}\n</code></pre>\n<p>REST versioning is harder:</p>\n<pre><code>GET /api/v1/projects - old clients\nGET /api/v2/projects - new clients with estimatedHours\n</code></pre>\n<h3>4. Nested Data Structures</h3>\n<p>When data is naturally hierarchical:</p>\n<pre><code class=\"language-graphql\">query OrgStructure {\n  organization {\n    departments {\n      name\n      teams {\n        name\n        members {\n          name\n          role\n        }\n      }\n    }\n  }\n}\n</code></pre>\n<p>With REST, you&#39;d need recursive requests or a huge single endpoint.</p>\n<h2>Performance Considerations</h2>\n<h3>The N+1 Problem in GraphQL</h3>\n<p>GraphQL&#39;s flexibility creates performance pitfalls:</p>\n<pre><code class=\"language-graphql\">query {\n  projects {\n    id\n    name\n    client {  # N+1! Loads client for each project\n      name\n    }\n  }\n}\n</code></pre>\n<p>Without DataLoader, this makes N+1 database queries. Solution:</p>\n<pre><code class=\"language-typescript\">@Resolver(() =&gt; Project)\nexport class ProjectResolver {\n  constructor(private dataLoader: DataLoaderService) {}\n\n  @ResolveField(() =&gt; Client)\n  async client(@Parent() project: Project) {\n    // DataLoader batches and caches\n    return await this.dataLoader.load(&#39;Client&#39;, project.clientId);\n  }\n}\n\n// DataLoader implementation\nexport class DataLoaderService {\n  private loaders = new Map();\n\n  load(type: string, id: string) {\n    if (!this.loaders.has(type)) {\n      this.loaders.set(type, new DataLoader(async (ids) =&gt; {\n        // Batch load all IDs at once\n        return await this.loadMany(type, ids);\n      }));\n    }\n\n    return this.loaders.get(type).load(id);\n  }\n}\n</code></pre>\n<p>REST doesn&#39;t have this problem—each endpoint is optimized explicitly.</p>\n<h3>Query Complexity</h3>\n<p>Malicious GraphQL queries can DOS your server:</p>\n<pre><code class=\"language-graphql\">query {\n  projects {\n    tasks {\n      timeEntries {\n        user {\n          projects {\n            tasks {\n              # Infinite depth!\n            }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>\n<p>You need query complexity limits:</p>\n<pre><code class=\"language-typescript\">import { GraphQLModule } from &#39;@nestjs/graphql&#39;;\nimport { ApolloDriver } from &#39;@nestjs/apollo&#39;;\n\n@Module({\n  imports: [\n    GraphQLModule.forRoot({\n      driver: ApolloDriver,\n      validationRules: [\n        depthLimit(5),\n        createComplexityLimitRule(1000),\n      ],\n    }),\n  ],\n})\nexport class AppModule {}\n</code></pre>\n<p>REST endpoints are bounded by design.</p>\n<h2>Developer Experience</h2>\n<h3>REST: Easier to Start</h3>\n<p>REST is intuitive. Any developer can understand:</p>\n<pre><code>GET /users/123\n</code></pre>\n<p>GraphQL has a learning curve. Schema definition language, resolvers, fragments, mutations vs queries—there&#39;s more to learn.</p>\n<h3>GraphQL: Better Tooling</h3>\n<p>Once you&#39;re up to speed, GraphQL tooling is excellent:</p>\n<ul>\n<li><strong>GraphQL Playground</strong>: Interactive API explorer</li>\n<li><strong>Schema introspection</strong>: Self-documenting</li>\n<li><strong>Type generation</strong>: Auto-generate TypeScript types from schema</li>\n</ul>\n<pre><code class=\"language-typescript\">// Auto-generated from GraphQL schema\nexport interface ProjectQuery {\n  project: {\n    id: string;\n    name: string;\n    client: {\n      name: string;\n    };\n  };\n}\n</code></pre>\n<p>REST requires manual documentation (OpenAPI) and type definitions.</p>\n<h2>Our Hybrid Approach in Catalyst PSA</h2>\n<p>We used both:</p>\n<h3>REST for:</h3>\n<ul>\n<li>User authentication (<code>POST /auth/login</code>)</li>\n<li>CRUD operations (<code>GET/POST/PUT/DELETE /projects</code>)</li>\n<li>File uploads (<code>POST /attachments</code>)</li>\n<li>Webhooks (<code>POST /webhooks/stripe</code>)</li>\n</ul>\n<h3>GraphQL for:</h3>\n<ul>\n<li>Dashboards (complex aggregations)</li>\n<li>Reporting (flexible queries)</li>\n<li>Mobile API (minimize bandwidth)</li>\n<li>Public API (let partners query flexibly)</li>\n</ul>\n<p>Implementation:</p>\n<pre><code class=\"language-typescript\">@Module({\n  imports: [\n    // REST controllers\n    ProjectModule,\n    TaskModule,\n    TimeEntryModule,\n\n    // GraphQL API\n    GraphQLModule.forRoot({\n      driver: ApolloDriver,\n      autoSchemaFile: true,\n      playground: true,\n    }),\n  ],\n})\nexport class AppModule {}\n</code></pre>\n<p>Both approaches coexist peacefully.</p>\n<h2>Migration Strategy</h2>\n<p>If you&#39;re migrating from REST to GraphQL (or vice versa), do it gradually:</p>\n<h3>Phase 1: Run Both</h3>\n<pre><code class=\"language-typescript\">// Keep existing REST\n@Get(&#39;projects&#39;)\nasync getProjects() { ... }\n\n// Add GraphQL alongside\n@Query(() =&gt; [Project])\nasync projects() { ... }\n</code></pre>\n<h3>Phase 2: Migrate Clients Incrementally</h3>\n<pre><code class=\"language-typescript\">// Old clients use REST\nGET /api/projects\n\n// New clients use GraphQL\nPOST /graphql\n  query { projects { id name } }\n</code></pre>\n<h3>Phase 3: Deprecate (Eventually)</h3>\n<p>Once all clients migrated, remove the old API. But there&#39;s no rush—both can coexist.</p>\n<h2>Decision Framework</h2>\n<p><strong>Use REST when:</strong></p>\n<ul>\n<li>Building simple CRUD APIs</li>\n<li>HTTP caching is important</li>\n<li>Simplicity &gt; flexibility</li>\n<li>Uploading files</li>\n<li>Public APIs with unpredictable usage</li>\n</ul>\n<p><strong>Use GraphQL when:</strong></p>\n<ul>\n<li>Fetching complex, nested data</li>\n<li>Supporting multiple clients (mobile, web, desktop)</li>\n<li>Bandwidth is limited</li>\n<li>Rapid frontend iteration (UI changes don&#39;t require backend changes)</li>\n<li>Schema evolution is important</li>\n</ul>\n<p><strong>Use both when:</strong></p>\n<ul>\n<li>You have both simple CRUD and complex queries</li>\n<li>Different clients have different needs</li>\n<li>You&#39;re migrating between approaches</li>\n</ul>\n<h2>Conclusion</h2>\n<p>After building both REST and GraphQL APIs in production, I&#39;ve learned that it&#39;s not either/or. They solve different problems.</p>\n<p>REST excels at simple, cacheable, resource-oriented APIs. GraphQL excels at complex data fetching with flexible clients.</p>\n<p>In Catalyst PSA, using both gave us the best of both worlds: simple CRUD with REST, flexible reporting with GraphQL.</p>\n<p>After 27 years of API development, I can say confidently: pick the right tool for the job, and don&#39;t be afraid to use both.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/graphql-vs-rest-when-to-use-each/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-12-02 00:00:00",
            "updated_at": "2024-12-02 00:00:00",
            "published_at": "2024-12-02 00:00:00",
            "custom_excerpt": "A practical comparison of GraphQL and REST APIs based on real-world experience building both for enterprise applications, including performance, complexity, and use case guidance."
          },
          {
            "id": "20",
            "title": "VoIP Integration: Building Telecom Solutions with Node.js",
            "slug": "voip-integration-nodejs",
            "html": "<p>During my years at Bravo Telecom and TeleFlip, I built VoIP systems that handled thousands of concurrent calls, integrated with legacy PBX systems, and provided real-time call analytics. Building telecom software taught me that voice communication has unique challenges that most web developers never encounter.</p>\n<p>Here&#39;s everything I learned about building VoIP systems with Node.js.</p>\n<h2>Understanding VoIP Fundamentals</h2>\n<p>VoIP (Voice over IP) converts voice into data packets sent over IP networks. The core protocol is SIP (Session Initiation Protocol), which handles call setup, management, and teardown.</p>\n<h3>Key Components</h3>\n<ul>\n<li><strong>SIP Server</strong>: Handles call signaling (Asterisk, FreeSWITCH, Kamailio)</li>\n<li><strong>Media Server</strong>: Processes audio streams (RTP/SRTP)</li>\n<li><strong>Application Server</strong>: Business logic (Node.js)</li>\n<li><strong>Database</strong>: CDRs (Call Detail Records), user data</li>\n</ul>\n<h3>The Call Flow</h3>\n<pre><code>Caller -&gt; SIP Client -&gt; SIP Server -&gt; Application Server (Node.js)\n                                   -&gt; Routes call\n                                   -&gt; Media Server (plays IVR, records)\n                                   -&gt; Routes to Agent\n</code></pre>\n<h2>Building a Node.js VoIP Application Server</h2>\n<p>We used Node.js to interface with Asterisk via AMI (Asterisk Manager Interface) and AGI (Asterisk Gateway Interface):</p>\n<pre><code class=\"language-typescript\">// infrastructure/asterisk/ami-client.ts\nimport { EventEmitter } from &#39;events&#39;;\nimport * as net from &#39;net&#39;;\n\nexport class AsteriskAMI extends EventEmitter {\n  private client: net.Socket;\n  private connected = false;\n\n  constructor(\n    private host: string,\n    private port: number,\n    private username: string,\n    private password: string\n  ) {\n    super();\n    this.connect();\n  }\n\n  private connect(): void {\n    this.client = net.createConnection(this.port, this.host);\n\n    this.client.on(&#39;connect&#39;, () =&gt; {\n      this.login();\n    });\n\n    this.client.on(&#39;data&#39;, (data) =&gt; {\n      this.handleData(data.toString());\n    });\n\n    this.client.on(&#39;error&#39;, (error) =&gt; {\n      console.error(&#39;AMI connection error:&#39;, error);\n      this.reconnect();\n    });\n  }\n\n  private async login(): Promise&lt;void&gt; {\n    this.send({\n      Action: &#39;Login&#39;,\n      Username: this.username,\n      Secret: this.password,\n    });\n  }\n\n  private send(command: Record&lt;string, string&gt;): void {\n    const message =\n      Object.entries(command)\n        .map(([key, value]) =&gt; `${key}: ${value}`)\n        .join(&#39;\\r\\n&#39;) + &#39;\\r\\n\\r\\n&#39;;\n\n    this.client.write(message);\n  }\n\n  async originate(\n    channel: string,\n    context: string,\n    exten: string,\n    priority: number,\n    callerID: string\n  ): Promise&lt;void&gt; {\n    this.send({\n      Action: &#39;Originate&#39;,\n      Channel: channel,\n      Context: context,\n      Exten: exten,\n      Priority: priority.toString(),\n      CallerID: callerID,\n    });\n  }\n\n  async hangup(channel: string): Promise&lt;void&gt; {\n    this.send({\n      Action: &#39;Hangup&#39;,\n      Channel: channel,\n    });\n  }\n\n  private handleData(data: string): void {\n    const lines = data.split(&#39;\\r\\n&#39;);\n    const event: Record&lt;string, string&gt; = {};\n\n    for (const line of lines) {\n      const match = line.match(/^([^:]+):\\s*(.*)$/);\n      if (match) {\n        event[match[1]] = match[2];\n      }\n    }\n\n    if (event.Event) {\n      this.emit(event.Event, event);\n    }\n  }\n\n  private reconnect(): void {\n    setTimeout(() =&gt; {\n      this.connect();\n    }, 5000);\n  }\n}\n</code></pre>\n<h2>Implementing Click-to-Call</h2>\n<p>A common VoIP feature: user clicks a button on a web page, and their phone rings to connect them to a customer:</p>\n<pre><code class=\"language-typescript\">// application/use-cases/initiate-call.use-case.ts\n@Injectable()\nexport class InitiateCallUseCase {\n  constructor(\n    private readonly ami: AsteriskAMI,\n    private readonly callRepository: CallRepository,\n    private readonly websocket: WebSocketService\n  ) {}\n\n  async execute(\n    agentId: string,\n    customerPhone: string\n  ): Promise&lt;{ callId: string }&gt; {\n    const agent = await this.agentRepository.findById(agentId);\n\n    if (!agent) {\n      throw new NotFoundException(&#39;Agent not found&#39;);\n    }\n\n    // Create call record\n    const call = Call.create({\n      agentId: agent.id,\n      customerPhone,\n      status: CallStatus.INITIATING,\n      direction: CallDirection.OUTBOUND,\n    });\n\n    await this.callRepository.save(call);\n\n    // Originate call through Asterisk\n    // This rings the agent&#39;s phone first\n    await this.ami.originate(\n      `SIP/${agent.extension}`, // Ring agent&#39;s phone\n      &#39;outbound-calls&#39;, // Dialplan context\n      customerPhone, // When agent answers, dial customer\n      1, // Priority\n      customerPhone // Caller ID shown to agent\n    );\n\n    // Notify agent via WebSocket\n    this.websocket.sendToUser(agent.id, &#39;call-initiated&#39;, {\n      callId: call.id,\n      customerPhone,\n    });\n\n    return { callId: call.id };\n  }\n}\n</code></pre>\n<p>Client-side implementation:</p>\n<pre><code class=\"language-typescript\">const ClickToCallButton = ({ customerPhone }) =&gt; {\n  const [calling, setCalling] = useState(false);\n  const socket = useWebSocket();\n\n  useEffect(() =&gt; {\n    socket.on(&#39;call-initiated&#39;, ({ callId, customerPhone }) =&gt; {\n      toast.success(`Calling ${customerPhone}...`);\n      setCalling(true);\n    });\n\n    socket.on(&#39;call-answered&#39;, ({ callId }) =&gt; {\n      toast.success(&#39;Call connected&#39;);\n      setCalling(false);\n    });\n\n    socket.on(&#39;call-ended&#39;, ({ callId }) =&gt; {\n      setCalling(false);\n    });\n  }, [socket]);\n\n  const handleCall = async () =&gt; {\n    await api.post(&#39;/calls/initiate&#39;, { customerPhone });\n  };\n\n  return (\n    &lt;button\n      onClick={handleCall}\n      disabled={calling}\n      className=&quot;btn btn-primary&quot;\n    &gt;\n      {calling ? &#39;Calling...&#39; : `Call ${customerPhone}`}\n    &lt;/button&gt;\n  );\n};\n</code></pre>\n<h2>Building an IVR (Interactive Voice Response) System</h2>\n<p>IVR systems play messages and collect DTMF input (keypad presses):</p>\n<pre><code class=\"language-typescript\">// application/ivr/ivr-controller.ts\nexport class IVRController {\n  constructor(\n    private readonly agi: AGIServer,\n    private readonly ttsService: TextToSpeechService\n  ) {\n    this.setupRoutes();\n  }\n\n  private setupRoutes(): void {\n    // Main menu\n    this.agi.on(&#39;main-menu&#39;, async (context) =&gt; {\n      const choice = await context.getOption(\n        &#39;/var/lib/asterisk/sounds/main-menu.wav&#39;,\n        &#39;123&#39;, // Accept digits 1, 2, or 3\n        5000 // 5 second timeout\n      );\n\n      switch (choice) {\n        case &#39;1&#39;:\n          await context.goto(&#39;sales-queue&#39;);\n          break;\n        case &#39;2&#39;:\n          await context.goto(&#39;support-queue&#39;);\n          break;\n        case &#39;3&#39;:\n          await context.goto(&#39;billing-queue&#39;);\n          break;\n        default:\n          await context.goto(&#39;main-menu&#39;); // Replay menu\n      }\n    });\n\n    // Sales queue\n    this.agi.on(&#39;sales-queue&#39;, async (context) =&gt; {\n      await context.say(&#39;Connecting you to sales&#39;);\n\n      // Get caller ID\n      const callerNumber = context.request.callerid;\n\n      // Look up customer\n      const customer = await this.customerRepository.findByPhone(\n        callerNumber\n      );\n\n      // Add to queue with priority\n      await context.queueCall(&#39;sales&#39;, {\n        priority: customer?.isPremium ? 10 : 1,\n        timeout: 300,\n        announcePosition: true,\n      });\n    });\n  }\n}\n\n// AGI Server implementation\nexport class AGIServer extends EventEmitter {\n  private server: net.Server;\n\n  constructor(port: number) {\n    super();\n    this.server = net.createServer(this.handleConnection.bind(this));\n    this.server.listen(port);\n  }\n\n  private handleConnection(socket: net.Socket): void {\n    const context = new AGIContext(socket);\n\n    context.on(&#39;ready&#39;, (request) =&gt; {\n      // Route based on extension\n      this.emit(request.extension, context);\n    });\n  }\n}\n\nclass AGIContext extends EventEmitter {\n  private request: any = {};\n\n  constructor(private socket: net.Socket) {\n    super();\n    this.readRequest();\n  }\n\n  private async readRequest(): Promise&lt;void&gt; {\n    // Parse AGI environment variables\n    // Format: &quot;agi_variable: value&quot;\n    const readline = require(&#39;readline&#39;);\n    const rl = readline.createInterface({ input: this.socket });\n\n    for await (const line of rl) {\n      if (line === &#39;&#39;) break; // Empty line marks end of request\n\n      const match = line.match(/^agi_(\\w+):\\s*(.*)$/);\n      if (match) {\n        this.request[match[1]] = match[2];\n      }\n    }\n\n    this.emit(&#39;ready&#39;, this.request);\n  }\n\n  async streamFile(filename: string): Promise&lt;void&gt; {\n    return this.sendCommand(`STREAM FILE ${filename} &quot;&quot;`);\n  }\n\n  async getOption(\n    filename: string,\n    digits: string,\n    timeout: number\n  ): Promise&lt;string&gt; {\n    const response = await this.sendCommand(\n      `GET OPTION ${filename} &quot;${digits}&quot; ${timeout}`\n    );\n\n    // Parse response: &quot;200 result=49&quot; (49 is ASCII for &#39;1&#39;)\n    const match = response.match(/result=(\\d+)/);\n    if (match &amp;&amp; match[1] !== &#39;0&#39;) {\n      return String.fromCharCode(parseInt(match[1]));\n    }\n\n    return &#39;&#39;;\n  }\n\n  async say(text: string): Promise&lt;void&gt; {\n    // Use TTS to generate audio\n    const audioFile = await this.ttsService.generate(text);\n    await this.streamFile(audioFile);\n  }\n\n  async queueCall(\n    queueName: string,\n    options: {\n      priority?: number;\n      timeout?: number;\n      announcePosition?: boolean;\n    }\n  ): Promise&lt;void&gt; {\n    const opts = [\n      `priority=${options.priority || 0}`,\n      `timeout=${options.timeout || 300}`,\n      options.announcePosition ? &#39;announce-position=yes&#39; : &#39;&#39;,\n    ].join(&#39;,&#39;);\n\n    await this.sendCommand(`EXEC Queue ${queueName},${opts}`);\n  }\n\n  private async sendCommand(command: string): Promise&lt;string&gt; {\n    return new Promise((resolve) =&gt; {\n      this.socket.write(command + &#39;\\n&#39;);\n\n      this.socket.once(&#39;data&#39;, (data) =&gt; {\n        resolve(data.toString());\n      });\n    });\n  }\n}\n</code></pre>\n<h2>Call Recording and CDR Processing</h2>\n<p>Recording calls and processing call detail records:</p>\n<pre><code class=\"language-typescript\">// infrastructure/asterisk/cdr-processor.ts\n@Injectable()\nexport class CDRProcessor {\n  constructor(\n    private readonly ami: AsteriskAMI,\n    private readonly callRepository: CallRepository,\n    private readonly analyticsService: AnalyticsService\n  ) {\n    this.listenForCDRs();\n  }\n\n  private listenForCDRs(): void {\n    this.ami.on(&#39;Cdr&#39;, async (event) =&gt; {\n      await this.processCDR({\n        uniqueId: event.UniqueID,\n        source: event.Source,\n        destination: event.Destination,\n        callerID: event.CallerID,\n        startTime: new Date(event.StartTime),\n        answerTime: event.AnswerTime ? new Date(event.AnswerTime) : null,\n        endTime: new Date(event.EndTime),\n        duration: parseInt(event.Duration),\n        billableSeconds: parseInt(event.BillableSeconds),\n        disposition: event.Disposition, // ANSWERED, NO ANSWER, BUSY, FAILED\n        recordingFile: event.RecordingFile,\n      });\n    });\n  }\n\n  private async processCDR(cdr: CDRData): Promise&lt;void&gt; {\n    // Find associated call record\n    const call = await this.callRepository.findByUniqueId(cdr.uniqueId);\n\n    if (!call) {\n      console.warn(`No call found for CDR: ${cdr.uniqueId}`);\n      return;\n    }\n\n    // Update call with CDR data\n    call.setDuration(cdr.duration);\n    call.setDisposition(cdr.disposition);\n\n    if (cdr.recordingFile) {\n      call.setRecordingPath(cdr.recordingFile);\n    }\n\n    await this.callRepository.save(call);\n\n    // Process for analytics\n    await this.analyticsService.recordCall({\n      agentId: call.agentId,\n      customerId: call.customerId,\n      duration: cdr.duration,\n      disposition: cdr.disposition,\n      timestamp: cdr.startTime,\n    });\n\n    // If call was answered and recorded, queue for transcription\n    if (cdr.disposition === &#39;ANSWERED&#39; &amp;&amp; cdr.recordingFile) {\n      await this.queueTranscription(call.id, cdr.recordingFile);\n    }\n  }\n\n  private async queueTranscription(\n    callId: string,\n    recordingFile: string\n  ): Promise&lt;void&gt; {\n    await this.queue.add(&#39;transcribe-call&#39;, {\n      callId,\n      recordingFile,\n    });\n  }\n}\n</code></pre>\n<h2>Real-Time Call Monitoring</h2>\n<p>Display live call statistics on a dashboard:</p>\n<pre><code class=\"language-typescript\">// application/services/call-monitoring.service.ts\n@Injectable()\nexport class CallMonitoringService {\n  private activeCalls: Map&lt;string, ActiveCall&gt; = new Map();\n\n  constructor(\n    private readonly ami: AsteriskAMI,\n    private readonly websocket: WebSocketService\n  ) {\n    this.monitorCalls();\n  }\n\n  private monitorCalls(): void {\n    // Track new calls\n    this.ami.on(&#39;Newchannel&#39;, (event) =&gt; {\n      this.activeCalls.set(event.Channel, {\n        channel: event.Channel,\n        callerID: event.CallerIDNum,\n        state: &#39;ringing&#39;,\n        startTime: new Date(),\n      });\n\n      this.broadcastUpdate();\n    });\n\n    // Track answered calls\n    this.ami.on(&#39;Newstate&#39;, (event) =&gt; {\n      if (event.ChannelStateDesc === &#39;Up&#39;) {\n        const call = this.activeCalls.get(event.Channel);\n        if (call) {\n          call.state = &#39;active&#39;;\n          call.answerTime = new Date();\n          this.broadcastUpdate();\n        }\n      }\n    });\n\n    // Track ended calls\n    this.ami.on(&#39;Hangup&#39;, (event) =&gt; {\n      const call = this.activeCalls.get(event.Channel);\n      if (call) {\n        call.endTime = new Date();\n        call.duration = Math.floor(\n          (call.endTime.getTime() - call.startTime.getTime()) / 1000\n        );\n\n        this.activeCalls.delete(event.Channel);\n        this.broadcastUpdate();\n      }\n    });\n  }\n\n  private broadcastUpdate(): void {\n    const stats = {\n      activeCalls: this.activeCalls.size,\n      calls: Array.from(this.activeCalls.values()),\n      avgCallDuration: this.calculateAvgDuration(),\n    };\n\n    this.websocket.broadcast(&#39;call-stats&#39;, stats);\n  }\n\n  getActiveCallsForAgent(agentId: string): ActiveCall[] {\n    return Array.from(this.activeCalls.values()).filter(\n      (call) =&gt; call.agentId === agentId\n    );\n  }\n\n  private calculateAvgDuration(): number {\n    const durations = Array.from(this.activeCalls.values())\n      .filter((call) =&gt; call.answerTime)\n      .map((call) =&gt;\n        Math.floor((Date.now() - call.answerTime!.getTime()) / 1000)\n      );\n\n    if (durations.length === 0) return 0;\n\n    return Math.floor(\n      durations.reduce((sum, d) =&gt; sum + d, 0) / durations.length\n    );\n  }\n}\n</code></pre>\n<p>Dashboard component:</p>\n<pre><code class=\"language-typescript\">const CallDashboard = () =&gt; {\n  const [stats, setStats] = useState({\n    activeCalls: 0,\n    calls: [],\n    avgCallDuration: 0,\n  });\n\n  useEffect(() =&gt; {\n    const socket = getWebSocket();\n\n    socket.on(&#39;call-stats&#39;, (data) =&gt; {\n      setStats(data);\n    });\n\n    return () =&gt; socket.off(&#39;call-stats&#39;);\n  }, []);\n\n  return (\n    &lt;div className=&quot;dashboard&quot;&gt;\n      &lt;div className=&quot;stat-card&quot;&gt;\n        &lt;h3&gt;Active Calls&lt;/h3&gt;\n        &lt;p className=&quot;text-4xl&quot;&gt;{stats.activeCalls}&lt;/p&gt;\n      &lt;/div&gt;\n\n      &lt;div className=&quot;stat-card&quot;&gt;\n        &lt;h3&gt;Avg Duration&lt;/h3&gt;\n        &lt;p className=&quot;text-4xl&quot;&gt;\n          {Math.floor(stats.avgCallDuration / 60)}:\n          {(stats.avgCallDuration % 60).toString().padStart(2, &#39;0&#39;)}\n        &lt;/p&gt;\n      &lt;/div&gt;\n\n      &lt;div className=&quot;calls-list&quot;&gt;\n        {stats.calls.map((call) =&gt; (\n          \n        ))}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  );\n};\n</code></pre>\n<h2>Lessons Learned</h2>\n<h3>Lesson 1: SIP is Complex</h3>\n<p>SIP has many edge cases—NAT traversal, codec negotiation, early media. Don&#39;t try to implement SIP from scratch; use proven servers like Asterisk.</p>\n<h3>Lesson 2: Asterisk is Powerful but Quirky</h3>\n<p>Asterisk is industry-standard but has a learning curve. AGI and AMI are callback-based and require careful error handling.</p>\n<h3>Lesson 3: Call Quality Depends on Networks</h3>\n<p>VoIP quality degrades with packet loss, latency, and jitter. Always test on real networks, not just localhost.</p>\n<h3>Lesson 4: CDRs are Gold</h3>\n<p>Call Detail Records are invaluable for analytics, billing, and compliance. Process them reliably.</p>\n<h3>Lesson 5: Real-Time Monitoring is Essential</h3>\n<p>Supervisors need live dashboards showing active calls, queue depths, and agent status.</p>\n<h2>When to Build vs. Buy</h2>\n<p><strong>Build</strong> when:</p>\n<ul>\n<li>You need deep customization</li>\n<li>You&#39;re integrating with existing systems</li>\n<li>You have telecom expertise on the team</li>\n</ul>\n<p><strong>Buy</strong> (Twilio, etc.) when:</p>\n<ul>\n<li>You need basic calling/SMS</li>\n<li>You want fast time-to-market</li>\n<li>You don&#39;t have VoIP expertise</li>\n</ul>\n<h2>Conclusion</h2>\n<p>Building VoIP systems with Node.js and Asterisk gave us complete control over call flows, IVR systems, and integrations. We handled millions of calls reliably across Bravo Telecom and TeleFlip.</p>\n<p>The key is understanding SIP fundamentals, using proven infrastructure like Asterisk, and building robust application logic in Node.js to orchestrate calls and process data.</p>\n<p>After 27 years of building software, VoIP remains one of the most challenging but rewarding domains—when a customer calls and the system routes them perfectly, that&#39;s real-world impact.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/voip-integration-nodejs/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-11-19 00:00:00",
            "updated_at": "2024-11-19 00:00:00",
            "published_at": "2024-11-19 00:00:00",
            "custom_excerpt": "Building enterprise VoIP systems with Node.js, covering SIP integration, call routing, IVR systems, and lessons from developing telecom platforms for Bravo Telecom and TeleFlip."
          },
          {
            "id": "21",
            "title": "Building Scalable APIs with NestJS",
            "slug": "building-scalable-apis-nestjs",
            "html": "<p>NestJS transformed how we built the Catalyst PSA backend. Coming from years of Express development, NestJS&#39;s structure, dependency injection, and TypeScript-first approach made building and maintaining a large-scale API dramatically easier.</p>\n<p>After building an API handling 50,000+ requests/minute with NestJS, here&#39;s everything I learned about building scalable APIs.</p>\n<h2>Why NestJS Over Express</h2>\n<p>Express is great for simple APIs, but enterprise applications need structure. NestJS provides:</p>\n<ul>\n<li><strong>Dependency Injection</strong>: Testable, modular code</li>\n<li><strong>TypeScript</strong>: Type safety end-to-end</li>\n<li><strong>Modularity</strong>: Clear boundaries between features</li>\n<li><strong>Decorators</strong>: Clean, declarative syntax</li>\n<li><strong>Built-in tools</strong>: Guards, interceptors, pipes, exception filters</li>\n</ul>\n<p>For Catalyst PSA&#39;s 320K+ LOC codebase, this structure was essential.</p>\n<h2>Project Structure</h2>\n<pre><code>src/\n├── main.ts\n├── app.module.ts\n├── modules/\n│   ├── projects/\n│   │   ├── projects.module.ts\n│   │   ├── projects.controller.ts\n│   │   ├── projects.service.ts\n│   │   ├── entities/\n│   │   ├── dto/\n│   │   └── repositories/\n│   ├── time-tracking/\n│   ├── invoicing/\n│   └── users/\n├── common/\n│   ├── decorators/\n│   ├── filters/\n│   ├── guards/\n│   ├── interceptors/\n│   └── pipes/\n└── infrastructure/\n    ├── database/\n    ├── cache/\n    └── messaging/\n</code></pre>\n<p>Each feature is a self-contained module with its own controllers, services, and repositories.</p>\n<h2>Dependency Injection for Testability</h2>\n<p>NestJS&#39;s DI makes testing trivial:</p>\n<pre><code class=\"language-typescript\">// projects/projects.service.ts\n@Injectable()\nexport class ProjectsService {\n  constructor(\n    @Inject(&#39;ProjectRepository&#39;)\n    private readonly repository: ProjectRepository,\n    private readonly eventBus: EventBus,\n    private readonly cache: CacheService\n  ) {}\n\n  async create(dto: CreateProjectDTO): Promise&lt;Project&gt; {\n    const project = Project.create(dto);\n\n    await this.repository.save(project);\n\n    await this.eventBus.publish(new ProjectCreatedEvent(project));\n\n    return project;\n  }\n}\n\n// Testing is easy - inject mocks\ndescribe(&#39;ProjectsService&#39;, () =&gt; {\n  let service: ProjectsService;\n  let repository: jest.Mocked&lt;ProjectRepository&gt;;\n\n  beforeEach(async () =&gt; {\n    const module = await Test.createTestingModule({\n      providers: [\n        ProjectsService,\n        {\n          provide: &#39;ProjectRepository&#39;,\n          useValue: {\n            save: jest.fn(),\n            findById: jest.fn(),\n          },\n        },\n        {\n          provide: EventBus,\n          useValue: { publish: jest.fn() },\n        },\n      ],\n    }).compile();\n\n    service = module.get(ProjectsService);\n    repository = module.get(&#39;ProjectRepository&#39;);\n  });\n\n  it(&#39;should create project&#39;, async () =&gt; {\n    const dto = { name: &#39;Test Project&#39;, clientId: &#39;123&#39; };\n\n    await service.create(dto);\n\n    expect(repository.save).toHaveBeenCalled();\n  });\n});\n</code></pre>\n<p>No need for complex dependency mocking—NestJS DI handles it.</p>\n<h2>DTOs for Validation</h2>\n<p>Data Transfer Objects with class-validator ensure type safety and validation:</p>\n<pre><code class=\"language-typescript\">// dto/create-project.dto.ts\nimport { IsString, IsUUID, IsOptional, Min, Max } from &#39;class-validator&#39;;\nimport { ApiProperty } from &#39;@nestjs/swagger&#39;;\n\nexport class CreateProjectDTO {\n  @ApiProperty({ description: &#39;Project name&#39;, example: &#39;Website Redesign&#39; })\n  @IsString()\n  @MinLength(1)\n  @MaxLength(255)\n  name: string;\n\n  @ApiProperty({ description: &#39;Client UUID&#39; })\n  @IsUUID()\n  clientId: string;\n\n  @ApiProperty({ description: &#39;Project budget&#39;, example: 50000, required: false })\n  @IsOptional()\n  @Min(0)\n  @Max(10000000)\n  budget?: number;\n\n  @ApiProperty({ description: &#39;Project description&#39;, required: false })\n  @IsOptional()\n  @IsString()\n  @MaxLength(5000)\n  description?: string;\n}\n\n// Controller\n@Post()\nasync create(@Body() dto: CreateProjectDTO) {\n  // dto is validated automatically!\n  return await this.projectsService.create(dto);\n}\n</code></pre>\n<p>Invalid requests return detailed error messages:</p>\n<pre><code class=\"language-json\">{\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: [\n    &quot;name should not be empty&quot;,\n    &quot;clientId must be a UUID&quot;,\n    &quot;budget must not be greater than 10000000&quot;\n  ],\n  &quot;error&quot;: &quot;Bad Request&quot;\n}\n</code></pre>\n<h2>Guards for Authorization</h2>\n<p>Guards control access to routes:</p>\n<pre><code class=\"language-typescript\">// guards/roles.guard.ts\n@Injectable()\nexport class RolesGuard implements CanActivate {\n  constructor(private reflector: Reflector) {}\n\n  canActivate(context: ExecutionContext): boolean {\n    const requiredRoles = this.reflector.get&lt;string[]&gt;(&#39;roles&#39;, context.getHandler());\n\n    if (!requiredRoles) {\n      return true;\n    }\n\n    const request = context.switchToHttp().getRequest();\n    const user = request.user;\n\n    return requiredRoles.some(role =&gt; user.roles?.includes(role));\n  }\n}\n\n// Decorator\nexport const Roles = (...roles: string[]) =&gt; SetMetadata(&#39;roles&#39;, roles);\n\n// Usage\n@Post(&#39;projects&#39;)\n@UseGuards(JwtAuthGuard, RolesGuard)\n@Roles(&#39;admin&#39;, &#39;project-manager&#39;)\nasync create(@Body() dto: CreateProjectDTO) {\n  return await this.projectsService.create(dto);\n}\n</code></pre>\n<p>Only admins and project managers can create projects. Guards apply across controllers, ensuring consistent auth.</p>\n<h2>Interceptors for Logging and Transformation</h2>\n<p>Interceptors handle cross-cutting concerns:</p>\n<pre><code class=\"language-typescript\">// interceptors/logging.interceptor.ts\n@Injectable()\nexport class LoggingInterceptor implements NestInterceptor {\n  constructor(private logger: Logger) {}\n\n  intercept(context: ExecutionContext, next: CallHandler): Observable&lt;any&gt; {\n    const request = context.switchToHttp().getRequest();\n    const { method, url, user } = request;\n    const startTime = Date.now();\n\n    return next.handle().pipe(\n      tap(() =&gt; {\n        const duration = Date.now() - startTime;\n        this.logger.log(`${method} ${url} - ${duration}ms - User: ${user?.id}`);\n      }),\n      catchError(error =&gt; {\n        const duration = Date.now() - startTime;\n        this.logger.error(\n          `${method} ${url} - ${duration}ms - Error: ${error.message}`,\n          error.stack\n        );\n        throw error;\n      })\n    );\n  }\n}\n\n// Apply globally\napp.useGlobalInterceptors(new LoggingInterceptor(logger));\n</code></pre>\n<p>Every request is logged with duration and user context automatically.</p>\n<h3>Transform Response Format</h3>\n<pre><code class=\"language-typescript\">@Injectable()\nexport class TransformInterceptor implements NestInterceptor {\n  intercept(context: ExecutionContext, next: CallHandler): Observable&lt;any&gt; {\n    return next.handle().pipe(\n      map(data =&gt; ({\n        success: true,\n        data,\n        timestamp: new Date().toISOString(),\n      }))\n    );\n  }\n}\n\n// Responses become:\n{\n  &quot;success&quot;: true,\n  &quot;data&quot;: { &quot;id&quot;: &quot;123&quot;, &quot;name&quot;: &quot;Project&quot; },\n  &quot;timestamp&quot;: &quot;2024-11-05T10:30:00.000Z&quot;\n}\n</code></pre>\n<h2>Exception Filters for Error Handling</h2>\n<p>Centralize error handling:</p>\n<pre><code class=\"language-typescript\">// filters/http-exception.filter.ts\n@Catch()\nexport class HttpExceptionFilter implements ExceptionFilter {\n  constructor(private logger: Logger) {}\n\n  catch(exception: unknown, host: ArgumentsHost) {\n    const ctx = host.switchToHttp();\n    const response = ctx.getResponse();\n    const request = ctx.getRequest();\n\n    const status =\n      exception instanceof HttpException\n        ? exception.getStatus()\n        : HttpStatus.INTERNAL_SERVER_ERROR;\n\n    const message =\n      exception instanceof HttpException\n        ? exception.getResponse()\n        : &#39;Internal server error&#39;;\n\n    this.logger.error(\n      `${request.method} ${request.url}`,\n      exception instanceof Error ? exception.stack : exception\n    );\n\n    response.status(status).json({\n      statusCode: status,\n      timestamp: new Date().toISOString(),\n      path: request.url,\n      message,\n    });\n  }\n}\n\n// Apply globally\napp.useGlobalFilters(new HttpExceptionFilter(logger));\n</code></pre>\n<p>All exceptions are caught, logged, and returned in consistent format.</p>\n<h2>Pipes for Transformation and Validation</h2>\n<p>Transform and validate data:</p>\n<pre><code class=\"language-typescript\">// pipes/parse-uuid.pipe.ts\n@Injectable()\nexport class ParseUUIDPipe implements PipeTransform {\n  transform(value: string): string {\n    if (!isUUID(value)) {\n      throw new BadRequestException(&#39;Invalid UUID format&#39;);\n    }\n    return value;\n  }\n}\n\n// Usage\n@Get(&#39;:id&#39;)\nasync getProject(@Param(&#39;id&#39;, ParseUUIDPipe) id: string) {\n  return await this.projectsService.findById(id);\n}\n</code></pre>\n<p>NestJS includes built-in pipes: <code>ValidationPipe</code>, <code>ParseIntPipe</code>, <code>ParseUUIDPipe</code>, etc.</p>\n<h2>Caching for Performance</h2>\n<p>Add caching with decorators:</p>\n<pre><code class=\"language-typescript\">// Install cache manager\nimport { CacheModule } from &#39;@nestjs/cache-manager&#39;;\n\n@Module({\n  imports: [\n    CacheModule.register({\n      ttl: 300, // 5 minutes\n      max: 100, // max items in cache\n    }),\n  ],\n})\nexport class ProjectsModule {}\n\n// Controller\n@Controller(&#39;projects&#39;)\nexport class ProjectsController {\n  @Get(&#39;:id&#39;)\n  @UseInterceptors(CacheInterceptor)\n  @CacheKey(&#39;project&#39;)\n  @CacheTTL(300)\n  async getProject(@Param(&#39;id&#39;) id: string) {\n    return await this.projectsService.findById(id);\n  }\n}\n</code></pre>\n<p>Requests are cached automatically. For Redis:</p>\n<pre><code class=\"language-typescript\">import * as redisStore from &#39;cache-manager-redis-store&#39;;\n\nCacheModule.register({\n  store: redisStore,\n  host: process.env.REDIS_HOST,\n  port: process.env.REDIS_PORT,\n  ttl: 300,\n});\n</code></pre>\n<h2>Pagination and Filtering</h2>\n<p>Standardize pagination:</p>\n<pre><code class=\"language-typescript\">// dto/pagination.dto.ts\nexport class PaginationDTO {\n  @IsOptional()\n  @Type(() =&gt; Number)\n  @IsInt()\n  @Min(1)\n  @Max(100)\n  limit?: number = 20;\n\n  @IsOptional()\n  @Type(() =&gt; Number)\n  @IsInt()\n  @Min(0)\n  page?: number = 0;\n\n  @IsOptional()\n  @IsString()\n  sortBy?: string = &#39;createdAt&#39;;\n\n  @IsOptional()\n  @IsEnum([&#39;asc&#39;, &#39;desc&#39;])\n  sortOrder?: &#39;asc&#39; | &#39;desc&#39; = &#39;desc&#39;;\n}\n\n// Controller\n@Get()\nasync getProjects(@Query() pagination: PaginationDTO) {\n  return await this.projectsService.findAll(pagination);\n}\n\n// Service\nasync findAll(pagination: PaginationDTO) {\n  const { limit, page, sortBy, sortOrder } = pagination;\n\n  const [items, total] = await this.repository.findAndCount({\n    take: limit,\n    skip: page * limit,\n    order: { [sortBy]: sortOrder },\n  });\n\n  return {\n    items,\n    total,\n    page,\n    pageCount: Math.ceil(total / limit),\n  };\n}\n</code></pre>\n<p>Requests: <code>GET /projects?limit=10&amp;page=2&amp;sortBy=name&amp;sortOrder=asc</code></p>\n<h2>Rate Limiting</h2>\n<p>Protect against abuse:</p>\n<pre><code class=\"language-bash\">npm install @nestjs/throttler\n</code></pre>\n<pre><code class=\"language-typescript\">import { ThrottlerModule } from &#39;@nestjs/throttler&#39;;\n\n@Module({\n  imports: [\n    ThrottlerModule.forRoot({\n      ttl: 60,\n      limit: 100, // 100 requests per 60 seconds\n    }),\n  ],\n})\nexport class AppModule {}\n\n// Apply to specific routes\n@Controller(&#39;projects&#39;)\n@UseGuards(ThrottlerGuard)\nexport class ProjectsController {\n  // All routes limited to 100/min\n\n  @Post()\n  @Throttle(10, 60) // Override: 10 per minute\n  async create(@Body() dto: CreateProjectDTO) {\n    return await this.projectsService.create(dto);\n  }\n}\n</code></pre>\n<h2>API Documentation with Swagger</h2>\n<p>Auto-generate API docs:</p>\n<pre><code class=\"language-typescript\">// main.ts\nimport { SwaggerModule, DocumentBuilder } from &#39;@nestjs/swagger&#39;;\n\nconst config = new DocumentBuilder()\n  .setTitle(&#39;Catalyst PSA API&#39;)\n  .setDescription(&#39;Professional Services Automation Platform API&#39;)\n  .setVersion(&#39;1.0&#39;)\n  .addBearerAuth()\n  .build();\n\nconst document = SwaggerModule.createDocument(app, config);\nSwaggerModule.setup(&#39;api/docs&#39;, app, document);\n</code></pre>\n<p>Visit <code>/api/docs</code> for interactive API documentation. DTOs with <code>@ApiProperty()</code> appear automatically.</p>\n<h2>Background Jobs with Bull</h2>\n<p>Handle async tasks:</p>\n<pre><code class=\"language-bash\">npm install @nestjs/bull bull\n</code></pre>\n<pre><code class=\"language-typescript\">import { BullModule } from &#39;@nestjs/bull&#39;;\n\n@Module({\n  imports: [\n    BullModule.forRoot({\n      redis: {\n        host: &#39;localhost&#39;,\n        port: 6379,\n      },\n    }),\n    BullModule.registerQueue({\n      name: &#39;email&#39;,\n    }),\n  ],\n})\nexport class AppModule {}\n\n// Producer\n@Injectable()\nexport class ProjectsService {\n  constructor(@InjectQueue(&#39;email&#39;) private emailQueue: Queue) {}\n\n  async create(dto: CreateProjectDTO) {\n    const project = await this.repository.save(dto);\n\n    // Queue email asynchronously\n    await this.emailQueue.add(&#39;project-created&#39;, {\n      projectId: project.id,\n      managerId: dto.managerId,\n    });\n\n    return project;\n  }\n}\n\n// Consumer\n@Processor(&#39;email&#39;)\nexport class EmailProcessor {\n  @Process(&#39;project-created&#39;)\n  async handleProjectCreated(job: Job) {\n    const { projectId, managerId } = job.data;\n\n    await this.emailService.sendProjectCreatedEmail(managerId, projectId);\n  }\n}\n</code></pre>\n<p>Emails are sent in the background, not blocking API responses.</p>\n<h2>Health Checks</h2>\n<p>Monitor application health:</p>\n<pre><code class=\"language-bash\">npm install @nestjs/terminus\n</code></pre>\n<pre><code class=\"language-typescript\">import { TerminusModule } from &#39;@nestjs/terminus&#39;;\n\n@Controller(&#39;health&#39;)\nexport class HealthController {\n  constructor(\n    private health: HealthCheckService,\n    private db: TypeOrmHealthIndicator,\n    private redis: RedisHealthIndicator,\n  ) {}\n\n  @Get()\n  @HealthCheck()\n  check() {\n    return this.health.check([\n      () =&gt; this.db.pingCheck(&#39;database&#39;),\n      () =&gt; this.redis.pingCheck(&#39;redis&#39;),\n    ]);\n  }\n}\n</code></pre>\n<p><code>GET /health</code> returns:</p>\n<pre><code class=\"language-json\">{\n  &quot;status&quot;: &quot;ok&quot;,\n  &quot;info&quot;: {\n    &quot;database&quot;: { &quot;status&quot;: &quot;up&quot; },\n    &quot;redis&quot;: { &quot;status&quot;: &quot;up&quot; }\n  }\n}\n</code></pre>\n<h2>Scaling Horizontally</h2>\n<p>NestJS apps are stateless and scale horizontally. We run 10+ instances behind a load balancer.</p>\n<p>For shared state, use Redis:</p>\n<pre><code class=\"language-typescript\">// Shared cache\nimport { CacheModule } from &#39;@nestjs/cache-manager&#39;;\nimport * as redisStore from &#39;cache-manager-redis-store&#39;;\n\nCacheModule.register({\n  store: redisStore,\n  host: process.env.REDIS_HOST,\n});\n\n// Shared sessions\nimport * as session from &#39;express-session&#39;;\nimport * as connectRedis from &#39;connect-redis&#39;;\n\nconst RedisStore = connectRedis(session);\n\napp.use(\n  session({\n    store: new RedisStore({ client: redisClient }),\n    secret: process.env.SESSION_SECRET,\n  })\n);\n</code></pre>\n<h2>Performance Monitoring</h2>\n<p>Use APM tools:</p>\n<pre><code class=\"language-bash\">npm install @sentry/node @sentry/tracing\n</code></pre>\n<pre><code class=\"language-typescript\">import * as Sentry from &#39;@sentry/node&#39;;\n\nSentry.init({\n  dsn: process.env.SENTRY_DSN,\n  tracesSampleRate: 0.1,\n  integrations: [\n    new Sentry.Integrations.Http({ tracing: true }),\n  ],\n});\n\napp.use(Sentry.Handlers.requestHandler());\napp.use(Sentry.Handlers.tracingHandler());\n// ... routes ...\napp.use(Sentry.Handlers.errorHandler());\n</code></pre>\n<p>Monitor request duration, error rates, and database queries in production.</p>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Structure matters</strong>: NestJS&#39;s module system keeps large codebases organized</li>\n<li><strong>DI is essential</strong>: Makes testing and mocking trivial</li>\n<li><strong>Decorators are powerful</strong>: Clean, declarative code</li>\n<li><strong>Middleware is composable</strong>: Guards, interceptors, and pipes handle cross-cutting concerns</li>\n<li><strong>Type safety</strong>: End-to-end TypeScript catches bugs early</li>\n</ol>\n<h2>Conclusion</h2>\n<p>NestJS transformed how we built the Catalyst PSA API. Its structure, dependency injection, and built-in features made building a scalable, maintainable API handling 50K+ req/min straightforward.</p>\n<p>After 27 years of building APIs, NestJS is my go-to framework for enterprise Node.js applications. It provides the structure and patterns needed for large codebases without sacrificing flexibility.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/building-scalable-apis-nestjs/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-11-04 00:00:00",
            "updated_at": "2024-11-04 00:00:00",
            "published_at": "2024-11-04 00:00:00",
            "custom_excerpt": "Architectural patterns and best practices for building enterprise-grade scalable APIs with NestJS, based on experience building the Catalyst PSA backend serving thousands of requests per minute."
          },
          {
            "id": "22",
            "title": "Building Blockchain-Integrated Applications with Node.js",
            "slug": "blockchain-integration-nodejs",
            "html": "<p>At Verizon, I developed a full-stack NFT collectibles solution for AMC&#39;s Walking Dead franchise. This wasn&#39;t just another blockchain project - it was a production system that had to handle real users, real transactions, and real money. Here&#39;s what I learned.</p>\n<h2>The Architecture Challenge</h2>\n<p>Blockchain interactions are fundamentally different from traditional API calls. They&#39;re slow, expensive, and can fail in unpredictable ways. We needed an architecture that could:</p>\n<ol>\n<li>Handle blockchain latency without blocking users</li>\n<li>Manage transaction costs (gas fees)</li>\n<li>Provide a responsive user experience</li>\n<li>Scale to thousands of concurrent users</li>\n</ol>\n<h2>The Request Queue System</h2>\n<p>Our solution was a request scheduling system using Redis and Socket.io:</p>\n<pre><code class=\"language-typescript\">// blockchain-queue.service.ts\nimport Redis from &#39;ioredis&#39;;\nimport { Server as SocketServer } from &#39;socket.io&#39;;\nimport { ethers } from &#39;ethers&#39;;\n\ninterface BlockchainRequest {\n  id: string;\n  type: &#39;mint&#39; | &#39;transfer&#39; | &#39;query&#39;;\n  userId: string;\n  params: any;\n  priority: number;\n  timestamp: number;\n}\n\nexport class BlockchainQueueService {\n  private redis: Redis;\n  private io: SocketServer;\n  private provider: ethers.providers.Provider;\n  private contract: ethers.Contract;\n  private processing = false;\n  private maxConcurrent = 3;\n  private activeRequests = 0;\n\n  constructor(\n    redisClient: Redis,\n    socketServer: SocketServer,\n    contractAddress: string,\n    contractABI: any\n  ) {\n    this.redis = redisClient;\n    this.io = socketServer;\n\n    this.provider = new ethers.providers.JsonRpcProvider(\n      process.env.ETHEREUM_RPC_URL\n    );\n\n    this.contract = new ethers.Contract(\n      contractAddress,\n      contractABI,\n      this.provider\n    );\n  }\n\n  async enqueueRequest(request: Omit\n      )}\n\n      {status === &#39;error&#39; &amp;&amp; (\n        \n      )}\n    &lt;/View&gt;\n  );\n}\n</code></pre>\n<h2>Key Lessons</h2>\n<ol>\n<li><strong>Never block the UI</strong>: Blockchain operations are slow - always queue them</li>\n<li><strong>Gas estimation is critical</strong>: Running out of gas wastes money</li>\n<li><strong>Confirmations matter</strong>: Wait for multiple confirmations for important transactions</li>\n<li><strong>Error handling is complex</strong>: Network issues, gas issues, smart contract reverts - handle them all</li>\n<li><strong>Test on testnet extensively</strong>: Real ETH is expensive - test thoroughly first</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Blockchain development requires rethinking traditional application architecture. The key is building systems that gracefully handle the asynchronous, expensive nature of blockchain interactions while providing a great user experience.</p>\n<p>The queue-based approach we built at Verizon scaled well and kept users happy even during high load periods.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/blockchain-integration-nodejs/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-11-01 00:00:00",
            "updated_at": "2024-11-01 00:00:00",
            "published_at": "2024-11-01 00:00:00",
            "custom_excerpt": "Lessons from building an NFT collectibles platform that interfaces with Ethereum blockchain"
          },
          {
            "id": "23",
            "title": "From Legacy to Modern: Migration Strategies That Work",
            "slug": "legacy-to-modern-migration-strategies",
            "html": "<p>Over 27 years, I&#39;ve migrated systems from VB6 to .NET, Classic ASP to Node.js, and monoliths to microservices. I&#39;ve seen migrations succeed spectacularly and fail catastrophically. The difference isn&#39;t the tech—it&#39;s the strategy.</p>\n<p>Here&#39;s everything I&#39;ve learned about migrating legacy systems to modern architectures without destroying your business in the process.</p>\n<h2>The Big Rewrite is a Trap</h2>\n<p>The most tempting and most dangerous migration strategy: &quot;Let&#39;s rewrite from scratch!&quot;</p>\n<h3>Why Rewrites Fail</h3>\n<ol>\n<li><strong>Business keeps evolving</strong>: By the time you finish, requirements changed</li>\n<li><strong>Hidden complexity</strong>: Legacy code contains years of bug fixes and edge cases</li>\n<li><strong>No feedback loop</strong>: Can&#39;t release until complete</li>\n<li><strong>Team burnout</strong>: Rewriting existing functionality is demoralizing</li>\n<li><strong>Opportunity cost</strong>: 2 years building, 0 years shipping features</li>\n</ol>\n<p>I&#39;ve seen companies spend 3 years on rewrites, only to abandon them and go back to the legacy system.</p>\n<h3>The Strangler Fig Pattern</h3>\n<p>Instead of rewriting, gradually replace:</p>\n<pre><code>Legacy System (100% traffic)\n    ↓\nLegacy (90%) + New (10%)\n    ↓\nLegacy (50%) + New (50%)\n    ↓\nLegacy (10%) + New (90%)\n    ↓\nNew System (100% traffic)\n</code></pre>\n<p>Named after strangler fig vines that eventually replace their host tree.</p>\n<h2>Step 1: Add a Routing Layer</h2>\n<p>Route requests between legacy and new systems:</p>\n<pre><code class=\"language-typescript\">// api-gateway/router.ts\nimport express from &#39;express&#39;;\nimport { createProxyMiddleware } from &#39;http-proxy-middleware&#39;;\n\nconst app = express();\n\n// Feature flags determine routing\nconst featureFlags = {\n  &#39;new-projects-api&#39;: {\n    enabled: true,\n    rolloutPercentage: 20, // 20% of traffic\n  },\n};\n\napp.use(&#39;/api/projects&#39;, (req, res, next) =&gt; {\n  const useNewSystem =\n    featureFlags[&#39;new-projects-api&#39;].enabled &amp;&amp;\n    Math.random() &lt; featureFlags[&#39;new-projects-api&#39;].rolloutPercentage / 100;\n\n  if (useNewSystem) {\n    // Route to new system\n    createProxyMiddleware({\n      target: &#39;http://new-api:3000&#39;,\n      changeOrigin: true,\n    })(req, res, next);\n  } else {\n    // Route to legacy system\n    createProxyMiddleware({\n      target: &#39;http://legacy-api:8080&#39;,\n      changeOrigin: true,\n    })(req, res, next);\n  }\n});\n\n// Legacy system handles everything else\napp.use(&#39;/&#39;, createProxyMiddleware({\n  target: &#39;http://legacy-api:8080&#39;,\n  changeOrigin: true,\n}));\n</code></pre>\n<p>Start at 1% traffic, gradually increase. Roll back instantly if issues arise.</p>\n<h2>Step 2: Create Anti-Corruption Layer</h2>\n<p>Legacy systems have weird data models. Don&#39;t let that leak into new code.</p>\n<pre><code class=\"language-typescript\">// legacy-adapter/project-adapter.ts\nexport class LegacyProjectAdapter {\n  // Legacy system returns bizarre format\n  fromLegacy(legacyProject: any): Project {\n    return {\n      id: legacyProject.PROJECT_ID,\n      name: legacyProject.NAME,\n      status: this.mapStatus(legacyProject.STATUS_CODE),\n      clientId: legacyProject.CLIENT_FK,\n      budget: parseFloat(legacyProject.BUDGET_AMT || &#39;0&#39;),\n      createdAt: new Date(legacyProject.CREATED_DATE),\n      // Map 20+ weird fields to clean domain model\n    };\n  }\n\n  // Clean model to legacy format\n  toLegacy(project: Project): any {\n    return {\n      PROJECT_ID: project.id,\n      NAME: project.name,\n      STATUS_CODE: this.unmapStatus(project.status),\n      CLIENT_FK: project.clientId,\n      BUDGET_AMT: project.budget.toString(),\n      CREATED_DATE: project.createdAt.toISOString(),\n    };\n  }\n\n  private mapStatus(legacyStatus: string): ProjectStatus {\n    const mapping: Record&lt;string, ProjectStatus&gt; = {\n      &#39;A&#39;: &#39;active&#39;,\n      &#39;C&#39;: &#39;completed&#39;,\n      &#39;H&#39;: &#39;on-hold&#39;,\n      &#39;X&#39;: &#39;cancelled&#39;,\n    };\n    return mapping[legacyStatus] || &#39;active&#39;;\n  }\n}\n</code></pre>\n<p>New code only sees clean <code>Project</code> domain model. Legacy weirdness is isolated.</p>\n<h2>Step 3: Dual-Write Strategy</h2>\n<p>Write to both systems during migration:</p>\n<pre><code class=\"language-typescript\">export class ProjectService {\n  constructor(\n    private legacyAdapter: LegacyProjectAdapter,\n    private newRepository: ProjectRepository,\n    private featureFlags: FeatureFlagService\n  ) {}\n\n  async create(dto: CreateProjectDTO): Promise&lt;Project&gt; {\n    // Create in new system\n    const project = await this.newRepository.create(dto);\n\n    if (this.featureFlags.isEnabled(&#39;dual-write-projects&#39;)) {\n      try {\n        // Also write to legacy system for consistency\n        await this.legacyAdapter.create(project);\n      } catch (error) {\n        // Log but don&#39;t fail - new system is source of truth\n        logger.error(&#39;Failed to sync to legacy&#39;, error);\n      }\n    }\n\n    return project;\n  }\n}\n</code></pre>\n<p>Both systems stay in sync. When migration is complete, remove dual writes.</p>\n<h2>Step 4: Migrate Data Incrementally</h2>\n<p>Don&#39;t migrate all data at once. Do it in phases:</p>\n<pre><code class=\"language-typescript\">// migration-scripts/migrate-projects.ts\nasync function migrateProjects() {\n  let offset = 0;\n  const batchSize = 1000;\n\n  while (true) {\n    // Fetch batch from legacy\n    const legacyProjects = await legacyDB.query(\n      `SELECT * FROM PROJECTS LIMIT ${batchSize} OFFSET ${offset}`\n    );\n\n    if (legacyProjects.length === 0) break;\n\n    // Transform and insert into new system\n    const projects = legacyProjects.map(p =&gt; adapter.fromLegacy(p));\n\n    await newDB.insert(projects).values(projects);\n\n    console.log(`Migrated ${offset + legacyProjects.length} projects`);\n\n    offset += batchSize;\n\n    // Don&#39;t overload database\n    await sleep(1000);\n  }\n}\n</code></pre>\n<p>Run during off-hours. If it fails, restart from last offset.</p>\n<h2>Step 5: Validate Data Consistency</h2>\n<p>Ensure both systems return same data:</p>\n<pre><code class=\"language-typescript\">async function validateProjects() {\n  const projectIds = await getRandomProjectIds(100);\n\n  for (const id of projectIds) {\n    const legacyProject = await legacyAPI.getProject(id);\n    const newProject = await newAPI.getProject(id);\n\n    const legacyNormalized = adapter.fromLegacy(legacyProject);\n\n    if (!deepEqual(legacyNormalized, newProject)) {\n      logger.error(&#39;Data mismatch&#39;, {\n        projectId: id,\n        legacy: legacyNormalized,\n        new: newProject,\n        diff: diff(legacyNormalized, newProject),\n      });\n    }\n  }\n}\n\n// Run hourly in production\nsetInterval(validateProjects, 60 * 60 * 1000);\n</code></pre>\n<p>Catches data inconsistencies early.</p>\n<h2>Step 6: Feature Parity Testing</h2>\n<p>Ensure new system can do everything legacy does:</p>\n<pre><code class=\"language-typescript\">describe(&#39;Project API Parity&#39;, () =&gt; {\n  it(&#39;should support all legacy query parameters&#39;, async () =&gt; {\n    const legacyQueries = [\n      &#39;/projects?status=active&#39;,\n      &#39;/projects?client_id=123&#39;,\n      &#39;/projects?sort=name&amp;order=desc&#39;,\n      &#39;/projects?search=website&#39;,\n    ];\n\n    for (const query of legacyQueries) {\n      const legacyResponse = await legacyAPI.get(query);\n      const newResponse = await newAPI.get(query);\n\n      expect(newResponse.length).toBeGreaterThan(0);\n      expect(newResponse).toMatchSchema(legacyResponse);\n    }\n  });\n\n  it(&#39;should maintain backward compatibility&#39;, async () =&gt; {\n    // New API accepts both old and new formats\n    const oldFormat = { project_id: &#39;123&#39;, project_name: &#39;Test&#39; };\n    const newFormat = { id: &#39;123&#39;, name: &#39;Test&#39; };\n\n    await expect(newAPI.create(oldFormat)).resolves.toBeDefined();\n    await expect(newAPI.create(newFormat)).resolves.toBeDefined();\n  });\n});\n</code></pre>\n<h2>Real-World Migration: OneTravel.com</h2>\n<p>When I worked on OneTravel.com, we migrated from a .NET monolith to Node.js microservices.</p>\n<h3>Our Approach</h3>\n<p><strong>Phase 1 (Months 1-3): Infrastructure</strong></p>\n<ul>\n<li>Set up new Node.js services</li>\n<li>Create API gateway for routing</li>\n<li>Build anti-corruption layer</li>\n</ul>\n<p><strong>Phase 2 (Months 4-9): Migrate Features</strong></p>\n<ul>\n<li>Month 4: Search (10% traffic, then 100%)</li>\n<li>Month 6: Booking (5% traffic, then 100%)</li>\n<li>Month 8: User accounts (20% traffic, then 100%)</li>\n</ul>\n<p><strong>Phase 3 (Months 10-12): Data Migration</strong></p>\n<ul>\n<li>Migrate historical data</li>\n<li>Validate consistency</li>\n<li>Remove dual writes</li>\n</ul>\n<p><strong>Phase 4 (Month 13): Decommission Legacy</strong></p>\n<ul>\n<li>Turn off old system</li>\n<li>Celebrate!</li>\n</ul>\n<h3>What Worked</h3>\n<ol>\n<li><strong>Gradual rollout</strong>: Caught issues with 1% traffic, not 100%</li>\n<li><strong>Feature flags</strong>: Instant rollback if problems</li>\n<li><strong>Parallel running</strong>: Legacy was fallback for 6 months</li>\n<li><strong>Team buy-in</strong>: Shipped features in new system early, kept team motivated</li>\n</ol>\n<h3>What Didn&#39;t</h3>\n<ol>\n<li><strong>Underestimated data</strong>: Data migration took 3x longer than expected</li>\n<li><strong>Hidden dependencies</strong>: Discovered 15+ integrations we didn&#39;t know about</li>\n<li><strong>Performance testing</strong>: New system was slower initially, needed optimization</li>\n</ol>\n<h2>Migration Anti-Patterns to Avoid</h2>\n<h3>1. Big Bang Deployment</h3>\n<pre><code>❌ Friday 5pm: Switch everyone to new system\n❌ Monday 9am: Everything breaks\n❌ Tuesday: Roll back, project cancelled\n</code></pre>\n<p>Always migrate gradually with rollback capability.</p>\n<h3>2. Ignoring Edge Cases</h3>\n<pre><code class=\"language-typescript\">// ❌ &quot;The legacy system checks for null, but we&#39;ll use TypeScript so it&#39;s fine&quot;\n\n// Reality: Legacy system has data like this:\n{\n  project_name: null,        // Should be impossible\n  status: &#39;Q&#39;,               // Undocumented status\n  budget: &#39;TBD&#39;,             // Number field with string?!\n  created_date: &#39;0000-00-00&#39; // Invalid date\n}\n</code></pre>\n<p>Legacy systems contain years of edge cases. Handle them.</p>\n<h3>3. Assuming Perfect Documentation</h3>\n<p>Legacy systems are never documented. Plan for discovery:</p>\n<pre><code class=\"language-typescript\">// Document as you migrate\nasync function migrateProjects() {\n  const projects = await legacyDB.query(&#39;SELECT * FROM PROJECTS&#39;);\n\n  // Discover actual data\n  const statusValues = new Set(projects.map(p =&gt; p.STATUS_CODE));\n  console.log(&#39;Found status values:&#39;, statusValues);\n  // Output: Set { &#39;A&#39;, &#39;C&#39;, &#39;H&#39;, &#39;X&#39;, &#39;P&#39;, &#39;Q&#39;, &#39;?&#39; }\n  //         ^ What&#39;s &#39;P&#39;, &#39;Q&#39;, &#39;?&#39;? Not documented!\n}\n</code></pre>\n<h3>4. No Rollback Plan</h3>\n<pre><code class=\"language-typescript\">// ✅ Always have killswitch\nconst config = {\n  useNewSystem: process.env.USE_NEW_SYSTEM === &#39;true&#39;,\n  rolloutPercentage: parseInt(process.env.ROLLOUT_PERCENTAGE || &#39;0&#39;),\n};\n\n// Can change these via environment variables instantly\n</code></pre>\n<h2>Tools and Techniques</h2>\n<h3>Database Migration Tools</h3>\n<ul>\n<li><strong>Flyway</strong>: Version-controlled SQL migrations</li>\n<li><strong>Liquibase</strong>: Database-agnostic migrations</li>\n<li><strong>Prisma Migrate</strong>: TypeScript-first migrations</li>\n</ul>\n<h3>Feature Flags</h3>\n<ul>\n<li><strong>LaunchDarkly</strong>: Enterprise feature flags</li>\n<li><strong>Unleash</strong>: Open-source feature flags</li>\n<li><strong>Custom solution</strong>: Simple flag service in database</li>\n</ul>\n<h3>Monitoring</h3>\n<p>Track both systems during migration:</p>\n<pre><code class=\"language-typescript\">import * as Sentry from &#39;@sentry/node&#39;;\n\nasync function getProject(id: string) {\n  const transaction = Sentry.startTransaction({\n    op: &#39;project.get&#39;,\n    name: &#39;Get Project&#39;,\n  });\n\n  try {\n    const useNew = shouldUseNewSystem();\n\n    const span = transaction.startChild({\n      op: &#39;db.query&#39;,\n      description: useNew ? &#39;New System&#39; : &#39;Legacy System&#39;,\n    });\n\n    const result = useNew\n      ? await newAPI.getProject(id)\n      : await legacyAPI.getProject(id);\n\n    span.finish();\n\n    return result;\n  } finally {\n    transaction.finish();\n  }\n}\n</code></pre>\n<p>Compare performance and error rates between systems.</p>\n<h2>When to Give Up and Rewrite</h2>\n<p>Sometimes rewrites are necessary:</p>\n<ul>\n<li><strong>Technology is dead</strong>: VB6, Flash, Silverlight</li>\n<li><strong>No one understands it</strong>: Original developers gone, no documentation</li>\n<li><strong>Can&#39;t be maintained</strong>: Every change breaks something</li>\n<li><strong>Security holes</strong>: Unfixable vulnerabilities</li>\n<li><strong>Performance is unfixable</strong>: Architecture is fundamentally flawed</li>\n</ul>\n<p>But even then, consider strangler pattern first.</p>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Migrate gradually</strong>: Small batches, feature flags, rollback capability</li>\n<li><strong>Legacy has hidden complexity</strong>: Plan for 3x more edge cases than you expect</li>\n<li><strong>Both systems will run in parallel</strong>: For months or years</li>\n<li><strong>Data migration is hardest</strong>: Allocate 40% of time to data</li>\n<li><strong>Feature parity first</strong>: New system must do everything legacy does</li>\n<li><strong>Monitor everything</strong>: Compare performance, errors, data consistency</li>\n<li><strong>Team needs wins</strong>: Ship features in new system early to maintain morale</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Migrating legacy systems to modern architectures is one of the hardest things in software engineering. The key is avoiding the big rewrite trap and using the strangler pattern: gradually replace the old system with the new, one piece at a time.</p>\n<p>After 27 years and countless migrations, I&#39;ve learned that successful migrations aren&#39;t about perfect code or cutting-edge tech—they&#39;re about incremental progress, careful validation, and always having a rollback plan.</p>\n<p>The best migration is the one that&#39;s so gradual, users never notice it happened.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/legacy-to-modern-migration-strategies/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-10-24 00:00:00",
            "updated_at": "2024-10-24 00:00:00",
            "published_at": "2024-10-24 00:00:00",
            "custom_excerpt": "Proven strategies for migrating legacy applications to modern tech stacks without disrupting business, based on migrating multiple enterprise systems over 27 years."
          },
          {
            "id": "24",
            "title": "Building Real-Time Features with WebSockets and Redis",
            "slug": "building-realtime-features-websockets-redis",
            "html": "<p>When we added real-time collaboration to the Catalyst PSA platform—live updates to project timesheets, instant notifications, real-time presence indicators—we discovered that building truly scalable real-time features is harder than it looks.</p>\n<p>After handling millions of WebSocket messages and scaling to support hundreds of concurrent users collaborating on shared projects, here&#39;s everything we learned about building real-time features the right way.</p>\n<h2>Why Real-Time Matters in Enterprise Software</h2>\n<p>Traditional request-response works fine for many features, but enterprise collaboration demands real-time:</p>\n<ul>\n<li><strong>Time tracking</strong>: When a team member logs time, project managers see it immediately</li>\n<li><strong>Notifications</strong>: Critical alerts need instant delivery</li>\n<li><strong>Presence</strong>: Who&#39;s currently viewing this project?</li>\n<li><strong>Collaborative editing</strong>: Multiple users editing schedules simultaneously</li>\n<li><strong>Live dashboards</strong>: Metrics updating as data changes</li>\n</ul>\n<p>Polling (checking the server every N seconds) is wasteful and doesn&#39;t scale. WebSockets provide persistent connections for bidirectional communication.</p>\n<h2>The Architecture: WebSockets + Redis Pub/Sub</h2>\n<p>Our real-time architecture has three layers:</p>\n<h3>1. WebSocket Gateway (Connection Layer)</h3>\n<p>NestJS WebSocket gateway manages client connections:</p>\n<pre><code class=\"language-typescript\">// websocket/realtime.gateway.ts\n@WebSocketGateway({\n  cors: {\n    origin: process.env.CORS_ORIGINS?.split(&#39;,&#39;) || [],\n    credentials: true,\n  },\n})\nexport class RealtimeGateway implements OnGatewayConnection, OnGatewayDisconnect {\n  constructor(\n    private readonly authService: AuthService,\n    private readonly redisService: RedisService\n  ) {}\n\n  async handleConnection(client: Socket) {\n    try {\n      // Authenticate connection\n      const token = client.handshake.auth.token;\n      const user = await this.authService.verifyToken(token);\n\n      // Store user context on socket\n      client.data.userId = user.id;\n      client.data.tenantId = user.tenantId;\n\n      // Join user-specific room for private messages\n      client.join(`user:${user.id}`);\n\n      // Publish connection event\n      await this.redisService.publish(&#39;user:connected&#39;, {\n        userId: user.id,\n        tenantId: user.tenantId,\n        timestamp: new Date(),\n      });\n\n      console.log(`Client connected: ${user.id}`);\n    } catch (error) {\n      console.error(&#39;Connection auth failed:&#39;, error);\n      client.disconnect();\n    }\n  }\n\n  async handleDisconnect(client: Socket) {\n    const userId = client.data.userId;\n\n    if (userId) {\n      await this.redisService.publish(&#39;user:disconnected&#39;, {\n        userId,\n        timestamp: new Date(),\n      });\n\n      console.log(`Client disconnected: ${userId}`);\n    }\n  }\n}\n</code></pre>\n<h3>2. Redis Pub/Sub (Message Bus)</h3>\n<p>Redis handles message distribution across multiple server instances:</p>\n<pre><code class=\"language-typescript\">// infrastructure/redis/redis-pubsub.service.ts\n@Injectable()\nexport class RedisPubSubService {\n  private publisher: Redis;\n  private subscriber: Redis;\n  private handlers: Map&lt;string, Set&lt;MessageHandler&gt;&gt; = new Map();\n\n  constructor() {\n    this.publisher = new Redis(process.env.REDIS_URL);\n    this.subscriber = new Redis(process.env.REDIS_URL);\n\n    // Listen for messages\n    this.subscriber.on(&#39;message&#39;, (channel, message) =&gt; {\n      this.handleMessage(channel, JSON.parse(message));\n    });\n  }\n\n  async publish(channel: string, data: any): Promise&lt;void&gt; {\n    await this.publisher.publish(channel, JSON.stringify(data));\n  }\n\n  async subscribe(channel: string, handler: MessageHandler): Promise&lt;void&gt; {\n    if (!this.handlers.has(channel)) {\n      this.handlers.set(channel, new Set());\n      await this.subscriber.subscribe(channel);\n    }\n\n    this.handlers.get(channel)!.add(handler);\n  }\n\n  private handleMessage(channel: string, data: any): void {\n    const handlers = this.handlers.get(channel);\n    if (handlers) {\n      handlers.forEach((handler) =&gt; handler(data));\n    }\n  }\n}\n</code></pre>\n<h3>3. Event Publishers (Business Logic)</h3>\n<p>Domain events trigger real-time updates:</p>\n<pre><code class=\"language-typescript\">// application/events/time-entry-created.handler.ts\n@EventsHandler(TimeEntryCreatedEvent)\nexport class TimeEntryCreatedRealtimeHandler {\n  constructor(private readonly redisPubSub: RedisPubSubService) {}\n\n  async handle(event: TimeEntryCreatedEvent): Promise&lt;void&gt; {\n    // Publish to Redis so all server instances can broadcast\n    await this.redisPubSub.publish(&#39;time-entry:created&#39;, {\n      tenantId: event.tenantId,\n      projectId: event.projectId,\n      timeEntry: event.timeEntry,\n    });\n  }\n}\n</code></pre>\n<h2>Implementing Specific Real-Time Features</h2>\n<h3>Feature 1: Live Time Entry Updates</h3>\n<p>When someone logs time, everyone viewing that project sees it instantly:</p>\n<pre><code class=\"language-typescript\">// Server side - subscribe to Redis and emit to WebSocket clients\n@WebSocketGateway()\nexport class RealtimeGateway implements OnGatewayInit {\n  @WebSocketServer()\n  server: Server;\n\n  async afterInit() {\n    // Subscribe to time entry events from Redis\n    await this.redisPubSub.subscribe(&#39;time-entry:created&#39;, (data) =&gt; {\n      // Emit to all clients viewing this project\n      this.server\n        .to(`project:${data.projectId}`)\n        .emit(&#39;time-entry:created&#39;, data.timeEntry);\n    });\n  }\n\n  @SubscribeMessage(&#39;watch-project&#39;)\n  async handleWatchProject(client: Socket, projectId: string) {\n    // Verify user has access to project\n    const hasAccess = await this.checkProjectAccess(\n      client.data.userId,\n      projectId\n    );\n\n    if (!hasAccess) {\n      throw new WsException(&#39;Unauthorized&#39;);\n    }\n\n    // Join project room to receive updates\n    client.join(`project:${projectId}`);\n\n    return { success: true };\n  }\n\n  @SubscribeMessage(&#39;unwatch-project&#39;)\n  async handleUnwatchProject(client: Socket, projectId: string) {\n    client.leave(`project:${projectId}`);\n    return { success: true };\n  }\n}\n</code></pre>\n<p>Client side with React:</p>\n<pre><code class=\"language-typescript\">// hooks/useProjectRealtimeUpdates.ts\nimport { io, Socket } from &#39;socket.io-client&#39;;\nimport { useEffect, useState } from &#39;react&#39;;\n\nexport function useProjectRealtimeUpdates(projectId: string) {\n  const [socket, setSocket] = useState&lt;Socket | null&gt;(null);\n  const [timeEntries, setTimeEntries] = useState&lt;TimeEntry[]&gt;([]);\n\n  useEffect(() =&gt; {\n    const token = getAuthToken();\n\n    const socketInstance = io(process.env.NEXT_PUBLIC_WS_URL, {\n      auth: { token },\n    });\n\n    socketInstance.on(&#39;connect&#39;, () =&gt; {\n      console.log(&#39;WebSocket connected&#39;);\n\n      // Start watching project\n      socketInstance.emit(&#39;watch-project&#39;, projectId);\n    });\n\n    socketInstance.on(&#39;time-entry:created&#39;, (timeEntry: TimeEntry) =&gt; {\n      setTimeEntries((prev) =&gt; [...prev, timeEntry]);\n\n      // Show toast notification\n      toast.success(`New time entry: ${timeEntry.hours} hours logged`);\n    });\n\n    socketInstance.on(&#39;disconnect&#39;, () =&gt; {\n      console.log(&#39;WebSocket disconnected&#39;);\n    });\n\n    setSocket(socketInstance);\n\n    return () =&gt; {\n      socketInstance.emit(&#39;unwatch-project&#39;, projectId);\n      socketInstance.disconnect();\n    };\n  }, [projectId]);\n\n  return { socket, timeEntries };\n}\n</code></pre>\n<p>Usage in component:</p>\n<pre><code class=\"language-typescript\">const ProjectTimeEntries = ({ projectId }) =&gt; {\n  const { timeEntries } = useProjectRealtimeUpdates(projectId);\n\n  return (\n    &lt;div&gt;\n      {timeEntries.map((entry) =&gt; (\n        \n      ))}\n    &lt;/div&gt;\n  );\n};\n</code></pre>\n<h3>Feature 2: Real-Time Notifications</h3>\n<p>Push notifications to specific users:</p>\n<pre><code class=\"language-typescript\">@EventsHandler(TaskAssignedEvent)\nexport class TaskAssignedNotificationHandler {\n  constructor(private readonly redisPubSub: RedisPubSubService) {}\n\n  async handle(event: TaskAssignedEvent): Promise&lt;void&gt; {\n    // Send notification to assigned user\n    await this.redisPubSub.publish(&#39;notification&#39;, {\n      userId: event.assigneeId,\n      type: &#39;task-assigned&#39;,\n      title: &#39;New Task Assigned&#39;,\n      message: `You&#39;ve been assigned to: ${event.taskName}`,\n      data: {\n        taskId: event.taskId,\n        projectId: event.projectId,\n      },\n      timestamp: new Date(),\n    });\n  }\n}\n\n// Gateway listens and emits to specific user\n@WebSocketGateway()\nexport class RealtimeGateway {\n  async afterInit() {\n    await this.redisPubSub.subscribe(&#39;notification&#39;, (data) =&gt; {\n      // Send to specific user&#39;s room\n      this.server.to(`user:${data.userId}`).emit(&#39;notification&#39;, data);\n    });\n  }\n}\n</code></pre>\n<p>Client notification handler:</p>\n<pre><code class=\"language-typescript\">export function useNotifications() {\n  const [notifications, setNotifications] = useState&lt;Notification[]&gt;([]);\n\n  useEffect(() =&gt; {\n    const socket = io(process.env.NEXT_PUBLIC_WS_URL, {\n      auth: { token: getAuthToken() },\n    });\n\n    socket.on(&#39;notification&#39;, (notification: Notification) =&gt; {\n      setNotifications((prev) =&gt; [notification, ...prev]);\n\n      // Show desktop notification if permitted\n      if (Notification.permission === &#39;granted&#39;) {\n        new Notification(notification.title, {\n          body: notification.message,\n          icon: &#39;/icon.png&#39;,\n        });\n      }\n\n      // Play sound\n      new Audio(&#39;/notification.mp3&#39;).play();\n    });\n\n    return () =&gt; socket.disconnect();\n  }, []);\n\n  return { notifications };\n}\n</code></pre>\n<h3>Feature 3: User Presence</h3>\n<p>Show who&#39;s currently viewing a project:</p>\n<pre><code class=\"language-typescript\">@WebSocketGateway()\nexport class RealtimeGateway {\n  private presenceMap: Map&lt;string, Set&lt;string&gt;&gt; = new Map(); // projectId -&gt; Set&lt;userId&gt;\n\n  @SubscribeMessage(&#39;watch-project&#39;)\n  async handleWatchProject(client: Socket, projectId: string) {\n    const userId = client.data.userId;\n\n    // Add to presence map\n    if (!this.presenceMap.has(projectId)) {\n      this.presenceMap.set(projectId, new Set());\n    }\n    this.presenceMap.get(projectId)!.add(userId);\n\n    // Join room\n    client.join(`project:${projectId}`);\n\n    // Notify others of new viewer\n    this.server.to(`project:${projectId}`).emit(&#39;user-joined&#39;, {\n      userId,\n      projectId,\n    });\n\n    // Send current viewers to newly joined user\n    const viewers = Array.from(this.presenceMap.get(projectId)!);\n    client.emit(&#39;current-viewers&#39;, { projectId, viewers });\n\n    return { success: true };\n  }\n\n  @SubscribeMessage(&#39;unwatch-project&#39;)\n  async handleUnwatchProject(client: Socket, projectId: string) {\n    const userId = client.data.userId;\n\n    // Remove from presence map\n    this.presenceMap.get(projectId)?.delete(userId);\n\n    client.leave(`project:${projectId}`);\n\n    // Notify others\n    this.server.to(`project:${projectId}`).emit(&#39;user-left&#39;, {\n      userId,\n      projectId,\n    });\n\n    return { success: true };\n  }\n\n  async handleDisconnect(client: Socket) {\n    const userId = client.data.userId;\n\n    // Remove from all projects\n    for (const [projectId, viewers] of this.presenceMap.entries()) {\n      if (viewers.has(userId)) {\n        viewers.delete(userId);\n        this.server.to(`project:${projectId}`).emit(&#39;user-left&#39;, {\n          userId,\n          projectId,\n        });\n      }\n    }\n  }\n}\n</code></pre>\n<p>Display presence in UI:</p>\n<pre><code class=\"language-typescript\">const ProjectPresence = ({ projectId }) =&gt; {\n  const [viewers, setViewers] = useState&lt;string[]&gt;([]);\n  const socket = useWebSocket();\n\n  useEffect(() =&gt; {\n    socket.on(&#39;current-viewers&#39;, ({ viewers }) =&gt; {\n      setViewers(viewers);\n    });\n\n    socket.on(&#39;user-joined&#39;, ({ userId }) =&gt; {\n      setViewers((prev) =&gt; [...prev, userId]);\n    });\n\n    socket.on(&#39;user-left&#39;, ({ userId }) =&gt; {\n      setViewers((prev) =&gt; prev.filter((id) =&gt; id !== userId));\n    });\n  }, [socket]);\n\n  return (\n    &lt;div className=&quot;flex items-center space-x-2&quot;&gt;\n      \n      &lt;span&gt;{viewers.length} viewing&lt;/span&gt;\n      &lt;div className=&quot;flex -space-x-2&quot;&gt;\n        {viewers.slice(0, 3).map((userId) =&gt; (\n          \n        ))}\n        {viewers.length &gt; 3 &amp;&amp; (\n          &lt;div className=&quot;flex items-center justify-center w-8 h-8 rounded-full bg-gray-200&quot;&gt;\n            +{viewers.length - 3}\n          &lt;/div&gt;\n        )}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  );\n};\n</code></pre>\n<h2>Scaling WebSockets with Redis</h2>\n<p>The critical insight: WebSocket connections are stateful and tied to specific server instances. Without Redis, messages only go to clients connected to the same server.</p>\n<h3>The Problem: Multiple Server Instances</h3>\n<pre><code>User A -&gt; Server 1\nUser B -&gt; Server 2\n\nUser A creates time entry\nServer 1 gets event\nServer 1 emits to its clients (only User A)\nUser B never gets update!\n</code></pre>\n<h3>The Solution: Redis Pub/Sub</h3>\n<pre><code>User A -&gt; Server 1\nUser B -&gt; Server 2\n\nUser A creates time entry\nServer 1 publishes to Redis\nAll servers (1 and 2) receive from Redis\nBoth servers emit to their clients\nUsers A and B both get update!\n</code></pre>\n<p>Implementation:</p>\n<pre><code class=\"language-typescript\">@WebSocketGateway()\nexport class RealtimeGateway implements OnGatewayInit {\n  @WebSocketServer()\n  server: Server;\n\n  constructor(private readonly redisPubSub: RedisPubSubService) {}\n\n  async afterInit() {\n    // Every server instance subscribes to Redis\n    await this.redisPubSub.subscribe(&#39;*&#39;, (channel, data) =&gt; {\n      // Emit received messages to WebSocket clients\n      this.server.emit(channel, data);\n    });\n  }\n}\n</code></pre>\n<p>This works across any number of server instances, automatically load-balanced.</p>\n<h2>Performance Optimizations</h2>\n<h3>1. Message Throttling</h3>\n<p>Prevent overwhelming clients with too many updates:</p>\n<pre><code class=\"language-typescript\">export class ThrottledEmitter {\n  private queues: Map&lt;string, any[]&gt; = new Map();\n  private timers: Map&lt;string, NodeJS.Timeout&gt; = new Map();\n\n  emit(room: string, event: string, data: any, throttleMs = 1000) {\n    const key = `${room}:${event}`;\n\n    if (!this.queues.has(key)) {\n      this.queues.set(key, []);\n    }\n\n    this.queues.get(key)!.push(data);\n\n    if (!this.timers.has(key)) {\n      this.timers.set(\n        key,\n        setTimeout(() =&gt; {\n          this.flush(room, event, key);\n        }, throttleMs)\n      );\n    }\n  }\n\n  private flush(room: string, event: string, key: string) {\n    const batch = this.queues.get(key) || [];\n\n    if (batch.length &gt; 0) {\n      this.server.to(room).emit(event, batch);\n    }\n\n    this.queues.delete(key);\n    this.timers.delete(key);\n  }\n}\n</code></pre>\n<p>Usage:</p>\n<pre><code class=\"language-typescript\">// Instead of emitting every individual update\nthis.server.to(`project:${projectId}`).emit(&#39;time-entry:created&#39;, entry);\n\n// Batch them\nthis.throttledEmitter.emit(\n  `project:${projectId}`,\n  &#39;time-entry:created&#39;,\n  entry,\n  1000 // Emit at most once per second\n);\n</code></pre>\n<h3>2. Selective Subscription</h3>\n<p>Don&#39;t subscribe to everything. Let clients choose what they want:</p>\n<pre><code class=\"language-typescript\">@SubscribeMessage(&#39;subscribe&#39;)\nasync handleSubscribe(\n  client: Socket,\n  payload: { channels: string[] }\n): Promise&lt;void&gt; {\n  for (const channel of payload.channels) {\n    // Validate access\n    const hasAccess = await this.checkAccess(client.data.userId, channel);\n\n    if (hasAccess) {\n      client.join(channel);\n    }\n  }\n}\n</code></pre>\n<p>Client:</p>\n<pre><code class=\"language-typescript\">socket.emit(&#39;subscribe&#39;, {\n  channels: [\n    `project:${projectId}`,\n    `user:${userId}`,\n    &#39;global-notifications&#39;,\n  ],\n});\n</code></pre>\n<h3>3. Connection Pooling</h3>\n<p>Reuse WebSocket connections across components:</p>\n<pre><code class=\"language-typescript\">// lib/websocket.ts\nlet socket: Socket | null = null;\n\nexport function getWebSocket(): Socket {\n  if (!socket) {\n    socket = io(process.env.NEXT_PUBLIC_WS_URL, {\n      auth: { token: getAuthToken() },\n    });\n  }\n\n  return socket;\n}\n\n// All components share one connection\nconst socket = getWebSocket();\n</code></pre>\n<h2>Monitoring and Debugging</h2>\n<h3>Connection Monitoring</h3>\n<p>Track connection health:</p>\n<pre><code class=\"language-typescript\">@WebSocketGateway()\nexport class RealtimeGateway {\n  private metrics = {\n    totalConnections: 0,\n    activeConnections: 0,\n    messagesPerSecond: 0,\n  };\n\n  handleConnection(client: Socket) {\n    this.metrics.totalConnections++;\n    this.metrics.activeConnections++;\n\n    // Expose metrics endpoint\n    this.metricsService.gauge(&#39;websocket.active_connections&#39;, this.metrics.activeConnections);\n  }\n\n  handleDisconnect(client: Socket) {\n    this.metrics.activeConnections--;\n    this.metricsService.gauge(&#39;websocket.active_connections&#39;, this.metrics.activeConnections);\n  }\n}\n</code></pre>\n<h3>Message Logging</h3>\n<p>Log messages for debugging (disable in production):</p>\n<pre><code class=\"language-typescript\">if (process.env.NODE_ENV === &#39;development&#39;) {\n  socket.onAny((event, ...args) =&gt; {\n    console.log(`[WebSocket] ${event}`, args);\n  });\n}\n</code></pre>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Redis is essential for multi-server</strong>: Don&#39;t try to build real-time without a pub/sub layer</li>\n<li><strong>Authentication is critical</strong>: Always verify tokens on connection</li>\n<li><strong>Throttle aggressively</strong>: Clients can&#39;t handle 100 updates/second</li>\n<li><strong>Rooms are your friend</strong>: Group clients by what they&#39;re watching</li>\n<li><strong>Handle disconnections gracefully</strong>: Networks are unreliable</li>\n<li><strong>Monitor everything</strong>: Track connections, message rates, errors</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Building real-time features with WebSockets and Redis transformed Catalyst PSA into a collaborative platform. Team members see updates instantly, get notified immediately, and can see who else is working on projects in real-time.</p>\n<p>The architecture—WebSocket gateways + Redis pub/sub + domain events—scales horizontally and has handled millions of messages reliably.</p>\n<p>After 27 years of building applications, I can say that real-time features are no longer optional for modern enterprise software. Users expect instant updates, and with WebSockets and Redis, delivering that experience is achievable at scale.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/building-realtime-features-websockets-redis/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-10-11 00:00:00",
            "updated_at": "2024-10-11 00:00:00",
            "published_at": "2024-10-11 00:00:00",
            "custom_excerpt": "Implementing scalable real-time collaboration features in enterprise SaaS using WebSockets, Redis Pub/Sub, and React, with lessons from building live updates in Catalyst PSA."
          },
          {
            "id": "25",
            "title": "Testing Strategies for Enterprise Applications: Jest, Enzyme, and Beyond",
            "slug": "testing-strategies-enterprise-apps",
            "html": "<p>Over 27 years of development, I&#39;ve seen testing practices evolve dramatically. From manual QA to comprehensive automated test suites, here&#39;s what I&#39;ve learned about building reliable test strategies for enterprise applications.</p>\n<h2>The Testing Pyramid</h2>\n<p>A solid testing strategy follows the testing pyramid - lots of unit tests, fewer integration tests, and minimal end-to-end tests.</p>\n<h3>Unit Tests with Jest</h3>\n<p>Jest has become my go-to testing framework for JavaScript/TypeScript applications. Here&#39;s how we structured tests at Nutrien:</p>\n<pre><code class=\"language-typescript\">// CustomButton.test.tsx\nimport { render, screen, fireEvent } from &#39;@testing-library/react&#39;;\nimport &#39;@testing-library/jest-dom&#39;;\nimport { CustomButton } from &#39;./CustomButton&#39;;\n\ndescribe(&#39;CustomButton&#39;, () =&gt; {\n  it(&#39;renders with correct text&#39;, () =&gt; {\n    render();\n    expect(screen.getByText(&#39;Click Me&#39;)).toBeInTheDocument();\n  });\n\n  it(&#39;calls onPress when clicked&#39;, () =&gt; {\n    const mockOnPress = jest.fn();\n    render();\n\n    fireEvent.click(screen.getByRole(&#39;button&#39;));\n    expect(mockOnPress).toHaveBeenCalledTimes(1);\n  });\n\n  it(&#39;shows loading state correctly&#39;, () =&gt; {\n    render(\n      \n    );\n\n    expect(screen.queryByText(&#39;Submit&#39;)).not.toBeInTheDocument();\n    expect(screen.getByRole(&#39;status&#39;)).toBeInTheDocument(); // ActivityIndicator\n  });\n\n  it(&#39;is disabled when loading&#39;, () =&gt; {\n    const mockOnPress = jest.fn();\n    render(\n      \n    );\n\n    fireEvent.click(screen.getByRole(&#39;button&#39;));\n    expect(mockOnPress).not.toHaveBeenCalled();\n  });\n\n  it(&#39;applies correct variant styles&#39;, () =&gt; {\n    const { container, rerender } = render(\n      \n    );\n\n    expect(container.firstChild).toHaveClass(&#39;button-primary&#39;);\n\n    rerender(\n      \n    );\n\n    expect(container.firstChild).toHaveClass(&#39;button-danger&#39;);\n  });\n});\n</code></pre>\n<h3>Testing Services with Mocks</h3>\n<p>Service layer tests require mocking external dependencies:</p>\n<pre><code class=\"language-typescript\">// referrals.service.test.ts\nimport { ReferralsService } from &#39;./referrals.service&#39;;\nimport { PrismaService } from &#39;../prisma/prisma.service&#39;;\nimport { EmailService } from &#39;../email/email.service&#39;;\n\n// Mock the dependencies\njest.mock(&#39;../prisma/prisma.service&#39;);\njest.mock(&#39;../email/email.service&#39;);\n\ndescribe(&#39;ReferralsService&#39;, () =&gt; {\n  let service: ReferralsService;\n  let prisma: jest.Mocked&lt;PrismaService&gt;;\n  let emailService: jest.Mocked&lt;EmailService&gt;;\n\n  beforeEach(() =&gt; {\n    // Create mock instances\n    prisma = new PrismaService() as jest.Mocked&lt;PrismaService&gt;;\n    emailService = new EmailService() as jest.Mocked&lt;EmailService&gt;;\n\n    service = new ReferralsService(prisma, emailService);\n  });\n\n  afterEach(() =&gt; {\n    jest.clearAllMocks();\n  });\n\n  describe(&#39;create&#39;, () =&gt; {\n    it(&#39;creates a referral and sends email notification&#39;, async () =&gt; {\n      const mockReferral = {\n        id: &#39;123&#39;,\n        motherName: &#39;Jane Doe&#39;,\n        status: &#39;pending&#39;,\n        priority: &#39;HIGH&#39;,\n        userId: &#39;user123&#39;,\n        createdAt: new Date(),\n        updatedAt: new Date(),\n      };\n\n      const createDto = {\n        motherName: &#39;Jane Doe&#39;,\n        priority: &#39;HIGH&#39;,\n        userId: &#39;user123&#39;,\n        coordinatorEmail: &#39;coordinator@example.com&#39;,\n      };\n\n      // Mock Prisma response\n      prisma.referral.create.mockResolvedValue(mockReferral);\n\n      // Mock email service\n      emailService.sendReferralNotification.mockResolvedValue(undefined);\n\n      const result = await service.create(createDto);\n\n      // Verify Prisma was called correctly\n      expect(prisma.referral.create).toHaveBeenCalledWith({\n        data: expect.objectContaining({\n          motherName: &#39;Jane Doe&#39;,\n          priority: &#39;HIGH&#39;,\n        }),\n        include: expect.any(Object),\n      });\n\n      // Verify email was sent\n      expect(emailService.sendReferralNotification).toHaveBeenCalledWith(\n        &#39;coordinator@example.com&#39;,\n        expect.objectContaining({\n          motherName: &#39;Jane Doe&#39;,\n          priority: &#39;HIGH&#39;,\n        })\n      );\n\n      expect(result).toEqual(mockReferral);\n    });\n\n    it(&#39;still creates referral if email fails&#39;, async () =&gt; {\n      const mockReferral = {\n        id: &#39;123&#39;,\n        motherName: &#39;Jane Doe&#39;,\n        status: &#39;pending&#39;,\n        priority: &#39;HIGH&#39;,\n        userId: &#39;user123&#39;,\n        createdAt: new Date(),\n        updatedAt: new Date(),\n      };\n\n      prisma.referral.create.mockResolvedValue(mockReferral);\n      emailService.sendReferralNotification.mockRejectedValue(\n        new Error(&#39;Email service down&#39;)\n      );\n\n      // Should not throw - email failure shouldn&#39;t break referral creation\n      const result = await service.create({\n        motherName: &#39;Jane Doe&#39;,\n        priority: &#39;HIGH&#39;,\n        userId: &#39;user123&#39;,\n        coordinatorEmail: &#39;coordinator@example.com&#39;,\n      });\n\n      expect(result).toEqual(mockReferral);\n    });\n  });\n\n  describe(&#39;findByPriority&#39;, () =&gt; {\n    it(&#39;returns referrals filtered by priority&#39;, async () =&gt; {\n      const mockReferrals = [\n        { id: &#39;1&#39;, priority: &#39;HIGH&#39;, motherName: &#39;Jane&#39; },\n        { id: &#39;2&#39;, priority: &#39;HIGH&#39;, motherName: &#39;Mary&#39; },\n      ];\n\n      prisma.referral.findMany.mockResolvedValue(mockReferrals as any);\n\n      const result = await service.findByPriority(&#39;HIGH&#39;);\n\n      expect(prisma.referral.findMany).toHaveBeenCalledWith({\n        where: { priority: &#39;HIGH&#39; },\n        include: expect.any(Object),\n        orderBy: { createdAt: &#39;desc&#39; },\n      });\n\n      expect(result).toEqual(mockReferrals);\n    });\n  });\n});\n</code></pre>\n<h3>Integration Tests</h3>\n<p>Integration tests verify that multiple parts work together:</p>\n<pre><code class=\"language-typescript\">// referrals.integration.test.ts\nimport { Test } from &#39;@nestjs/testing&#39;;\nimport { INestApplication } from &#39;@nestjs/common&#39;;\nimport * as request from &#39;supertest&#39;;\nimport { AppModule } from &#39;../app.module&#39;;\nimport { PrismaService } from &#39;../prisma/prisma.service&#39;;\n\ndescribe(&#39;Referrals (Integration)&#39;, () =&gt; {\n  let app: INestApplication;\n  let prisma: PrismaService;\n\n  beforeAll(async () =&gt; {\n    const moduleRef = await Test.createTestingModule({\n      imports: [AppModule],\n    }).compile();\n\n    app = moduleRef.createNestApplication();\n    await app.init();\n\n    prisma = app.get(PrismaService);\n  });\n\n  afterAll(async () =&gt; {\n    await app.close();\n  });\n\n  beforeEach(async () =&gt; {\n    // Clean database before each test\n    await prisma.referral.deleteMany();\n    await prisma.user.deleteMany();\n  });\n\n  describe(&#39;POST /referrals&#39;, () =&gt; {\n    it(&#39;creates a new referral&#39;, async () =&gt; {\n      // Create a test user\n      const user = await prisma.user.create({\n        data: {\n          email: &#39;test@example.com&#39;,\n          firstName: &#39;Test&#39;,\n          lastName: &#39;User&#39;,\n          role: &#39;ADVOCATE&#39;,\n          passwordHash: &#39;hashed&#39;,\n        },\n      });\n\n      const response = await request(app.getHttpServer())\n        .post(&#39;/referrals&#39;)\n        .set(&#39;Authorization&#39;, `Bearer ${testToken}`)\n        .send({\n          motherName: &#39;Jane Doe&#39;,\n          priority: &#39;HIGH&#39;,\n          userId: user.id,\n          notes: &#39;Urgent case&#39;,\n        })\n        .expect(201);\n\n      expect(response.body).toMatchObject({\n        motherName: &#39;Jane Doe&#39;,\n        priority: &#39;HIGH&#39;,\n        status: &#39;pending&#39;,\n      });\n\n      // Verify in database\n      const referral = await prisma.referral.findUnique({\n        where: { id: response.body.id },\n      });\n\n      expect(referral).toBeTruthy();\n      expect(referral.motherName).toBe(&#39;Jane Doe&#39;);\n    });\n\n    it(&#39;returns 401 without authentication&#39;, async () =&gt; {\n      await request(app.getHttpServer())\n        .post(&#39;/referrals&#39;)\n        .send({\n          motherName: &#39;Jane Doe&#39;,\n          priority: &#39;HIGH&#39;,\n        })\n        .expect(401);\n    });\n  });\n});\n</code></pre>\n<h3>Enzyme for React Native</h3>\n<p>At Nutrien, we used Enzyme alongside Jest for React Native components:</p>\n<pre><code class=\"language-typescript\">// DatePicker.enzyme.test.tsx\nimport { shallow, mount } from &#39;enzyme&#39;;\nimport { DatePicker } from &#39;./DatePicker&#39;;\n\ndescribe(&#39;DatePicker (Enzyme)&#39;, () =&gt; {\n  it(&#39;renders correctly&#39;, () =&gt; {\n    const wrapper = shallow(\n      \n    );\n\n    expect(wrapper.find(&#39;TouchableOpacity&#39;)).toHaveLength(1);\n  });\n\n  it(&#39;opens modal when pressed&#39;, () =&gt; {\n    const wrapper = shallow(\n      \n    );\n\n    expect(wrapper.find(&#39;Modal&#39;).prop(&#39;visible&#39;)).toBe(false);\n\n    wrapper.find(&#39;TouchableOpacity&#39;).simulate(&#39;press&#39;);\n\n    expect(wrapper.find(&#39;Modal&#39;).prop(&#39;visible&#39;)).toBe(true);\n  });\n\n  it(&#39;calls onChange with selected date&#39;, () =&gt; {\n    const mockOnChange = jest.fn();\n    const wrapper = mount(\n      \n    );\n\n    const newDate = new Date(&#39;2024-12-31&#39;);\n\n    wrapper.find(&#39;DateTimePicker&#39;).prop(&#39;onChange&#39;)(null, newDate);\n\n    expect(mockOnChange).toHaveBeenCalledWith(newDate);\n  });\n});\n</code></pre>\n<h2>Testing Best Practices</h2>\n<h3>1. Test Behavior, Not Implementation</h3>\n<pre><code class=\"language-typescript\">// Bad - tests implementation\nit(&#39;calls setState with the correct value&#39;, () =&gt; {\n  const wrapper = shallow();\n  wrapper.instance().setState = jest.fn();\n  wrapper.find(&#39;button&#39;).simulate(&#39;click&#39;);\n  expect(wrapper.instance().setState).toHaveBeenCalledWith({ count: 1 });\n});\n\n// Good - tests behavior\nit(&#39;increments counter when button is clicked&#39;, () =&gt; {\n  render();\n  fireEvent.click(screen.getByRole(&#39;button&#39;));\n  expect(screen.getByText(&#39;Count: 1&#39;)).toBeInTheDocument();\n});\n</code></pre>\n<h3>2. Use Data-Driven Tests</h3>\n<pre><code class=\"language-typescript\">describe(&#39;formatPhoneNumber&#39;, () =&gt; {\n  it.each([\n    [&#39;1234567890&#39;, &#39;+1 (123) 456-7890&#39;],\n    [&#39;+11234567890&#39;, &#39;+1 (123) 456-7890&#39;],\n    [&#39;123-456-7890&#39;, &#39;+1 (123) 456-7890&#39;],\n    [&#39;(123) 456-7890&#39;, &#39;+1 (123) 456-7890&#39;],\n  ])(&#39;formats %s as %s&#39;, (input, expected) =&gt; {\n    expect(formatPhoneNumber(input)).toBe(expected);\n  });\n});\n</code></pre>\n<h3>3. Maintain Test Fixtures</h3>\n<pre><code class=\"language-typescript\">// test/fixtures/users.ts\nexport const testUsers = {\n  admin: {\n    id: &#39;admin-1&#39;,\n    email: &#39;admin@example.com&#39;,\n    role: &#39;ADMIN&#39;,\n    firstName: &#39;Admin&#39;,\n    lastName: &#39;User&#39;,\n  },\n  coordinator: {\n    id: &#39;coord-1&#39;,\n    email: &#39;coordinator@example.com&#39;,\n    role: &#39;COORDINATOR&#39;,\n    firstName: &#39;Coordinator&#39;,\n    lastName: &#39;User&#39;,\n  },\n};\n\n// Use in tests\nimport { testUsers } from &#39;../fixtures/users&#39;;\n\nit(&#39;allows admin to view all referrals&#39;, async () =&gt; {\n  const response = await request(app.getHttpServer())\n    .get(&#39;/referrals&#39;)\n    .set(&#39;Authorization&#39;, `Bearer ${getTokenFor(testUsers.admin)}`)\n    .expect(200);\n});\n</code></pre>\n<h2>CI/CD Integration</h2>\n<p>Tests are only valuable if they run automatically:</p>\n<pre><code class=\"language-yaml\"># .circleci/config.yml\nversion: 2.1\n\njobs:\n  test:\n    docker:\n      - image: circleci/node:16\n      - image: circleci/postgres:13\n        environment:\n          POSTGRES_USER: test\n          POSTGRES_PASSWORD: test\n          POSTGRES_DB: test_db\n\n    steps:\n      - checkout\n\n      - restore_cache:\n          keys:\n            - v1-dependencies-{{ checksum &quot;package.json&quot; }}\n\n      - run: npm install\n\n      - save_cache:\n          paths:\n            - node_modules\n          key: v1-dependencies-{{ checksum &quot;package.json&quot; }}\n\n      - run:\n          name: Run unit tests\n          command: npm test -- --coverage\n\n      - run:\n          name: Run integration tests\n          command: npm run test:integration\n\n      - store_test_results:\n          path: test-results\n\n      - store_artifacts:\n          path: coverage\n\nworkflows:\n  version: 2\n  test-and-deploy:\n    jobs:\n      - test\n</code></pre>\n<h2>Conclusion</h2>\n<p>Good testing isn&#39;t about achieving 100% coverage - it&#39;s about testing the right things in the right ways. Focus on testing behavior, maintain clear test code, and integrate testing into your development workflow.</p>\n<p>After 27 years, I can say with confidence: the upfront investment in testing pays massive dividends in reduced bugs, easier refactoring, and better sleep at night.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/testing-strategies-enterprise-apps/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-10-11 00:00:00",
            "updated_at": "2024-10-11 00:00:00",
            "published_at": "2024-10-11 00:00:00",
            "custom_excerpt": "Testing strategies from 27 years of building enterprise applications. Real-world approaches used at Nutrien, Verizon, and Servant—from unit tests to end-to-end testing at scale."
          },
          {
            "id": "26",
            "title": "CI/CD Deployment Strategies: From CircleCI to Railway",
            "slug": "cicd-deployment-strategies",
            "html": "<p>Deployment automation has come a long way. From manual FTP uploads in the early 2000s to sophisticated CI/CD pipelines today, I&#39;ve experienced the full evolution. Here&#39;s what works in 2024.</p>\n<h2>The Evolution of My Deployment Practices</h2>\n<p><strong>Early 2000s</strong>: Manual FTP uploads, crossing fingers\n<strong>2010s</strong>: Capistrano, then Jenkins\n<strong>Recent years</strong>: CircleCI, Docker, Kubernetes, Railway, Vercel</p>\n<p>Each evolution brought new capabilities and reduced deployment anxiety.</p>\n<h2>CircleCI: The Workhorse</h2>\n<p>CircleCI has been my primary CI/CD platform for the past several years. Here&#39;s a production-ready configuration from the Verizon NFT project:</p>\n<pre><code class=\"language-yaml\"># .circleci/config.yml\nversion: 2.1\n\norbs:\n  node: circleci/node@5.0.2\n  docker: circleci/docker@2.1.4\n  aws-ecr: circleci/aws-ecr@8.1.2\n  aws-ecs: circleci/aws-ecs@3.2.0\n\njobs:\n  test:\n    docker:\n      - image: cimg/node:16.20\n      - image: cimg/postgres:13.8\n        environment:\n          POSTGRES_USER: test_user\n          POSTGRES_PASSWORD: test_pass\n          POSTGRES_DB: test_db\n      - image: cimg/redis:7.0\n\n    steps:\n      - checkout\n\n      - restore_cache:\n          keys:\n            - v1-dependencies-{{ checksum &quot;package-lock.json&quot; }}\n            - v1-dependencies-\n\n      - run:\n          name: Install dependencies\n          command: npm ci\n\n      - save_cache:\n          paths:\n            - node_modules\n          key: v1-dependencies-{{ checksum &quot;package-lock.json&quot; }}\n\n      - run:\n          name: Wait for PostgreSQL\n          command: |\n            dockerize -wait tcp://localhost:5432 -timeout 1m\n\n      - run:\n          name: Run database migrations\n          command: npm run migrate:test\n          environment:\n            DATABASE_URL: postgresql://test_user:test_pass@localhost:5432/test_db\n\n      - run:\n          name: Run linter\n          command: npm run lint\n\n      - run:\n          name: Run unit tests\n          command: npm test -- --coverage --maxWorkers=2\n\n      - run:\n          name: Run integration tests\n          command: npm run test:integration\n          environment:\n            DATABASE_URL: postgresql://test_user:test_pass@localhost:5432/test_db\n            REDIS_URL: redis://localhost:6379\n\n      - store_test_results:\n          path: test-results\n\n      - store_artifacts:\n          path: coverage\n          destination: coverage\n\n  build-and-push:\n    docker:\n      - image: cimg/node:16.20\n\n    steps:\n      - checkout\n      - setup_remote_docker:\n          docker_layer_caching: true\n\n      - run:\n          name: Build application\n          command: npm run build\n\n      - aws-ecr/build-and-push-image:\n          repo: nft-collectibles-api\n          tag: ${CIRCLE_SHA1},latest\n          dockerfile: Dockerfile\n          path: .\n\n  deploy-staging:\n    docker:\n      - image: cimg/base:stable\n\n    steps:\n      - aws-ecs/update-service:\n          cluster: nft-staging-cluster\n          service-name: nft-api-service\n          container-image-name-updates: &gt;\n            container=nft-api,\n            tag=${CIRCLE_SHA1}\n          force-new-deployment: true\n\n  deploy-production:\n    docker:\n      - image: cimg/base:stable\n\n    steps:\n      - aws-ecs/update-service:\n          cluster: nft-production-cluster\n          service-name: nft-api-service\n          container-image-name-updates: &gt;\n            container=nft-api,\n            tag=${CIRCLE_SHA1}\n          force-new-deployment: true\n\nworkflows:\n  version: 2\n  test-build-deploy:\n    jobs:\n      - test\n\n      - build-and-push:\n          requires:\n            - test\n          filters:\n            branches:\n              only:\n                - develop\n                - main\n\n      - deploy-staging:\n          requires:\n            - build-and-push\n          filters:\n            branches:\n              only: develop\n\n      - hold-for-approval:\n          type: approval\n          requires:\n            - build-and-push\n          filters:\n            branches:\n              only: main\n\n      - deploy-production:\n          requires:\n            - hold-for-approval\n          filters:\n            branches:\n              only: main\n</code></pre>\n<h2>Docker Multi-Stage Builds</h2>\n<p>Efficient Docker images are crucial for fast deployments:</p>\n<pre><code class=\"language-dockerfile\"># Dockerfile\n# Stage 1: Dependencies\nFROM node:16-alpine AS deps\nWORKDIR /app\n\nCOPY package.json package-lock.json ./\nRUN npm ci --only=production\n\n# Stage 2: Build\nFROM node:16-alpine AS builder\nWORKDIR /app\n\nCOPY package.json package-lock.json ./\nRUN npm ci\n\nCOPY . .\nRUN npm run build\n\n# Stage 3: Production\nFROM node:16-alpine AS runner\nWORKDIR /app\n\nENV NODE_ENV production\n\n# Create non-root user\nRUN addgroup --system --gid 1001 nodejs\nRUN adduser --system --uid 1001 nestjs\n\n# Copy necessary files\nCOPY --from=deps --chown=nestjs:nodejs /app/node_modules ./node_modules\nCOPY --from=builder --chown=nestjs:nodejs /app/dist ./dist\nCOPY --from=builder --chown=nestjs:nodejs /app/package.json ./\n\nUSER nestjs\n\nEXPOSE 3000\n\nCMD [&quot;node&quot;, &quot;dist/main&quot;]\n</code></pre>\n<h2>Kubernetes Deployment</h2>\n<p>For the Verizon project, we deployed to AWS ECS with Kubernetes:</p>\n<pre><code class=\"language-yaml\"># k8s/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nft-api\n  namespace: production\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nft-api\n  template:\n    metadata:\n      labels:\n        app: nft-api\n    spec:\n      containers:\n      - name: nft-api\n        image: ${ECR_REGISTRY}/nft-collectibles-api:${IMAGE_TAG}\n        ports:\n        - containerPort: 3000\n        env:\n        - name: NODE_ENV\n          value: &quot;production&quot;\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: nft-secrets\n              key: database-url\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: nft-secrets\n              key: redis-url\n        - name: ETHEREUM_RPC_URL\n          valueFrom:\n            secretKeyRef:\n              name: nft-secrets\n              key: ethereum-rpc-url\n        resources:\n          requests:\n            memory: &quot;256Mi&quot;\n            cpu: &quot;250m&quot;\n          limits:\n            memory: &quot;512Mi&quot;\n            cpu: &quot;500m&quot;\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nft-api-service\n  namespace: production\nspec:\n  selector:\n    app: nft-api\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 3000\n  type: LoadBalancer\n</code></pre>\n<h2>Railway: Modern Deployment Simplified</h2>\n<p>For the Servant project, we used Railway, which simplified deployment significantly:</p>\n<pre><code class=\"language-toml\"># railway.toml\n[build]\nbuilder = &quot;NIXPACKS&quot;\nbuildCommand = &quot;npm run build&quot;\n\n[deploy]\nstartCommand = &quot;npm run start:prod&quot;\nrestartPolicyType = &quot;ON_FAILURE&quot;\nrestartPolicyMaxRetries = 10\n\n[env]\nNODE_ENV = &quot;production&quot;\n</code></pre>\n<p>Railway configuration in the dashboard:</p>\n<ul>\n<li>Automatic deployments from GitHub</li>\n<li>Environment variables managed in UI</li>\n<li>Built-in PostgreSQL service</li>\n<li>Automatic HTTPS</li>\n</ul>\n<h2>Vercel for Next.js</h2>\n<p>For Next.js frontends, Vercel is unbeatable:</p>\n<pre><code class=\"language-json\">// vercel.json\n{\n  &quot;buildCommand&quot;: &quot;npm run build&quot;,\n  &quot;devCommand&quot;: &quot;npm run dev&quot;,\n  &quot;installCommand&quot;: &quot;npm install&quot;,\n  &quot;framework&quot;: &quot;nextjs&quot;,\n  &quot;regions&quot;: [&quot;iad1&quot;],\n  &quot;env&quot;: {\n    &quot;NEXT_PUBLIC_API_URL&quot;: &quot;@api-url-production&quot;\n  },\n  &quot;build&quot;: {\n    &quot;env&quot;: {\n      &quot;NEXT_PUBLIC_API_URL&quot;: &quot;@api-url-production&quot;\n    }\n  },\n  &quot;headers&quot;: [\n    {\n      &quot;source&quot;: &quot;/(.*)&quot;,\n      &quot;headers&quot;: [\n        {\n          &quot;key&quot;: &quot;X-Content-Type-Options&quot;,\n          &quot;value&quot;: &quot;nosniff&quot;\n        },\n        {\n          &quot;key&quot;: &quot;X-Frame-Options&quot;,\n          &quot;value&quot;: &quot;DENY&quot;\n        },\n        {\n          &quot;key&quot;: &quot;X-XSS-Protection&quot;,\n          &quot;value&quot;: &quot;1; mode=block&quot;\n        }\n      ]\n    }\n  ]\n}\n</code></pre>\n<h2>Environment-Specific Configurations</h2>\n<p>Managing environment variables across environments:</p>\n<pre><code class=\"language-typescript\">// config/configuration.ts\nexport default () =&gt; ({\n  port: parseInt(process.env.PORT, 10) || 3000,\n  database: {\n    url: process.env.DATABASE_URL,\n    ssl: process.env.DATABASE_SSL === &#39;true&#39;,\n  },\n  jwt: {\n    secret: process.env.JWT_SECRET,\n    expiresIn: process.env.JWT_EXPIRES_IN || &#39;1h&#39;,\n  },\n  aws: {\n    region: process.env.AWS_REGION,\n    accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,\n    s3Bucket: process.env.S3_BUCKET_NAME,\n  },\n  redis: {\n    url: process.env.REDIS_URL,\n  },\n  ethereum: {\n    rpcUrl: process.env.ETHEREUM_RPC_URL,\n    contractAddress: process.env.NFT_CONTRACT_ADDRESS,\n    privateKey: process.env.MINTING_PRIVATE_KEY,\n  },\n});\n</code></pre>\n<h2>Database Migration Strategy</h2>\n<p>Always run migrations as part of deployment:</p>\n<pre><code class=\"language-bash\">#!/bin/bash\n# scripts/deploy.sh\n\nset -e\n\necho &quot;Running database migrations...&quot;\nnpm run migrate:deploy\n\necho &quot;Starting application...&quot;\nnpm run start:prod\n</code></pre>\n<p>In CircleCI:</p>\n<pre><code class=\"language-yaml\">- run:\n    name: Run migrations\n    command: npm run migrate:deploy\n    environment:\n      DATABASE_URL: $PRODUCTION_DATABASE_URL\n</code></pre>\n<h2>Monitoring and Rollback</h2>\n<p>Always have a rollback strategy:</p>\n<pre><code class=\"language-bash\"># scripts/rollback.sh\n#!/bin/bash\n\nPREVIOUS_TAG=$(git describe --tags --abbrev=0 HEAD^)\n\necho &quot;Rolling back to $PREVIOUS_TAG&quot;\n\n# Update ECS service to previous version\naws ecs update-service \\\n  --cluster production-cluster \\\n  --service api-service \\\n  --task-definition api:$PREVIOUS_TAG \\\n  --force-new-deployment\n\necho &quot;Rollback initiated&quot;\n</code></pre>\n<h2>Best Practices</h2>\n<ol>\n<li><strong>Always run tests before deployment</strong>: Never deploy broken code</li>\n<li><strong>Use staging environments</strong>: Test deployments in staging first</li>\n<li><strong>Implement health checks</strong>: Kubernetes/ECS should know when your app is ready</li>\n<li><strong>Automate rollbacks</strong>: If deployment fails, auto-rollback</li>\n<li><strong>Monitor deployments</strong>: Use tools like Datadog or New Relic</li>\n<li><strong>Use feature flags</strong>: Deploy code without exposing features</li>\n<li><strong>Keep secrets secret</strong>: Never commit credentials</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Modern CI/CD has made deployment incredibly reliable. The key is choosing the right tools for your needs:</p>\n<ul>\n<li><strong>CircleCI</strong>: Great for complex workflows and custom requirements</li>\n<li><strong>Railway</strong>: Perfect for rapid deployment of full-stack apps</li>\n<li><strong>Vercel</strong>: Unbeatable for Next.js applications</li>\n<li><strong>Docker + Kubernetes</strong>: When you need full control and scale</li>\n</ul>\n<p>The best deployment strategy is one that gives you confidence to deploy frequently and safely.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/cicd-deployment-strategies/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-09-27 00:00:00",
            "updated_at": "2024-09-27 00:00:00",
            "published_at": "2024-09-27 00:00:00",
            "custom_excerpt": "Practical CI/CD patterns learned from deploying applications across AWS, Vercel, Railway, and Docker"
          },
          {
            "id": "27",
            "title": "Prisma vs Drizzle: A Practical Comparison from Production Experience",
            "slug": "prisma-drizzle-orm-comparison",
            "html": "<p>Over the past few years, I&#39;ve built production applications with both Prisma and Drizzle ORM. At Servant, we used Prisma with ZenStack for the EMA project, while recent projects have leveraged Drizzle with PostgreSQL. Here&#39;s what I&#39;ve learned about each.</p>\n<h2>Prisma: The Developer Experience Champion</h2>\n<p>Prisma revolutionized TypeScript ORMs with its schema-first approach and incredible developer experience. Here&#39;s a typical Prisma schema:</p>\n<pre><code class=\"language-prisma\">// schema.prisma\ndatasource db {\n  provider = &quot;postgresql&quot;\n  url      = env(&quot;DATABASE_URL&quot;)\n}\n\ngenerator client {\n  provider = &quot;prisma-client-js&quot;\n}\n\nmodel User {\n  id        String   @id @default(uuid())\n  email     String   @unique\n  firstName String\n  lastName  String\n  role      UserRole\n  referrals Referral[]\n  createdAt DateTime @default(now())\n  updatedAt DateTime @updatedAt\n}\n\nmodel Referral {\n  id          String   @id @default(uuid())\n  userId      String\n  user        User     @relation(fields: [userId], references: [id])\n  motherName  String\n  status      String\n  priority    Priority\n  createdAt   DateTime @default(now())\n  updatedAt   DateTime @updatedAt\n\n  @@index([userId])\n  @@index([status])\n}\n\nenum UserRole {\n  ADMIN\n  COORDINATOR\n  ADVOCATE\n  MOTHER\n}\n\nenum Priority {\n  LOW\n  MEDIUM\n  HIGH\n  URGENT\n}\n</code></pre>\n<p>The auto-generated client is type-safe and intuitive:</p>\n<pre><code class=\"language-typescript\">// Using Prisma in a NestJS service\nimport { Injectable } from &#39;@nestjs/common&#39;;\nimport { PrismaService } from &#39;./prisma.service&#39;;\n\n@Injectable()\nexport class ReferralsService {\n  constructor(private prisma: PrismaService) {}\n\n  async create(data: CreateReferralDto) {\n    return this.prisma.referral.create({\n      data: {\n        motherName: data.motherName,\n        status: &#39;pending&#39;,\n        priority: data.priority,\n        user: {\n          connect: { id: data.userId }\n        }\n      },\n      include: {\n        user: {\n          select: {\n            id: true,\n            email: true,\n            firstName: true,\n            lastName: true\n          }\n        }\n      }\n    });\n  }\n\n  async findAllWithFilters(filters: ReferralFilters) {\n    return this.prisma.referral.findMany({\n      where: {\n        status: filters.status,\n        priority: filters.priority,\n        user: {\n          role: filters.userRole\n        }\n      },\n      include: {\n        user: true\n      },\n      orderBy: {\n        createdAt: &#39;desc&#39;\n      }\n    });\n  }\n}\n</code></pre>\n<h2>Prisma + ZenStack for Authorization</h2>\n<p>ZenStack extends Prisma with authorization rules at the schema level:</p>\n<pre><code class=\"language-prisma\">// With ZenStack extensions\nmodel Referral {\n  id          String   @id @default(uuid())\n  userId      String\n  user        User     @relation(fields: [userId], references: [id])\n  motherName  String\n\n  // ZenStack access policies\n  @@allow(&#39;create&#39;, auth().role == COORDINATOR || auth().role == ADVOCATE)\n  @@allow(&#39;read&#39;, auth().role == ADMIN || userId == auth().id)\n  @@allow(&#39;update&#39;, auth().role == COORDINATOR || userId == auth().id)\n  @@deny(&#39;delete&#39;, true) // Soft delete only\n}\n</code></pre>\n<p>This integrated beautifully with our authentication system and eliminated a lot of manual authorization checking.</p>\n<h2>Drizzle: The SQL-First Approach</h2>\n<p>Drizzle takes a different philosophy - staying closer to SQL while providing type safety. Here&#39;s the equivalent schema:</p>\n<pre><code class=\"language-typescript\">// schema.ts\nimport { pgTable, uuid, varchar, timestamp, pgEnum } from &#39;drizzle-orm/pg-core&#39;;\nimport { relations } from &#39;drizzle-orm&#39;;\n\nexport const userRoleEnum = pgEnum(&#39;user_role&#39;, [\n  &#39;admin&#39;,\n  &#39;coordinator&#39;,\n  &#39;advocate&#39;,\n  &#39;mother&#39;\n]);\n\nexport const priorityEnum = pgEnum(&#39;priority&#39;, [\n  &#39;low&#39;,\n  &#39;medium&#39;,\n  &#39;high&#39;,\n  &#39;urgent&#39;\n]);\n\nexport const users = pgTable(&#39;users&#39;, {\n  id: uuid(&#39;id&#39;).defaultRandom().primaryKey(),\n  email: varchar(&#39;email&#39;, { length: 255 }).notNull().unique(),\n  firstName: varchar(&#39;first_name&#39;, { length: 100 }).notNull(),\n  lastName: varchar(&#39;last_name&#39;, { length: 100 }).notNull(),\n  role: userRoleEnum(&#39;role&#39;).notNull(),\n  createdAt: timestamp(&#39;created_at&#39;).defaultNow().notNull(),\n  updatedAt: timestamp(&#39;updated_at&#39;).defaultNow().notNull(),\n});\n\nexport const referrals = pgTable(&#39;referrals&#39;, {\n  id: uuid(&#39;id&#39;).defaultRandom().primaryKey(),\n  userId: uuid(&#39;user_id&#39;).notNull().references(() =&gt; users.id),\n  motherName: varchar(&#39;mother_name&#39;, { length: 255 }).notNull(),\n  status: varchar(&#39;status&#39;, { length: 50 }).notNull(),\n  priority: priorityEnum(&#39;priority&#39;).notNull(),\n  createdAt: timestamp(&#39;created_at&#39;).defaultNow().notNull(),\n  updatedAt: timestamp(&#39;updated_at&#39;).defaultNow().notNull(),\n});\n\nexport const usersRelations = relations(users, ({ many }) =&gt; ({\n  referrals: many(referrals),\n}));\n\nexport const referralsRelations = relations(referrals, ({ one }) =&gt; ({\n  user: one(users, {\n    fields: [referrals.userId],\n    references: [users.id],\n  }),\n}));\n</code></pre>\n<p>Querying with Drizzle feels closer to SQL:</p>\n<pre><code class=\"language-typescript\">// Using Drizzle in a service\nimport { db } from &#39;./db&#39;;\nimport { referrals, users } from &#39;./schema&#39;;\nimport { eq, and, desc } from &#39;drizzle-orm&#39;;\n\nexport class ReferralsService {\n  async create(data: CreateReferralDto) {\n    const [referral] = await db\n      .insert(referrals)\n      .values({\n        motherName: data.motherName,\n        status: &#39;pending&#39;,\n        priority: data.priority,\n        userId: data.userId,\n      })\n      .returning();\n\n    return referral;\n  }\n\n  async findAllWithFilters(filters: ReferralFilters) {\n    return db\n      .select()\n      .from(referrals)\n      .leftJoin(users, eq(referrals.userId, users.id))\n      .where(\n        and(\n          filters.status ? eq(referrals.status, filters.status) : undefined,\n          filters.priority ? eq(referrals.priority, filters.priority) : undefined\n        )\n      )\n      .orderBy(desc(referrals.createdAt));\n  }\n}\n</code></pre>\n<h2>Key Differences</h2>\n<h3>Migration Workflow</h3>\n<p><strong>Prisma</strong>: Schema-first with automatic migrations</p>\n<pre><code class=\"language-bash\">npx prisma migrate dev --name add_referrals\nnpx prisma generate\n</code></pre>\n<p><strong>Drizzle</strong>: More control, closer to raw SQL</p>\n<pre><code class=\"language-bash\">npx drizzle-kit generate:pg\nnpx drizzle-kit push:pg\n</code></pre>\n<h3>Performance</h3>\n<p>Drizzle is generally lighter weight. For complex queries, Drizzle gives you more control:</p>\n<pre><code class=\"language-typescript\">// Complex query with Drizzle\nconst result = await db\n  .select({\n    id: referrals.id,\n    motherName: referrals.motherName,\n    userName: sql`${users.firstName} || &#39; &#39; || ${users.lastName}`,\n    daysOpen: sql`EXTRACT(DAY FROM ${referrals.createdAt} - NOW())`,\n  })\n  .from(referrals)\n  .leftJoin(users, eq(referrals.userId, users.id))\n  .where(eq(referrals.status, &#39;open&#39;));\n</code></pre>\n<h3>Type Safety</h3>\n<p>Both provide excellent type safety, but Prisma&#39;s generated types feel more polished:</p>\n<pre><code class=\"language-typescript\">// Prisma - incredibly smooth autocomplete\nconst user: User = await prisma.user.findUnique({...})\n\n// Drizzle - also type-safe, slightly more verbose\nconst [user]: typeof users.$inferSelect[] = await db.select()...\n</code></pre>\n<h2>When to Use Each</h2>\n<p><strong>Choose Prisma when:</strong></p>\n<ul>\n<li>Developer experience is top priority</li>\n<li>You want comprehensive migrations and schema management</li>\n<li>You need features like ZenStack for row-level security</li>\n<li>Team is less familiar with SQL</li>\n</ul>\n<p><strong>Choose Drizzle when:</strong></p>\n<ul>\n<li>Performance is critical</li>\n<li>You want to write custom SQL for complex queries</li>\n<li>You prefer staying close to SQL semantics</li>\n<li>Bundle size matters (Drizzle is lighter)</li>\n</ul>\n<h2>Conclusion</h2>\n<p>Both ORMs are excellent choices for TypeScript applications. Prisma offers an unmatched developer experience and ecosystem, while Drizzle provides more control and better performance for SQL-heavy applications.</p>\n<p>In my current projects, I&#39;m leaning toward Drizzle for its lightweight nature and SQL-first approach, but I wouldn&#39;t hesitate to use Prisma for teams that value rapid development and comprehensive tooling.</p>\n<p>The best choice depends on your specific needs, team expertise, and project requirements.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/prisma-drizzle-orm-comparison/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-09-13 00:00:00",
            "updated_at": "2024-09-13 00:00:00",
            "published_at": "2024-09-13 00:00:00",
            "custom_excerpt": "Comparing two popular TypeScript ORMs based on real-world usage in enterprise applications"
          },
          {
            "id": "28",
            "title": "Migrating from Prisma to Drizzle: A Practical Guide",
            "slug": "prisma-to-drizzle-migration",
            "html": "<p>When we migrated the Catalyst PSA platform from Prisma to Drizzle ORM, we were taking a calculated risk. Prisma was working fine, but we were hitting performance walls and wanted more control over our SQL. After successfully migrating a large codebase, I can confidently say it was worth it.</p>\n<p>Here&#39;s everything we learned about migrating from Prisma to Drizzle in a production enterprise application.</p>\n<h2>Why We Migrated</h2>\n<p>Prisma is an excellent ORM. We built Catalyst PSA on it and were mostly happy. But as the platform grew, we encountered issues:</p>\n<h3>Performance Problems</h3>\n<p>Prisma generates queries that aren&#39;t always optimal. We found ourselves writing raw SQL for complex queries, defeating the purpose of an ORM.</p>\n<pre><code class=\"language-typescript\">// Prisma query that generated suboptimal SQL\nconst projects = await prisma.project.findMany({\n  where: {\n    client: {\n      industry: &#39;technology&#39;,\n    },\n    status: &#39;active&#39;,\n  },\n  include: {\n    tasks: {\n      where: {\n        status: &#39;pending&#39;,\n      },\n    },\n    timeEntries: true,\n  },\n});\n</code></pre>\n<p>This generated a query with multiple subqueries and joins that were slow on large datasets.</p>\n<h3>Lack of Control</h3>\n<p>Prisma abstracts SQL away. Usually good, but sometimes you need fine-grained control. We wanted:</p>\n<ul>\n<li>Custom join strategies</li>\n<li>Lateral joins for complex aggregations</li>\n<li>Partial indexes</li>\n<li>Query hints for the planner</li>\n</ul>\n<p>Prisma didn&#39;t give us easy access to these.</p>\n<h3>Type Safety Concerns</h3>\n<p>Prisma&#39;s generated types are comprehensive but sometimes too broad. This passed type checking but crashed at runtime:</p>\n<pre><code class=\"language-typescript\">const project = await prisma.project.findUnique({\n  where: { id: &#39;some-id&#39; },\n});\n\n// TypeScript thinks project.client exists, but it doesn&#39;t\n// because we didn&#39;t include it\nconsole.log(project.client.name); // Runtime error!\n</code></pre>\n<p>Drizzle&#39;s types are stricter—if you don&#39;t select it, it&#39;s not in the type.</p>\n<h3>Migration Speed</h3>\n<p>Prisma migrations were slow. With 150+ tables, running <code>prisma migrate dev</code> took minutes. Drizzle&#39;s migration generation is nearly instant.</p>\n<h2>Why Drizzle?</h2>\n<p>Drizzle offered solutions to all our pain points:</p>\n<ul>\n<li><strong>Performance</strong>: Write SQL-like queries that compile to efficient SQL</li>\n<li><strong>Control</strong>: Full SQL access when needed</li>\n<li><strong>Type safety</strong>: Strict types based on what you actually select</li>\n<li><strong>Migrations</strong>: Fast, simple, SQL-based</li>\n<li><strong>Bundle size</strong>: Smaller than Prisma (important for edge deployments)</li>\n</ul>\n<p>Most importantly, Drizzle&#39;s API felt natural for developers who understand SQL.</p>\n<h2>Migration Strategy</h2>\n<p>Migrating a large codebase required a methodical approach. We couldn&#39;t stop development for a big-bang rewrite.</p>\n<h3>Phase 1: Set Up Drizzle Alongside Prisma (Week 1)</h3>\n<p>We ran both ORMs in parallel initially:</p>\n<pre><code class=\"language-typescript\">// database.module.ts\n@Module({\n  providers: [\n    PrismaService, // Keep existing\n    DrizzleService, // Add new\n  ],\n  exports: [PrismaService, DrizzleService],\n})\nexport class DatabaseModule {}\n</code></pre>\n<p>Install Drizzle:</p>\n<pre><code class=\"language-bash\">npm install drizzle-orm postgres\nnpm install -D drizzle-kit\n</code></pre>\n<p>Configure Drizzle:</p>\n<pre><code class=\"language-typescript\">// drizzle.config.ts\nimport { defineConfig } from &#39;drizzle-kit&#39;;\n\nexport default defineConfig({\n  schema: &#39;./src/infrastructure/database/schema/*&#39;,\n  out: &#39;./drizzle/migrations&#39;,\n  dialect: &#39;postgresql&#39;,\n  dbCredentials: {\n    url: process.env.DATABASE_URL!,\n  },\n});\n</code></pre>\n<h3>Phase 2: Create Drizzle Schema (Week 2-3)</h3>\n<p>We converted Prisma schema to Drizzle, table by table:</p>\n<pre><code class=\"language-typescript\">// Prisma schema\nmodel Project {\n  id          String   @id @default(uuid())\n  tenantId    String\n  name        String\n  status      String\n  clientId    String\n  client      Client   @relation(fields: [clientId], references: [id])\n  tasks       Task[]\n  createdAt   DateTime @default(now())\n  updatedAt   DateTime @updatedAt\n\n  @@index([tenantId, status])\n}\n</code></pre>\n<p>Becomes Drizzle:</p>\n<pre><code class=\"language-typescript\">// schema/project.schema.ts\nimport { pgTable, uuid, varchar, timestamp, index } from &#39;drizzle-orm/pg-core&#39;;\nimport { clients } from &#39;./client.schema&#39;;\n\nexport const projects = pgTable(\n  &#39;projects&#39;,\n  {\n    id: uuid(&#39;id&#39;).primaryKey().defaultRandom(),\n    tenantId: uuid(&#39;tenant_id&#39;).notNull(),\n    name: varchar(&#39;name&#39;, { length: 255 }).notNull(),\n    status: varchar(&#39;status&#39;, { length: 50 }).notNull(),\n    clientId: uuid(&#39;client_id&#39;)\n      .notNull()\n      .references(() =&gt; clients.id),\n    createdAt: timestamp(&#39;created_at&#39;).defaultNow().notNull(),\n    updatedAt: timestamp(&#39;updated_at&#39;).defaultNow().notNull(),\n  },\n  (table) =&gt; ({\n    tenantStatusIdx: index(&#39;idx_projects_tenant_status&#39;).on(\n      table.tenantId,\n      table.status\n    ),\n  })\n);\n</code></pre>\n<p>We used a script to generate initial Drizzle schemas from Prisma:</p>\n<pre><code class=\"language-typescript\">// scripts/generate-drizzle-schema.ts\nimport { DMMF } from &#39;@prisma/generator-helper&#39;;\nimport * as fs from &#39;fs&#39;;\n\nfunction convertPrismaFieldToDrizzle(field: DMMF.Field): string {\n  const type = mapPrismaTypeToDrizzle(field.type);\n  const constraints = [];\n\n  if (field.isId) constraints.push(&#39;.primaryKey()&#39;);\n  if (field.isRequired &amp;&amp; !field.hasDefaultValue) constraints.push(&#39;.notNull()&#39;);\n  if (field.default) constraints.push(`.default(${field.default})`);\n\n  return `${field.name}: ${type}(&#39;${field.name}&#39;)${constraints.join(&#39;&#39;)}`;\n}\n\n// This saved us days of manual conversion\n</code></pre>\n<h3>Phase 3: Migrate Repositories (Week 4-8)</h3>\n<p>We migrated repositories module by module, starting with low-risk areas:</p>\n<pre><code class=\"language-typescript\">// Old Prisma repository\n@Injectable()\nexport class ProjectRepository {\n  constructor(private prisma: PrismaService) {}\n\n  async findById(id: string): Promise&lt;Project | null&gt; {\n    const result = await this.prisma.project.findUnique({\n      where: { id },\n      include: {\n        client: true,\n        tasks: true,\n      },\n    });\n\n    return result ? this.toDomain(result) : null;\n  }\n}\n</code></pre>\n<p>Became:</p>\n<pre><code class=\"language-typescript\">// New Drizzle repository\n@Injectable()\nexport class ProjectRepository {\n  constructor(private db: DrizzleService) {}\n\n  async findById(id: string): Promise&lt;Project | null&gt; {\n    const result = await this.db\n      .select()\n      .from(projects)\n      .leftJoin(clients, eq(projects.clientId, clients.id))\n      .leftJoin(tasks, eq(tasks.projectId, projects.id))\n      .where(eq(projects.id, id))\n      .limit(1);\n\n    if (!result[0]) return null;\n\n    return this.toDomain({\n      ...result[0].projects,\n      client: result[0].clients,\n      tasks: result.filter((r) =&gt; r.tasks).map((r) =&gt; r.tasks),\n    });\n  }\n}\n</code></pre>\n<h3>Phase 4: Data Migration (Week 9)</h3>\n<p>We needed to ensure data integrity. Drizzle uses the same database, but column names might differ:</p>\n<pre><code class=\"language-typescript\">// Prisma uses camelCase, we standardized on snake_case with Drizzle\nawait db.execute(sql`\n  ALTER TABLE projects RENAME COLUMN &quot;clientId&quot; TO &quot;client_id&quot;;\n  ALTER TABLE projects RENAME COLUMN &quot;createdAt&quot; TO &quot;created_at&quot;;\n  ALTER TABLE projects RENAME COLUMN &quot;updatedAt&quot; TO &quot;updated_at&quot;;\n`);\n</code></pre>\n<p>We did this in a maintenance window with zero downtime using view trick:</p>\n<pre><code class=\"language-sql\">-- Create view with old column names\nCREATE VIEW projects_legacy AS\nSELECT\n  id,\n  tenant_id as &quot;tenantId&quot;,\n  client_id as &quot;clientId&quot;,\n  created_at as &quot;createdAt&quot;,\n  updated_at as &quot;updatedAt&quot;\nFROM projects;\n\n-- Prisma queries the view while we migrate code\n-- Once all code migrated, drop the view\n</code></pre>\n<h3>Phase 5: Remove Prisma (Week 10)</h3>\n<p>After all repositories migrated and tests passing, we removed Prisma:</p>\n<pre><code class=\"language-bash\">npm uninstall prisma @prisma/client\nrm -rf prisma/\n</code></pre>\n<p>Updated our database service:</p>\n<pre><code class=\"language-typescript\">// Before\n@Injectable()\nexport class DatabaseService {\n  constructor(\n    private prisma: PrismaService,\n    private drizzle: DrizzleService\n  ) {}\n}\n\n// After\n@Injectable()\nexport class DatabaseService {\n  constructor(private db: DrizzleService) {}\n}\n</code></pre>\n<h2>Key Differences: Prisma vs. Drizzle</h2>\n<h3>Query Builder vs. Schema First</h3>\n<p>Prisma is schema-first. You define schema in Prisma language, generate TypeScript:</p>\n<pre><code class=\"language-prisma\">model User {\n  id    String @id\n  email String @unique\n}\n</code></pre>\n<p>Drizzle is code-first. You define schema in TypeScript:</p>\n<pre><code class=\"language-typescript\">export const users = pgTable(&#39;users&#39;, {\n  id: uuid(&#39;id&#39;).primaryKey(),\n  email: varchar(&#39;email&#39;, { length: 255 }).unique(),\n});\n</code></pre>\n<p>I prefer code-first. No code generation step, and TypeScript is the source of truth.</p>\n<h3>Type Inference</h3>\n<p>Prisma generates types from schema. Drizzle infers types from your queries.</p>\n<pre><code class=\"language-typescript\">// Drizzle - type is exactly what you select\nconst result = await db\n  .select({\n    id: users.id,\n    email: users.email,\n  })\n  .from(users);\n\n// Type: { id: string; email: string }[]\n// If you don&#39;t select it, it&#39;s not in the type!\n</code></pre>\n<p>This caught bugs where we assumed fields were loaded but weren&#39;t.</p>\n<h3>Relations</h3>\n<p>Prisma handles relations automatically:</p>\n<pre><code class=\"language-typescript\">const user = await prisma.user.findUnique({\n  where: { id: &#39;123&#39; },\n  include: { posts: true },\n});\n</code></pre>\n<p>Drizzle requires explicit joins:</p>\n<pre><code class=\"language-typescript\">const result = await db\n  .select()\n  .from(users)\n  .leftJoin(posts, eq(posts.userId, users.id))\n  .where(eq(users.id, &#39;123&#39;));\n</code></pre>\n<p>More verbose, but you see exactly what SQL runs.</p>\n<h3>Migrations</h3>\n<p>Prisma has a full migration system with rollback:</p>\n<pre><code class=\"language-bash\">prisma migrate dev\nprisma migrate deploy\n</code></pre>\n<p>Drizzle generates SQL, but you run it:</p>\n<pre><code class=\"language-bash\">drizzle-kit generate:pg\n# Creates SQL file in drizzle/migrations/\n# You run it with your preferred tool\n</code></pre>\n<p>We used our existing migration runner (postgres-migrations).</p>\n<h2>Performance Improvements</h2>\n<p>After migration, we measured performance across 50 key queries:</p>\n<ul>\n<li><strong>Average query time</strong>: 40% faster</li>\n<li><strong>P95 latency</strong>: 60% faster</li>\n<li><strong>Complex reports</strong>: 70% faster</li>\n<li><strong>Memory usage</strong>: 30% lower</li>\n</ul>\n<p>The biggest gains came from:</p>\n<ol>\n<li><strong>Optimized joins</strong>: We controlled exactly how tables joined</li>\n<li><strong>Partial selects</strong>: Only selecting needed columns</li>\n<li><strong>Better indexes</strong>: Drizzle made it obvious what we needed to index</li>\n</ol>\n<p>Example optimization:</p>\n<pre><code class=\"language-typescript\">// Prisma (slow)\nconst projects = await prisma.project.findMany({\n  include: {\n    client: true,\n    tasks: {\n      include: {\n        assignee: true,\n      },\n    },\n  },\n});\n\n// Drizzle (fast)\nconst projects = await db\n  .select({\n    projectId: projects.id,\n    projectName: projects.name,\n    clientName: clients.name,\n    taskCount: sql&lt;number&gt;`count(${tasks.id})`,\n  })\n  .from(projects)\n  .leftJoin(clients, eq(projects.clientId, clients.id))\n  .leftJoin(tasks, eq(tasks.projectId, projects.id))\n  .groupBy(projects.id, clients.name);\n</code></pre>\n<p>The Drizzle version does aggregation in the database, not in application memory.</p>\n<h2>Challenges and Solutions</h2>\n<h3>Challenge 1: Complex Nested Queries</h3>\n<p>Prisma&#39;s nested includes were convenient. Drizzle requires manual mapping:</p>\n<pre><code class=\"language-typescript\">// Helper function for nested data\nfunction groupTasksByProject(rows: any[]): Project[] {\n  const projectMap = new Map&lt;string, Project&gt;();\n\n  for (const row of rows) {\n    let project = projectMap.get(row.projects.id);\n\n    if (!project) {\n      project = {\n        ...row.projects,\n        client: row.clients,\n        tasks: [],\n      };\n      projectMap.set(row.projects.id, project);\n    }\n\n    if (row.tasks) {\n      project.tasks.push(row.tasks);\n    }\n  }\n\n  return Array.from(projectMap.values());\n}\n</code></pre>\n<h3>Challenge 2: Transaction Syntax</h3>\n<p>Prisma transactions:</p>\n<pre><code class=\"language-typescript\">await prisma.$transaction(async (tx) =&gt; {\n  await tx.project.create({ data: { ... } });\n  await tx.task.create({ data: { ... } });\n});\n</code></pre>\n<p>Drizzle transactions:</p>\n<pre><code class=\"language-typescript\">await db.transaction(async (tx) =&gt; {\n  await tx.insert(projects).values({ ... });\n  await tx.insert(tasks).values({ ... });\n});\n</code></pre>\n<p>Similar but subtly different. We created wrapper functions to minimize changes.</p>\n<h3>Challenge 3: Testing</h3>\n<p>We had extensive tests using Prisma. We needed to update all of them:</p>\n<pre><code class=\"language-typescript\">// Test helper to swap implementations\nbeforeEach(async () =&gt; {\n  const module = await Test.createTestingModule({\n    providers: [\n      {\n        provide: &#39;ProjectRepository&#39;,\n        useClass: DrizzleProjectRepository, // Changed from PrismaProjectRepository\n      },\n    ],\n  }).compile();\n});\n</code></pre>\n<p>We ran both implementations in parallel for a week, comparing results to ensure correctness.</p>\n<h2>Would I Do It Again?</h2>\n<p>Absolutely. The migration took 10 weeks with a team of 3 developers, but the benefits were immediate:</p>\n<ul>\n<li><strong>Better performance</strong> meant happier users</li>\n<li><strong>More control</strong> meant we could optimize critical paths</li>\n<li><strong>Smaller bundle</strong> improved cold starts in serverless</li>\n<li><strong>Faster migrations</strong> improved developer productivity</li>\n</ul>\n<h2>Should You Migrate?</h2>\n<p>Consider migrating if:</p>\n<ul>\n<li>Performance is critical and you need query control</li>\n<li>You&#39;re comfortable writing SQL</li>\n<li>You want stricter type safety</li>\n<li>Your team understands databases well</li>\n</ul>\n<p>Stay with Prisma if:</p>\n<ul>\n<li>Performance is adequate</li>\n<li>You prefer high-level abstractions</li>\n<li>Your team is less experienced with SQL</li>\n<li>You love Prisma Studio (Drizzle has no equivalent)</li>\n</ul>\n<h2>Migration Tips</h2>\n<ol>\n<li><strong>Start small</strong>: Migrate one module, verify it works, then expand</li>\n<li><strong>Run in parallel</strong>: Keep both ORMs for a transition period</li>\n<li><strong>Test extensively</strong>: Compare outputs from both implementations</li>\n<li><strong>Use views</strong>: Bridge schema differences without downtime</li>\n<li><strong>Monitor performance</strong>: Track query performance before and after</li>\n<li><strong>Document differences</strong>: Help your team understand the changes</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Migrating from Prisma to Drizzle was one of our best technical decisions. It gave us the control and performance we needed while maintaining type safety and developer experience.</p>\n<p>The key is approaching it methodically—don&#39;t rush, test everything, and migrate incrementally. After 27 years of building applications, I&#39;ve learned that good migrations are never big-bang rewrites; they&#39;re careful, measured transitions.</p>\n<p>If you&#39;re hitting Prisma&#39;s limitations and your team has strong SQL skills, Drizzle is worth serious consideration. Just plan for a 2-3 month migration timeline for a large codebase.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/prisma-to-drizzle-migration/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-09-04 00:00:00",
            "updated_at": "2024-09-04 00:00:00",
            "published_at": "2024-09-04 00:00:00",
            "custom_excerpt": "Lessons learned migrating a 320K+ LOC enterprise SaaS platform from Prisma to Drizzle ORM, including performance gains, challenges, and step-by-step migration strategies."
          },
          {
            "id": "29",
            "title": "Advanced TypeScript Patterns for Enterprise Applications",
            "slug": "typescript-advanced-patterns",
            "html": "<p>TypeScript has transformed how I build applications. After using it extensively at Nutrien, Verizon, and Servant, I&#39;ve collected patterns that make large codebases more maintainable and catch bugs at compile time.</p>\n<h2>Discriminated Unions for State Management</h2>\n<p>One of the most powerful TypeScript features is discriminated unions, perfect for modeling application state:</p>\n<pre><code class=\"language-typescript\">// types/api-state.ts\ntype ApiState&lt;T&gt; =\n  | { status: &#39;idle&#39; }\n  | { status: &#39;loading&#39; }\n  | { status: &#39;success&#39;; data: T }\n  | { status: &#39;error&#39;; error: string };\n\n// Using the state\nfunction renderReferrals(state: ApiState&lt;Referral[]&gt;) {\n  switch (state.status) {\n    case &#39;idle&#39;:\n      return &lt;div&gt;Click to load referrals&lt;/div&gt;;\n\n    case &#39;loading&#39;:\n      return ;\n\n    case &#39;success&#39;:\n      // TypeScript knows state.data exists here\n      return ;\n\n    case &#39;error&#39;:\n      // TypeScript knows state.error exists here\n      return ;\n  }\n}\n</code></pre>\n<p>This pattern eliminates entire categories of bugs where you might try to access <code>data</code> when it doesn&#39;t exist.</p>\n<h2>Generic Repository Pattern</h2>\n<p>For data access, I use a generic repository pattern that works with any entity:</p>\n<pre><code class=\"language-typescript\">// repositories/base.repository.ts\nexport abstract class BaseRepository&lt;T&gt; {\n  constructor(protected db: Database) {}\n\n  abstract tableName: string;\n\n  async findById(id: string): Promise&lt;T | null&gt; {\n    const result = await this.db\n      .select()\n      .from(this.tableName)\n      .where(&#39;id&#39;, id)\n      .first();\n\n    return result || null;\n  }\n\n  async findAll(filters?: Partial&lt;T&gt;): Promise&lt;T[]&gt; {\n    let query = this.db.select().from(this.tableName);\n\n    if (filters) {\n      Object.entries(filters).forEach(([key, value]) =&gt; {\n        query = query.where(key, value);\n      });\n    }\n\n    return query;\n  }\n\n  async create(data: Omit&lt;T, &#39;id&#39; | &#39;createdAt&#39; | &#39;updatedAt&#39;&gt;): Promise&lt;T&gt; {\n    const [result] = await this.db\n      .insert(this.tableName)\n      .values({\n        ...data,\n        id: uuidv4(),\n        createdAt: new Date(),\n        updatedAt: new Date(),\n      })\n      .returning();\n\n    return result;\n  }\n\n  async update(id: string, data: Partial&lt;T&gt;): Promise&lt;T&gt; {\n    const [result] = await this.db\n      .update(this.tableName)\n      .set({\n        ...data,\n        updatedAt: new Date(),\n      })\n      .where(&#39;id&#39;, id)\n      .returning();\n\n    return result;\n  }\n\n  async delete(id: string): Promise&lt;void&gt; {\n    await this.db.delete(this.tableName).where(&#39;id&#39;, id);\n  }\n}\n\n// Concrete implementation\nexport class ReferralsRepository extends BaseRepository&lt;Referral&gt; {\n  tableName = &#39;referrals&#39;;\n\n  async findByPriority(priority: Priority): Promise&lt;Referral[]&gt; {\n    return this.db\n      .select()\n      .from(this.tableName)\n      .where(&#39;priority&#39;, priority)\n      .orderBy(&#39;createdAt&#39;, &#39;desc&#39;);\n  }\n\n  async findByUserId(userId: string): Promise&lt;Referral[]&gt; {\n    return this.db\n      .select()\n      .from(this.tableName)\n      .where(&#39;userId&#39;, userId)\n      .orderBy(&#39;createdAt&#39;, &#39;desc&#39;);\n  }\n}\n</code></pre>\n<h2>Type-Safe Event Emitters</h2>\n<p>Event-driven architecture benefits enormously from type safety:</p>\n<pre><code class=\"language-typescript\">// events/typed-emitter.ts\nimport { EventEmitter } from &#39;events&#39;;\n\ninterface Events {\n  &#39;referral:created&#39;: (referral: Referral) =&gt; void;\n  &#39;referral:updated&#39;: (referral: Referral, changes: Partial&lt;Referral&gt;) =&gt; void;\n  &#39;referral:deleted&#39;: (referralId: string) =&gt; void;\n  &#39;user:login&#39;: (user: User) =&gt; void;\n  &#39;user:logout&#39;: (userId: string) =&gt; void;\n}\n\nexport class TypedEventEmitter {\n  private emitter = new EventEmitter();\n\n  on&lt;K extends keyof Events&gt;(event: K, listener: Events[K]): void {\n    this.emitter.on(event, listener);\n  }\n\n  once&lt;K extends keyof Events&gt;(event: K, listener: Events[K]): void {\n    this.emitter.once(event, listener);\n  }\n\n  emit&lt;K extends keyof Events&gt;(\n    event: K,\n    ...args: Parameters&lt;Events[K]&gt;\n  ): void {\n    this.emitter.emit(event, ...args);\n  }\n\n  off&lt;K extends keyof Events&gt;(event: K, listener: Events[K]): void {\n    this.emitter.off(event, listener);\n  }\n}\n\n// Usage\nconst events = new TypedEventEmitter();\n\n// TypeScript enforces correct event names and parameter types\nevents.on(&#39;referral:created&#39;, (referral) =&gt; {\n  console.log(`New referral: ${referral.motherName}`);\n});\n\n// This would be a compile error:\n// events.on(&#39;invalid:event&#39;, () =&gt; {}); // Error!\n// events.emit(&#39;referral:created&#39;, &#39;wrong type&#39;); // Error!\n</code></pre>\n<h2>Builder Pattern for Complex Objects</h2>\n<p>For complex object construction, the builder pattern with TypeScript is elegant:</p>\n<pre><code class=\"language-typescript\">// builders/query.builder.ts\ntype OrderDirection = &#39;asc&#39; | &#39;desc&#39;;\n\ninterface QueryOptions&lt;T&gt; {\n  where?: Partial&lt;T&gt;;\n  orderBy?: Array&lt;[keyof T, OrderDirection]&gt;;\n  limit?: number;\n  offset?: number;\n  include?: string[];\n}\n\nexport class QueryBuilder&lt;T&gt; {\n  private options: QueryOptions&lt;T&gt; = {};\n\n  where(conditions: Partial&lt;T&gt;): this {\n    this.options.where = { ...this.options.where, ...conditions };\n    return this;\n  }\n\n  orderBy(field: keyof T, direction: OrderDirection = &#39;asc&#39;): this {\n    if (!this.options.orderBy) {\n      this.options.orderBy = [];\n    }\n    this.options.orderBy.push([field, direction]);\n    return this;\n  }\n\n  limit(limit: number): this {\n    this.options.limit = limit;\n    return this;\n  }\n\n  offset(offset: number): this {\n    this.options.offset = offset;\n    return this;\n  }\n\n  include(relations: string[]): this {\n    this.options.include = relations;\n    return this;\n  }\n\n  build(): QueryOptions&lt;T&gt; {\n    return this.options;\n  }\n}\n\n// Usage\nconst query = new QueryBuilder&lt;Referral&gt;()\n  .where({ status: &#39;pending&#39; })\n  .where({ priority: &#39;HIGH&#39; })\n  .orderBy(&#39;createdAt&#39;, &#39;desc&#39;)\n  .limit(10)\n  .include([&#39;user&#39;])\n  .build();\n</code></pre>\n<h2>Branded Types for Domain Validation</h2>\n<p>Branded types prevent mixing up similar primitive types:</p>\n<pre><code class=\"language-typescript\">// types/branded.ts\ntype Brand&lt;K, T&gt; = K &amp; { __brand: T };\n\nexport type UserId = Brand&lt;string, &#39;UserId&#39;&gt;;\nexport type ReferralId = Brand&lt;string, &#39;ReferralId&#39;&gt;;\nexport type Email = Brand&lt;string, &#39;Email&#39;&gt;;\nexport type PhoneNumber = Brand&lt;string, &#39;PhoneNumber&#39;&gt;;\n\n// Validation functions that return branded types\nexport function createUserId(id: string): UserId {\n  if (!id || id.length === 0) {\n    throw new Error(&#39;Invalid user ID&#39;);\n  }\n  return id as UserId;\n}\n\nexport function createEmail(email: string): Email {\n  const emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n  if (!emailRegex.test(email)) {\n    throw new Error(&#39;Invalid email format&#39;);\n  }\n  return email as Email;\n}\n\nexport function createPhoneNumber(phone: string): PhoneNumber {\n  const cleaned = phone.replace(/\\D/g, &#39;&#39;);\n  if (cleaned.length !== 10 &amp;&amp; cleaned.length !== 11) {\n    throw new Error(&#39;Invalid phone number&#39;);\n  }\n  return phone as PhoneNumber;\n}\n\n// Usage\nfunction sendEmail(email: Email, subject: string, body: string) {\n  // Implementation\n}\n\n// This works\nconst validEmail = createEmail(&#39;user@example.com&#39;);\nsendEmail(validEmail, &#39;Hello&#39;, &#39;World&#39;);\n\n// This won&#39;t compile\n// sendEmail(&#39;user@example.com&#39;, &#39;Hello&#39;, &#39;World&#39;); // Error!\n\n// You must validate first\nconst email = createEmail(&#39;user@example.com&#39;);\nsendEmail(email, &#39;Hello&#39;, &#39;World&#39;); // OK\n</code></pre>\n<h2>Conditional Types for API Responses</h2>\n<p>Conditional types help model different API response shapes:</p>\n<pre><code class=\"language-typescript\">// types/api-response.ts\ntype SuccessResponse&lt;T&gt; = {\n  success: true;\n  data: T;\n};\n\ntype ErrorResponse = {\n  success: false;\n  error: {\n    code: string;\n    message: string;\n    details?: unknown;\n  };\n};\n\ntype ApiResponse&lt;T&gt; = SuccessResponse&lt;T&gt; | ErrorResponse;\n\n// Type guard\nfunction isSuccess&lt;T&gt;(response: ApiResponse&lt;T&gt;): response is SuccessResponse&lt;T&gt; {\n  return response.success === true;\n}\n\n// Usage\nasync function fetchReferrals(): Promise&lt;ApiResponse&lt;Referral[]&gt;&gt; {\n  try {\n    const response = await fetch(&#39;/api/referrals&#39;);\n    const data = await response.json();\n\n    return {\n      success: true,\n      data,\n    };\n  } catch (error) {\n    return {\n      success: false,\n      error: {\n        code: &#39;FETCH_ERROR&#39;,\n        message: error.message,\n      },\n    };\n  }\n}\n\n// Type-safe handling\nconst response = await fetchReferrals();\n\nif (isSuccess(response)) {\n  // TypeScript knows response.data exists\n  console.log(response.data.length);\n} else {\n  // TypeScript knows response.error exists\n  console.error(response.error.message);\n}\n</code></pre>\n<h2>Mapped Types for Form State</h2>\n<p>Managing form state with mapped types:</p>\n<pre><code class=\"language-typescript\">// types/form-state.ts\ntype FormField&lt;T&gt; = {\n  value: T;\n  error?: string;\n  touched: boolean;\n  dirty: boolean;\n};\n\ntype FormState&lt;T&gt; = {\n  [K in keyof T]: FormField&lt;T[K]&gt;;\n};\n\ntype FormErrors&lt;T&gt; = {\n  [K in keyof T]?: string;\n};\n\n// Example entity\ninterface CreateReferralForm {\n  motherName: string;\n  priority: Priority;\n  notes: string;\n  userId: string;\n}\n\n// Automatically creates form state type\ntype ReferralFormState = FormState&lt;CreateReferralForm&gt;;\n\n// Helper functions\nfunction createFormState&lt;T&gt;(initialValues: T): FormState&lt;T&gt; {\n  return Object.entries(initialValues).reduce((acc, [key, value]) =&gt; {\n    acc[key as keyof T] = {\n      value,\n      touched: false,\n      dirty: false,\n    };\n    return acc;\n  }, {} as FormState&lt;T&gt;);\n}\n\nfunction setFieldValue&lt;T, K extends keyof T&gt;(\n  formState: FormState&lt;T&gt;,\n  field: K,\n  value: T[K]\n): FormState&lt;T&gt; {\n  return {\n    ...formState,\n    [field]: {\n      ...formState[field],\n      value,\n      dirty: true,\n    },\n  };\n}\n\nfunction setFieldError&lt;T, K extends keyof T&gt;(\n  formState: FormState&lt;T&gt;,\n  field: K,\n  error: string\n): FormState&lt;T&gt; {\n  return {\n    ...formState,\n    [field]: {\n      ...formState[field],\n      error,\n    },\n  };\n}\n\n// Usage\nconst formState = createFormState&lt;CreateReferralForm&gt;({\n  motherName: &#39;&#39;,\n  priority: &#39;MEDIUM&#39;,\n  notes: &#39;&#39;,\n  userId: &#39;&#39;,\n});\n\nconst updated = setFieldValue(formState, &#39;motherName&#39;, &#39;Jane Doe&#39;);\n</code></pre>\n<h2>Utility Types for DTOs</h2>\n<p>Creating DTOs with utility types:</p>\n<pre><code class=\"language-typescript\">// types/dto.ts\n// Pick only specific fields\ntype UserPublicDTO = Pick&lt;User, &#39;id&#39; | &#39;email&#39; | &#39;firstName&#39; | &#39;lastName&#39; | &#39;role&#39;&gt;;\n\n// Omit sensitive fields\ntype UserSafeDTO = Omit&lt;User, &#39;passwordHash&#39; | &#39;resetToken&#39;&gt;;\n\n// Make all fields optional for updates\ntype UpdateUserDTO = Partial&lt;User&gt;;\n\n// Make specific fields required\ntype CreateUserDTO = Required&lt;Pick&lt;User, &#39;email&#39; | &#39;firstName&#39; | &#39;lastName&#39; | &#39;role&#39;&gt;&gt; &amp; {\n  password: string;\n};\n\n// Readonly for responses\ntype UserResponse = Readonly&lt;UserPublicDTO&gt;;\n\n// Deep partial for nested updates\ntype DeepPartial&lt;T&gt; = {\n  [P in keyof T]?: T[P] extends object ? DeepPartial&lt;T[P]&gt; : T[P];\n};\n\ntype UpdateReferralDTO = DeepPartial&lt;Referral&gt;;\n</code></pre>\n<h2>Conclusion</h2>\n<p>These TypeScript patterns have saved me countless hours of debugging. The key is leveraging TypeScript&#39;s type system to catch errors at compile time rather than runtime.</p>\n<p>Advanced TypeScript takes time to master, but the investment pays off in more maintainable, less buggy code. Start with discriminated unions and branded types, then gradually adopt the more advanced patterns as your needs grow.</p>\n<p>The goal isn&#39;t to use every advanced TypeScript feature - it&#39;s to use the right features to make your code safer and more expressive.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/typescript-advanced-patterns/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-08-24 00:00:00",
            "updated_at": "2024-08-24 00:00:00",
            "published_at": "2024-08-24 00:00:00",
            "custom_excerpt": "Type-safe patterns and techniques learned from building large-scale TypeScript applications"
          },
          {
            "id": "30",
            "title": "Offline-First Mobile Apps: Architecture and Sync Strategies",
            "slug": "offline-first-mobile-architecture",
            "html": "<p>In the oil and gas industry, field workers spend most of their time in remote locations with zero cellular coverage. When we built WellOS, a field operations app for well inspections and maintenance tracking, we faced a hard requirement: the app must work perfectly offline and sync seamlessly when connectivity returns.</p>\n<p>After two years of running in production across hundreds of remote well sites, our offline-first architecture has proven reliable. Here&#39;s everything we learned about building truly offline-capable mobile applications.</p>\n<h2>Why Offline-First?</h2>\n<p>Most mobile apps use an &quot;online-first&quot; approach: they require internet connectivity and maybe cache some data. This fails catastrophically in environments with poor or no connectivity.</p>\n<p>Offline-first inverts this: the app works entirely offline by default, using a local database. When connectivity is available, it syncs with the server in the background. To the user, the app just works, whether they&#39;re deep in a canyon or in the office.</p>\n<p>Benefits of offline-first:</p>\n<ul>\n<li><strong>Reliability</strong>: App functions regardless of network state</li>\n<li><strong>Performance</strong>: Instant UI updates (no waiting for network)</li>\n<li><strong>User experience</strong>: No connection errors or loading spinners</li>\n<li><strong>Data integrity</strong>: Changes aren&#39;t lost if connection drops</li>\n<li><strong>Battery life</strong>: Fewer network requests</li>\n</ul>\n<p>The cost is complexity: you&#39;re building a distributed system with sync, conflicts, and eventual consistency.</p>\n<h2>The Local-First Architecture</h2>\n<p>Our architecture has three layers:</p>\n<h3>1. Local Database (SQLite)</h3>\n<p>SQLite is the source of truth on the device. Every read and write goes to SQLite first:</p>\n<pre><code class=\"language-typescript\">// infrastructure/database/sqlite-database.ts\nimport * as SQLite from &#39;expo-sqlite&#39;;\nimport { drizzle } from &#39;drizzle-orm/expo-sqlite&#39;;\n\nexport class LocalDatabase {\n  private db: ReturnType&lt;typeof drizzle&gt;;\n\n  async initialize(): Promise&lt;void&gt; {\n    const expo = SQLite.openDatabaseSync(&#39;wellos.db&#39;);\n    this.db = drizzle(expo);\n\n    // Run migrations\n    await this.runMigrations();\n  }\n\n  getDb() {\n    return this.db;\n  }\n}\n</code></pre>\n<h3>2. Local Repository Layer</h3>\n<p>Repositories work entirely with SQLite, no network calls:</p>\n<pre><code class=\"language-typescript\">export class LocalWellInspectionRepository implements WellInspectionRepository {\n  constructor(private readonly db: LocalDatabase) {}\n\n  async findById(id: string): Promise\n      \n      \n      \n      \n    &lt;/View&gt;\n  );\n};\n</code></pre>\n<h2>Background Sync</h2>\n<p>Sync happens automatically in the background:</p>\n<pre><code class=\"language-typescript\">// App.tsx\nuseEffect(() =&gt; {\n  // Sync when app comes to foreground\n  const subscription = AppState.addEventListener(&#39;change&#39;, (state) =&gt; {\n    if (state === &#39;active&#39;) {\n      syncService.sync();\n    }\n  });\n\n  // Periodic background sync (every 15 minutes if online)\n  const interval = setInterval(() =&gt; {\n    syncService.sync();\n  }, 15 * 60 * 1000);\n\n  return () =&gt; {\n    subscription.remove();\n    clearInterval(interval);\n  };\n}, []);\n</code></pre>\n<p>For React Native, we use background tasks:</p>\n<pre><code class=\"language-typescript\">import * as BackgroundFetch from &#39;expo-background-fetch&#39;;\nimport * as TaskManager from &#39;expo-task-manager&#39;;\n\nconst BACKGROUND_SYNC_TASK = &#39;background-sync&#39;;\n\nTaskManager.defineTask(BACKGROUND_SYNC_TASK, async () =&gt; {\n  try {\n    await syncService.sync();\n    return BackgroundFetch.BackgroundFetchResult.NewData;\n  } catch (error) {\n    return BackgroundFetch.BackgroundFetchResult.Failed;\n  }\n});\n\nexport async function registerBackgroundSync() {\n  await BackgroundFetch.registerTaskAsync(BACKGROUND_SYNC_TASK, {\n    minimumInterval: 15 * 60, // 15 minutes\n    stopOnTerminate: false,\n    startOnBoot: true,\n  });\n}\n</code></pre>\n<h2>UI Feedback for Sync Status</h2>\n<p>Users need to know sync status:</p>\n<pre><code class=\"language-typescript\">const SyncStatusIndicator = () =&gt; {\n  const { syncStatus, lastSyncAt, pendingChanges } = useSyncStatus();\n\n  return (\n    \n      )}\n\n      {syncStatus === &#39;synced&#39; &amp;&amp; (\n        &lt;&gt;\n          \n          \n        &lt;/&gt;\n      )}\n\n      {syncStatus === &#39;offline&#39; &amp;&amp; (\n        &lt;&gt;\n          \n          \n        &lt;/&gt;\n      )}\n\n      {syncStatus === &#39;error&#39; &amp;&amp; (\n        &lt;&gt;\n          \n          \n        &lt;/&gt;\n      )}\n    &lt;/View&gt;\n  );\n};\n</code></pre>\n<h2>Optimistic UI Updates</h2>\n<p>The app feels fast because we update the UI immediately:</p>\n<pre><code class=\"language-typescript\">const useCreateInspection = () =&gt; {\n  const queryClient = useQueryClient();\n\n  return useMutation({\n    mutationFn: async (inspection: CreateInspectionDTO) =&gt; {\n      return await localRepository.save(inspection);\n    },\n\n    // Immediately update UI before sync\n    onMutate: async (newInspection) =&gt; {\n      await queryClient.cancelQueries([&#39;inspections&#39;]);\n\n      const previous = queryClient.getQueryData([&#39;inspections&#39;]);\n\n      queryClient.setQueryData([&#39;inspections&#39;], (old: Inspection[]) =&gt; [\n        ...old,\n        { ...newInspection, syncStatus: &#39;pending&#39; },\n      ]);\n\n      return { previous };\n    },\n\n    // If mutation fails, rollback\n    onError: (err, newInspection, context) =&gt; {\n      queryClient.setQueryData([&#39;inspections&#39;], context.previous);\n    },\n\n    // Trigger sync in background\n    onSuccess: () =&gt; {\n      syncService.sync();\n    },\n  });\n};\n</code></pre>\n<p>User creates an inspection, it appears instantly, and syncs in the background.</p>\n<h2>Lessons Learned</h2>\n<h3>Lesson 1: SQLite is Reliable</h3>\n<p>We worried about SQLite performance and corruption. After two years, it&#39;s been rock solid. We&#39;ve had zero data loss incidents.</p>\n<h3>Lesson 2: Sync is Hard</h3>\n<p>Sync is the hardest part of offline-first. We spent 60% of development time on sync logic, conflict resolution, and edge cases. Budget accordingly.</p>\n<h3>Lesson 3: Test Offline Extensively</h3>\n<p>We built tools to simulate network conditions:</p>\n<pre><code class=\"language-typescript\">// Development only - simulate offline\nif (__DEV__) {\n  global.forceOffline = () =&gt; {\n    NetworkInterceptor.blockAllRequests();\n  };\n\n  global.forceOnline = () =&gt; {\n    NetworkInterceptor.allowAllRequests();\n  };\n}\n</code></pre>\n<h3>Lesson 4: Soft Deletes are Essential</h3>\n<p>Hard deletes break sync. Everything should be soft deleted with <code>deletedAt</code> timestamps.</p>\n<h3>Lesson 5: Eventual Consistency is Acceptable</h3>\n<p>Field workers understood that changes might take a few minutes to sync across devices. Eventual consistency was fine for their workflows.</p>\n<h2>When to Use Offline-First</h2>\n<p>Use offline-first when:</p>\n<ul>\n<li>Users work in environments with poor connectivity</li>\n<li>Data entry needs to continue regardless of network</li>\n<li>Mobile app is business-critical</li>\n<li>Users frequently switch between online/offline</li>\n</ul>\n<p>Don&#39;t use offline-first when:</p>\n<ul>\n<li>Connectivity is reliable</li>\n<li>Real-time collaboration is essential</li>\n<li>Complexity isn&#39;t worth the benefit</li>\n<li>Your data model doesn&#39;t fit eventual consistency</li>\n</ul>\n<h2>Conclusion</h2>\n<p>Building WellOS with offline-first architecture transformed the field worker experience. They don&#39;t worry about connectivity—they just do their work, and everything syncs automatically when possible.</p>\n<p>The key ingredients:</p>\n<ul>\n<li>SQLite as local source of truth</li>\n<li>Sync metadata on every record</li>\n<li>Background sync with conflict resolution</li>\n<li>Optimistic UI updates</li>\n<li>Extensive offline testing</li>\n</ul>\n<p>After 27 years of building applications, I can confidently say that offline-first is the future of mobile development for any app where reliability matters more than complexity.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/offline-first-mobile-architecture/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-08-13 00:00:00",
            "updated_at": "2024-08-13 00:00:00",
            "published_at": "2024-08-13 00:00:00",
            "custom_excerpt": "Building mobile applications that work seamlessly offline and sync when connected, based on experience building WellOS for oil field operations with unreliable connectivity."
          },
          {
            "id": "31",
            "title": "React Native in the Real World: Lessons from Cross-Platform Development",
            "slug": "react-native-cross-platform-reality",
            "html": "<p>React Native promises &quot;write once, run anywhere.&quot; After building multiple production apps with it, including an NFT collectibles platform at Verizon and cross-platform components at Nutrien, I can tell you the reality is more nuanced - but still incredibly valuable.</p>\n<h2>The NFT Collectibles Challenge</h2>\n<p>At Verizon, we built a full-stack NFT collectibles solution for AMC&#39;s Walking Dead franchise. The app needed to work seamlessly on iOS and Android while interfacing with the Ethereum blockchain and managing digital collectibles.</p>\n<p>Here&#39;s a simplified version of how we handled blockchain interactions:</p>\n<pre><code class=\"language-typescript\">// BlockchainService.ts\nimport { ethers } from &#39;ethers&#39;;\n\nclass BlockchainService {\n  private provider: ethers.providers.Provider;\n  private contract: ethers.Contract;\n\n  constructor(contractAddress: string, abi: any) {\n    this.provider = new ethers.providers.JsonRpcProvider(\n      process.env.ETHEREUM_RPC_URL\n    );\n    this.contract = new ethers.Contract(\n      contractAddress,\n      abi,\n      this.provider\n    );\n  }\n\n  async getNFTMetadata(tokenId: string): Promise\n      )}\n    &lt;/TouchableOpacity&gt;\n  );\n};\n\n// Jest + Enzyme testing\ndescribe(&#39;CustomButton&#39;, () =&gt; {\n  it(&#39;renders loading state correctly&#39;, () =&gt; {\n    const wrapper = shallow(\n      \n    );\n    expect(wrapper.find(ActivityIndicator)).toHaveLength(1);\n  });\n});\n</code></pre>\n<h2>Platform-Specific Considerations</h2>\n<p>React Native isn&#39;t truly &quot;write once, run anywhere.&quot; You&#39;ll encounter platform differences:</p>\n<pre><code class=\"language-typescript\">import { Platform } from &#39;react-native&#39;;\n\nconst styles = StyleSheet.create({\n  container: {\n    ...Platform.select({\n      ios: {\n        shadowColor: &#39;#000&#39;,\n        shadowOffset: { width: 0, height: 2 },\n        shadowOpacity: 0.25,\n        shadowRadius: 3.84,\n      },\n      android: {\n        elevation: 5,\n      },\n    }),\n  },\n});\n</code></pre>\n<h2>CI/CD with CircleCI</h2>\n<p>We used CircleCI for automated testing and deployment:</p>\n<pre><code class=\"language-yaml\">version: 2.1\njobs:\n  test:\n    docker:\n      - image: circleci/node:14\n    steps:\n      - checkout\n      - restore_cache:\n          keys:\n            - v1-dependencies-{{ checksum &quot;package.json&quot; }}\n      - run: npm install\n      - save_cache:\n          paths:\n            - node_modules\n          key: v1-dependencies-{{ checksum &quot;package.json&quot; }}\n      - run: npm test -- --coverage\n\n  deploy:\n    docker:\n      - image: circleci/node:14\n    steps:\n      - checkout\n      - run: npm install\n      - run: npm run build:ios\n      - run: npm run build:android\n</code></pre>\n<h2>Key Lessons</h2>\n<ol>\n<li><strong>TypeScript is essential</strong>: The type safety saved us countless hours debugging</li>\n<li><strong>Test thoroughly on both platforms</strong>: What works on iOS might break on Android</li>\n<li><strong>Performance monitoring matters</strong>: Use tools like Flipper and React Native Debugger</li>\n<li><strong>Native modules when needed</strong>: Don&#39;t hesitate to write native code for performance-critical features</li>\n<li><strong>State management scales</strong>: We used Redux for complex apps, Context for simpler ones</li>\n</ol>\n<p>React Native continues to mature, and with the new architecture rolling out, it&#39;s only getting better. For teams that need to target multiple platforms with limited resources, it remains an excellent choice.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/react-native-cross-platform-reality/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-08-09 00:00:00",
            "updated_at": "2024-08-09 00:00:00",
            "published_at": "2024-08-09 00:00:00",
            "custom_excerpt": "Practical insights from building production React Native apps at Nutrien and Verizon, including the NFT collectibles platform"
          },
          {
            "id": "32",
            "title": "Database-Agnostic Architecture: Why and How",
            "slug": "database-agnostic-architecture",
            "html": "<p>When we started building WellOS, an oil and gas field operations platform, we faced an unusual challenge: our application needed to run in three completely different environments with three different databases.</p>\n<ul>\n<li><strong>Cloud deployment</strong>: PostgreSQL for the main SaaS platform</li>\n<li><strong>On-premise installations</strong>: MongoDB for clients with existing NoSQL infrastructure</li>\n<li><strong>Mobile offline mode</strong>: SQLite for field workers without internet connectivity</li>\n</ul>\n<p>Building three separate applications was out of the question. We needed a database-agnostic architecture that could work seamlessly across all three databases. After two years in production, this architecture has proven to be one of our best decisions.</p>\n<h2>Why Database-Agnostic Architecture?</h2>\n<p>Most applications are tightly coupled to their database. You use PostgreSQL-specific features, write raw SQL, or leverage ORM features specific to your database. This works great until you need to support multiple databases or migrate to a different one.</p>\n<p>Database-agnostic architecture decouples your business logic from database implementation details. The benefits:</p>\n<ul>\n<li><strong>Flexibility</strong>: Support multiple database engines without code duplication</li>\n<li><strong>Easier migrations</strong>: Switch databases without rewriting business logic</li>\n<li><strong>Better testing</strong>: Use SQLite in tests, PostgreSQL in production</li>\n<li><strong>Offline support</strong>: Sync between SQLite (offline) and PostgreSQL (server)</li>\n<li><strong>Client choice</strong>: Let enterprise clients use their preferred database</li>\n</ul>\n<p>The tradeoff is giving up database-specific optimizations. For WellOS, this was acceptable—business logic complexity far outweighed database complexity.</p>\n<h2>The Three-Layer Approach</h2>\n<p>We built database-agnostic architecture using three distinct layers:</p>\n<h3>1. Domain Layer: Pure Business Logic</h3>\n<p>The domain layer has zero database dependencies:</p>\n<pre><code class=\"language-typescript\">// domain/entities/well-inspection.ts\nexport class WellInspection {\n  constructor(\n    private readonly id: string,\n    private wellId: string,\n    private inspectorId: string,\n    private inspectionDate: Date,\n    private status: InspectionStatus,\n    private findings: InspectionFinding[]\n  ) {}\n\n  addFinding(finding: InspectionFinding): void {\n    if (this.status === InspectionStatus.COMPLETED) {\n      throw new CompletedInspectionError();\n    }\n    this.findings.push(finding);\n  }\n\n  complete(): void {\n    if (this.findings.length === 0) {\n      throw new NoFindingsError(&#39;Cannot complete inspection without findings&#39;);\n    }\n    this.status = InspectionStatus.COMPLETED;\n  }\n\n  // Pure domain logic, no database code\n}\n</code></pre>\n<p>Notice there&#39;s no imports from any database library. Just TypeScript classes representing business concepts.</p>\n<h3>2. Repository Interface: The Contract</h3>\n<p>Repositories define what operations are needed, not how they&#39;re implemented:</p>\n<pre><code class=\"language-typescript\">// domain/repositories/well-inspection.repository.ts\nexport interface WellInspectionRepository {\n  findById(id: string): Promise&lt;WellInspection | null&gt;;\n  findByWellId(wellId: string): Promise&lt;WellInspection[]&gt;;\n  findPendingInspections(): Promise&lt;WellInspection[]&gt;;\n  save(inspection: WellInspection): Promise&lt;void&gt;;\n  delete(id: string): Promise&lt;void&gt;;\n}\n</code></pre>\n<p>This interface is database-agnostic. It describes business operations without mentioning SQL, documents, or any database specifics.</p>\n<h3>3. Multiple Repository Implementations</h3>\n<p>Each database gets its own implementation:</p>\n<pre><code class=\"language-typescript\">// infrastructure/persistence/postgresql/well-inspection.repository.ts\n@Injectable()\nexport class PostgreSQLWellInspectionRepository implements WellInspectionRepository {\n  constructor(private readonly db: DrizzleService) {}\n\n  async findById(id: string): Promise&lt;WellInspection | null&gt; {\n    const result = await this.db\n      .select()\n      .from(wellInspections)\n      .where(eq(wellInspections.id, id))\n      .limit(1);\n\n    return result[0] ? this.toDomain(result[0]) : null;\n  }\n\n  // ... other methods\n}\n\n// infrastructure/persistence/mongodb/well-inspection.repository.ts\n@Injectable()\nexport class MongoDBWellInspectionRepository implements WellInspectionRepository {\n  constructor(private readonly db: MongoClient) {}\n\n  async findById(id: string): Promise&lt;WellInspection | null&gt; {\n    const collection = this.db.collection(&#39;well_inspections&#39;);\n    const doc = await collection.findOne({ _id: id });\n\n    return doc ? this.toDomain(doc) : null;\n  }\n\n  // ... other methods\n}\n\n// infrastructure/persistence/sqlite/well-inspection.repository.ts\n@Injectable()\nexport class SQLiteWellInspectionRepository implements WellInspectionRepository {\n  constructor(private readonly db: DrizzleSQLite) {}\n\n  async findById(id: string): Promise&lt;WellInspection | null&gt; {\n    const result = await this.db\n      .select()\n      .from(wellInspections)\n      .where(eq(wellInspections.id, id))\n      .limit(1);\n\n    return result[0] ? this.toDomain(result[0]) : null;\n  }\n\n  // ... other methods\n}\n</code></pre>\n<p>All three implement the same interface, so business logic doesn&#39;t know which database it&#39;s using.</p>\n<h2>Choosing the Right Repository at Runtime</h2>\n<p>We use dependency injection to select the appropriate repository based on configuration:</p>\n<pre><code class=\"language-typescript\">// infrastructure/persistence/persistence.module.ts\n@Module({})\nexport class PersistenceModule {\n  static forRoot(config: DatabaseConfig): DynamicModule {\n    const repositoryProviders = this.getRepositoryProviders(config.type);\n\n    return {\n      module: PersistenceModule,\n      providers: [...repositoryProviders],\n      exports: [...repositoryProviders],\n    };\n  }\n\n  private static getRepositoryProviders(dbType: DatabaseType): Provider[] {\n    switch (dbType) {\n      case &#39;postgresql&#39;:\n        return [\n          {\n            provide: &#39;WellInspectionRepository&#39;,\n            useClass: PostgreSQLWellInspectionRepository,\n          },\n          // ... other PostgreSQL repositories\n        ];\n\n      case &#39;mongodb&#39;:\n        return [\n          {\n            provide: &#39;WellInspectionRepository&#39;,\n            useClass: MongoDBWellInspectionRepository,\n          },\n          // ... other MongoDB repositories\n        ];\n\n      case &#39;sqlite&#39;:\n        return [\n          {\n            provide: &#39;WellInspectionRepository&#39;,\n            useClass: SQLiteWellInspectionRepository,\n          },\n          // ... other SQLite repositories\n        ];\n    }\n  }\n}\n</code></pre>\n<p>Application code just injects the interface:</p>\n<pre><code class=\"language-typescript\">@Injectable()\nexport class CompleteInspectionUseCase {\n  constructor(\n    @Inject(&#39;WellInspectionRepository&#39;)\n    private readonly repository: WellInspectionRepository\n  ) {}\n\n  async execute(inspectionId: string): Promise&lt;void&gt; {\n    const inspection = await this.repository.findById(inspectionId);\n\n    if (!inspection) {\n      throw new NotFoundException();\n    }\n\n    inspection.complete();\n\n    await this.repository.save(inspection);\n  }\n}\n</code></pre>\n<p>This use case works identically with PostgreSQL, MongoDB, or SQLite. The database is swapped by changing a config variable.</p>\n<h2>Using Drizzle ORM for Abstraction</h2>\n<p>We chose Drizzle ORM because it supports both SQL databases (PostgreSQL, SQLite) and has a consistent API:</p>\n<pre><code class=\"language-typescript\">// schema/well-inspection.schema.ts\nimport { pgTable, uuid, timestamp, text } from &#39;drizzle-orm/pg-core&#39;;\n\nexport const wellInspections = pgTable(&#39;well_inspections&#39;, {\n  id: uuid(&#39;id&#39;).primaryKey(),\n  wellId: uuid(&#39;well_id&#39;).notNull(),\n  inspectorId: uuid(&#39;inspector_id&#39;).notNull(),\n  inspectionDate: timestamp(&#39;inspection_date&#39;).notNull(),\n  status: text(&#39;status&#39;).notNull(),\n  findings: text(&#39;findings&#39;).notNull(), // JSON\n});\n</code></pre>\n<p>For SQLite, we use the same schema with SQLite-specific imports:</p>\n<pre><code class=\"language-typescript\">// schema/well-inspection.schema.sqlite.ts\nimport { sqliteTable, text, integer } from &#39;drizzle-orm/sqlite-core&#39;;\n\nexport const wellInspections = sqliteTable(&#39;well_inspections&#39;, {\n  id: text(&#39;id&#39;).primaryKey(),\n  wellId: text(&#39;well_id&#39;).notNull(),\n  inspectorId: text(&#39;inspector_id&#39;).notNull(),\n  inspectionDate: integer(&#39;inspection_date&#39;, { mode: &#39;timestamp&#39; }).notNull(),\n  status: text(&#39;status&#39;).notNull(),\n  findings: text(&#39;findings&#39;).notNull(), // JSON\n});\n</code></pre>\n<p>Drizzle&#39;s query API is identical for both:</p>\n<pre><code class=\"language-typescript\">// Same code works for PostgreSQL and SQLite\nconst results = await db\n  .select()\n  .from(wellInspections)\n  .where(eq(wellInspections.status, &#39;pending&#39;));\n</code></pre>\n<p>For MongoDB, we wrote a thin adapter that mimics Drizzle&#39;s API:</p>\n<pre><code class=\"language-typescript\">export class MongoDBAdapter {\n  constructor(private client: MongoClient) {}\n\n  collection(name: string) {\n    return this.client.db().collection(name);\n  }\n\n  async select() {\n    return {\n      from: (collection: string) =&gt; ({\n        where: (filter: any) =&gt; this.collection(collection).find(filter).toArray(),\n      }),\n    };\n  }\n}\n</code></pre>\n<p>This isn&#39;t perfect but gave us 80% API compatibility.</p>\n<h2>Handling Database-Specific Features</h2>\n<p>Database-agnostic architecture means giving up some database-specific features. Here&#39;s how we handled common scenarios:</p>\n<h3>Transactions</h3>\n<p>Each database handles transactions differently. We created an abstract transaction interface:</p>\n<pre><code class=\"language-typescript\">export interface TransactionManager {\n  runInTransaction&lt;T&gt;(work: () =&gt; Promise&lt;T&gt;): Promise&lt;T&gt;;\n}\n\n// PostgreSQL implementation\nexport class PostgreSQLTransactionManager implements TransactionManager {\n  async runInTransaction&lt;T&gt;(work: () =&gt; Promise&lt;T&gt;): Promise&lt;T&gt; {\n    return await this.db.transaction(async (tx) =&gt; {\n      return await work();\n    });\n  }\n}\n\n// MongoDB implementation\nexport class MongoDBTransactionManager implements TransactionManager {\n  async runInTransaction&lt;T&gt;(work: () =&gt; Promise&lt;T&gt;): Promise&lt;T&gt; {\n    const session = this.client.startSession();\n    try {\n      session.startTransaction();\n      const result = await work();\n      await session.commitTransaction();\n      return result;\n    } catch (error) {\n      await session.abortTransaction();\n      throw error;\n    } finally {\n      session.endSession();\n    }\n  }\n}\n</code></pre>\n<h3>Full-Text Search</h3>\n<p>PostgreSQL has excellent full-text search. MongoDB has text indexes. SQLite has FTS5. We abstracted it:</p>\n<pre><code class=\"language-typescript\">export interface SearchRepository {\n  searchWells(query: string): Promise&lt;Well[]&gt;;\n}\n\n// PostgreSQL uses ts_vector\nexport class PostgreSQLSearchRepository implements SearchRepository {\n  async searchWells(query: string): Promise&lt;Well[]&gt; {\n    const results = await this.db.execute(sql`\n      SELECT * FROM wells\n      WHERE search_vector @@ to_tsquery(${query})\n    `);\n    return results.map(this.toDomain);\n  }\n}\n\n// MongoDB uses text index\nexport class MongoDBSearchRepository implements SearchRepository {\n  async searchWells(query: string): Promise&lt;Well[]&gt; {\n    const results = await this.collection.find({\n      $text: { $search: query }\n    }).toArray();\n    return results.map(this.toDomain);\n  }\n}\n</code></pre>\n<h3>JSON Columns</h3>\n<p>PostgreSQL has native JSONB. MongoDB is document-based. SQLite stores JSON as text:</p>\n<pre><code class=\"language-typescript\">// Domain model uses plain objects\nexport class WellInspection {\n  private metadata: Record&lt;string, any&gt;;\n}\n\n// PostgreSQL stores as JSONB\nconst pgSchema = pgTable(&#39;inspections&#39;, {\n  metadata: jsonb(&#39;metadata&#39;),\n});\n\n// MongoDB stores natively\nawait collection.insertOne({\n  metadata: { key: &#39;value&#39; }\n});\n\n// SQLite serializes to text\nconst sqliteSchema = sqliteTable(&#39;inspections&#39;, {\n  metadata: text(&#39;metadata&#39;), // Store as JSON string\n});\n</code></pre>\n<h2>Syncing Between Databases</h2>\n<p>For offline mobile support, we sync between SQLite (mobile) and PostgreSQL (server). This required careful design:</p>\n<h3>Change Tracking</h3>\n<p>Every entity tracks its last modified timestamp:</p>\n<pre><code class=\"language-typescript\">export class WellInspection {\n  private updatedAt: Date;\n\n  update(): void {\n    this.updatedAt = new Date();\n  }\n}\n</code></pre>\n<h3>Sync Algorithm</h3>\n<pre><code class=\"language-typescript\">export class SyncService {\n  async syncInspections(lastSyncAt: Date): Promise&lt;void&gt; {\n    // 1. Get changes from server since last sync\n    const serverChanges = await this.serverRepository.findModifiedSince(lastSyncAt);\n\n    // 2. Get local changes since last sync\n    const localChanges = await this.localRepository.findModifiedSince(lastSyncAt);\n\n    // 3. Apply server changes to local DB\n    for (const change of serverChanges) {\n      await this.localRepository.save(change);\n    }\n\n    // 4. Push local changes to server\n    for (const change of localChanges) {\n      await this.serverRepository.save(change);\n    }\n\n    // 5. Update sync timestamp\n    await this.syncStateRepository.updateLastSync(new Date());\n  }\n}\n</code></pre>\n<h3>Conflict Resolution</h3>\n<p>When the same entity is modified offline and on the server, we use last-write-wins:</p>\n<pre><code class=\"language-typescript\">async resolveConflict(\n  local: WellInspection,\n  server: WellInspection\n): Promise&lt;WellInspection&gt; {\n  if (local.updatedAt &gt; server.updatedAt) {\n    return local; // Local change is newer\n  }\n  return server; // Server change is newer\n}\n</code></pre>\n<p>For critical data, we flag conflicts for manual resolution:</p>\n<pre><code class=\"language-typescript\">if (local.updatedAt === server.updatedAt &amp;&amp; !local.equals(server)) {\n  await this.conflictRepository.save(new Conflict(local, server));\n  throw new ConflictError(&#39;Manual resolution required&#39;);\n}\n</code></pre>\n<h2>Testing Across Databases</h2>\n<p>Database-agnostic architecture made testing straightforward:</p>\n<pre><code class=\"language-typescript\">describe(&#39;WellInspectionService&#39;, () =&gt; {\n  const databases = [&#39;postgresql&#39;, &#39;mongodb&#39;, &#39;sqlite&#39;];\n\n  databases.forEach((dbType) =&gt; {\n    describe(`with ${dbType}`, () =&gt; {\n      let repository: WellInspectionRepository;\n\n      beforeEach(async () =&gt; {\n        repository = await createRepositoryForDatabase(dbType);\n      });\n\n      it(&#39;should complete inspection&#39;, async () =&gt; {\n        const inspection = new WellInspection(/* ... */);\n        await repository.save(inspection);\n\n        inspection.complete();\n        await repository.save(inspection);\n\n        const retrieved = await repository.findById(inspection.id);\n        expect(retrieved.status).toBe(InspectionStatus.COMPLETED);\n      });\n    });\n  });\n});\n</code></pre>\n<p>We run the full test suite against all three databases. This caught database-specific bugs early.</p>\n<h2>Lessons Learned</h2>\n<h3>Lesson 1: Keep Repository Interface Simple</h3>\n<p>We initially added too many specialized query methods. This made it hard to implement across databases. We learned to keep the interface minimal and use specification pattern for complex queries.</p>\n<h3>Lesson 2: Accept Some Duplication</h3>\n<p>Having separate schema files for PostgreSQL and SQLite felt like duplication, but trying to abstract them was worse. Some duplication is okay.</p>\n<h3>Lesson 3: Document Database Limitations</h3>\n<p>We documented which features weren&#39;t available on which databases:</p>\n<pre><code class=\"language-typescript\">/**\n * Note: Full-text search quality varies by database:\n * - PostgreSQL: Excellent (ts_vector)\n * - MongoDB: Good (text indexes)\n * - SQLite: Basic (FTS5)\n */\nexport interface SearchRepository {\n  searchWells(query: string): Promise&lt;Well[]&gt;;\n}\n</code></pre>\n<h3>Lesson 4: Use the Right Tool Per Environment</h3>\n<p>We don&#39;t force the same database everywhere:</p>\n<ul>\n<li><strong>Production SaaS</strong>: PostgreSQL for ACID guarantees</li>\n<li><strong>Mobile offline</strong>: SQLite for simplicity</li>\n<li><strong>On-premise</strong>: MongoDB if client prefers it</li>\n</ul>\n<p>Database-agnostic architecture gave us this flexibility.</p>\n<h2>When to Use Database-Agnostic Architecture</h2>\n<p>Use it when:</p>\n<ul>\n<li>Supporting multiple deployment environments</li>\n<li>Building offline-first applications</li>\n<li>Migrating between databases</li>\n<li>Letting enterprise clients choose their database</li>\n<li>Future database flexibility is valuable</li>\n</ul>\n<p>Don&#39;t use it when:</p>\n<ul>\n<li>You need advanced database-specific features</li>\n<li>Performance optimizations require database-specific code</li>\n<li>You&#39;re certain you&#39;ll never change databases</li>\n<li>Team lacks discipline to maintain abstraction</li>\n</ul>\n<h2>Conclusion</h2>\n<p>Building WellOS with database-agnostic architecture let us support PostgreSQL, MongoDB, and SQLite with a single codebase. Field workers use SQLite offline, corporate users have PostgreSQL, and on-premise clients can use MongoDB.</p>\n<p>The key is clean separation between domain logic and persistence, simple repository interfaces, and discipline to avoid database-specific features leaking into business code.</p>\n<p>After 27 years of building applications, I&#39;ve learned that staying database-agnostic provides invaluable flexibility. The slight complexity cost pays for itself when requirements inevitably change.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/database-agnostic-architecture/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-07-27 00:00:00",
            "updated_at": "2024-07-27 00:00:00",
            "published_at": "2024-07-27 00:00:00",
            "custom_excerpt": "Why WellOS needed database-agnostic architecture to run on PostgreSQL (cloud), MongoDB (on-premise), and SQLite (offline mobile). Two years of production lessons supporting 6 apps across 3 databases."
          },
          {
            "id": "33",
            "title": "AWS Integration Patterns: SES, SNS, and S3 in Production Applications",
            "slug": "aws-integration-patterns",
            "html": "<p>During the EMA project at Servant, we integrated multiple AWS services to handle notifications, file uploads, and messaging. Here&#39;s what I learned about building reliable, scalable integrations with AWS.</p>\n<h2>AWS SES for Email Notifications</h2>\n<p>Amazon Simple Email Service (SES) provided our email infrastructure. Here&#39;s how we implemented a robust email service:</p>\n<pre><code class=\"language-typescript\">// email.service.ts\nimport { Injectable } from &#39;@nestjs/common&#39;;\nimport { SESClient, SendEmailCommand } from &#39;@aws-sdk/client-ses&#39;;\nimport { ConfigService } from &#39;@nestjs/config&#39;;\n\n@Injectable()\nexport class EmailService {\n  private sesClient: SESClient;\n  private fromEmail: string;\n\n  constructor(private configService: ConfigService) {\n    this.sesClient = new SESClient({\n      region: this.configService.get(&#39;AWS_REGION&#39;),\n      credentials: {\n        accessKeyId: this.configService.get(&#39;AWS_ACCESS_KEY_ID&#39;),\n        secretAccessKey: this.configService.get(&#39;AWS_SECRET_ACCESS_KEY&#39;),\n      },\n    });\n    this.fromEmail = this.configService.get(&#39;SES_FROM_EMAIL&#39;);\n  }\n\n  async sendReferralNotification(\n    to: string,\n    referralData: ReferralNotificationData\n  ): Promise&lt;void&gt; {\n    const htmlBody = this.generateReferralEmailHtml(referralData);\n    const textBody = this.generateReferralEmailText(referralData);\n\n    const command = new SendEmailCommand({\n      Source: this.fromEmail,\n      Destination: {\n        ToAddresses: [to],\n      },\n      Message: {\n        Subject: {\n          Data: `New Referral: ${referralData.motherName}`,\n          Charset: &#39;UTF-8&#39;,\n        },\n        Body: {\n          Html: {\n            Data: htmlBody,\n            Charset: &#39;UTF-8&#39;,\n          },\n          Text: {\n            Data: textBody,\n            Charset: &#39;UTF-8&#39;,\n          },\n        },\n      },\n    });\n\n    try {\n      await this.sesClient.send(command);\n      console.log(`Email sent successfully to ${to}`);\n    } catch (error) {\n      console.error(&#39;Failed to send email:&#39;, error);\n      // Don&#39;t throw - we don&#39;t want email failures to break the app\n      // Instead, log to monitoring service\n      this.logToMonitoring(error);\n    }\n  }\n\n  private generateReferralEmailHtml(data: ReferralNotificationData): string {\n    return `\n      &lt;!DOCTYPE html&gt;\n      &lt;html&gt;\n        &lt;head&gt;\n          &lt;style&gt;\n            body { font-family: Arial, sans-serif; }\n            .container { max-width: 600px; margin: 0 auto; padding: 20px; }\n            .header { background-color: #4A90E2; color: white; padding: 20px; }\n            .content { padding: 20px; background-color: #f5f5f5; }\n            .footer { padding: 10px; text-align: center; color: #666; }\n            .priority-high { color: #E74C3C; font-weight: bold; }\n          &lt;/style&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n          &lt;div class=&quot;container&quot;&gt;\n            &lt;div class=&quot;header&quot;&gt;\n              &lt;h1&gt;New Referral Received&lt;/h1&gt;\n            &lt;/div&gt;\n            &lt;div class=&quot;content&quot;&gt;\n              &lt;p&gt;&lt;strong&gt;Mother&#39;s Name:&lt;/strong&gt; ${data.motherName}&lt;/p&gt;\n              &lt;p&gt;&lt;strong&gt;Priority:&lt;/strong&gt;\n                &lt;span class=&quot;${data.priority === &#39;HIGH&#39; ? &#39;priority-high&#39; : &#39;&#39;}&quot;&gt;\n                  ${data.priority}\n                &lt;/span&gt;\n              &lt;/p&gt;\n              &lt;p&gt;&lt;strong&gt;Referred By:&lt;/strong&gt; ${data.referredBy}&lt;/p&gt;\n              &lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt; ${data.notes}&lt;/p&gt;\n              &lt;p&gt;\n                &lt;a href=&quot;${data.portalUrl}&quot;\n                   style=&quot;background-color: #4A90E2; color: white; padding: 10px 20px;\n                          text-decoration: none; border-radius: 5px;&quot;&gt;\n                  View in Portal\n                &lt;/a&gt;\n              &lt;/p&gt;\n            &lt;/div&gt;\n            &lt;div class=&quot;footer&quot;&gt;\n              &lt;p&gt;EMA Portal - Supporting Mothers and Families&lt;/p&gt;\n            &lt;/div&gt;\n          &lt;/div&gt;\n        &lt;/body&gt;\n      &lt;/html&gt;\n    `;\n  }\n}\n</code></pre>\n<h2>AWS SNS for SMS Notifications</h2>\n<p>For time-sensitive notifications, we used SNS to send SMS messages:</p>\n<pre><code class=\"language-typescript\">// sms.service.ts\nimport { Injectable } from &#39;@nestjs/common&#39;;\nimport { SNSClient, PublishCommand } from &#39;@aws-sdk/client-sns&#39;;\nimport { ConfigService } from &#39;@nestjs/config&#39;;\n\n@Injectable()\nexport class SmsService {\n  private snsClient: SNSClient;\n\n  constructor(private configService: ConfigService) {\n    this.snsClient = new SNSClient({\n      region: this.configService.get(&#39;AWS_REGION&#39;),\n      credentials: {\n        accessKeyId: this.configService.get(&#39;AWS_ACCESS_KEY_ID&#39;),\n        secretAccessKey: this.configService.get(&#39;AWS_SECRET_ACCESS_KEY&#39;),\n      },\n    });\n  }\n\n  async sendUrgentAlert(\n    phoneNumber: string,\n    message: string\n  ): Promise&lt;void&gt; {\n    // Format phone number to E.164 format\n    const formattedNumber = this.formatPhoneNumber(phoneNumber);\n\n    const command = new PublishCommand({\n      PhoneNumber: formattedNumber,\n      Message: message,\n      MessageAttributes: {\n        &#39;AWS.SNS.SMS.SenderID&#39;: {\n          DataType: &#39;String&#39;,\n          StringValue: &#39;EMAPortal&#39;,\n        },\n        &#39;AWS.SNS.SMS.SMSType&#39;: {\n          DataType: &#39;String&#39;,\n          StringValue: &#39;Transactional&#39;, // For important, time-sensitive messages\n        },\n      },\n    });\n\n    try {\n      const response = await this.snsClient.send(command);\n      console.log(`SMS sent successfully. MessageId: ${response.MessageId}`);\n    } catch (error) {\n      console.error(&#39;Failed to send SMS:&#39;, error);\n      throw new Error(`SMS delivery failed: ${error.message}`);\n    }\n  }\n\n  private formatPhoneNumber(phoneNumber: string): string {\n    // Remove all non-numeric characters\n    const cleaned = phoneNumber.replace(/\\D/g, &#39;&#39;);\n\n    // Add +1 for US numbers if not present\n    if (!cleaned.startsWith(&#39;1&#39;) &amp;&amp; cleaned.length === 10) {\n      return `+1${cleaned}`;\n    }\n\n    if (!cleaned.startsWith(&#39;+&#39;)) {\n      return `+${cleaned}`;\n    }\n\n    return cleaned;\n  }\n\n  async sendBatchAlerts(\n    recipients: Array&lt;{ phoneNumber: string; message: string }&gt;\n  ): Promise&lt;void&gt; {\n    const promises = recipients.map(({ phoneNumber, message }) =&gt;\n      this.sendUrgentAlert(phoneNumber, message)\n        .catch(error =&gt; ({\n          phoneNumber,\n          error: error.message,\n        }))\n    );\n\n    const results = await Promise.allSettled(promises);\n\n    // Log any failures\n    results.forEach((result, index) =&gt; {\n      if (result.status === &#39;rejected&#39;) {\n        console.error(\n          `Failed to send SMS to ${recipients[index].phoneNumber}:`,\n          result.reason\n        );\n      }\n    });\n  }\n}\n</code></pre>\n<h2>AWS S3 for File Uploads</h2>\n<p>File uploads required careful handling, especially for large files. We implemented multipart uploads:</p>\n<pre><code class=\"language-typescript\">// file-upload.service.ts\nimport { Injectable } from &#39;@nestjs/common&#39;;\nimport {\n  S3Client,\n  PutObjectCommand,\n  GetObjectCommand,\n  DeleteObjectCommand\n} from &#39;@aws-sdk/client-s3&#39;;\nimport { getSignedUrl } from &#39;@aws-sdk/s3-request-presigner&#39;;\nimport { ConfigService } from &#39;@nestjs/config&#39;;\nimport { v4 as uuidv4 } from &#39;uuid&#39;;\n\n@Injectable()\nexport class FileUploadService {\n  private s3Client: S3Client;\n  private bucketName: string;\n\n  constructor(private configService: ConfigService) {\n    this.s3Client = new S3Client({\n      region: this.configService.get(&#39;AWS_REGION&#39;),\n      credentials: {\n        accessKeyId: this.configService.get(&#39;AWS_ACCESS_KEY_ID&#39;),\n        secretAccessKey: this.configService.get(&#39;AWS_SECRET_ACCESS_KEY&#39;),\n      },\n    });\n    this.bucketName = this.configService.get(&#39;S3_BUCKET_NAME&#39;);\n  }\n\n  async uploadFile(\n    file: Express.Multer.File,\n    folder: string = &#39;documents&#39;\n  ): Promise&lt;{ url: string; key: string }&gt; {\n    // Validate file type and size\n    this.validateFile(file);\n\n    // Generate unique filename\n    const fileExtension = file.originalname.split(&#39;.&#39;).pop();\n    const key = `${folder}/${uuidv4()}.${fileExtension}`;\n\n    const command = new PutObjectCommand({\n      Bucket: this.bucketName,\n      Key: key,\n      Body: file.buffer,\n      ContentType: file.mimetype,\n      Metadata: {\n        originalName: file.originalname,\n        uploadedAt: new Date().toISOString(),\n      },\n    });\n\n    try {\n      await this.s3Client.send(command);\n\n      const url = `https://${this.bucketName}.s3.${\n        this.configService.get(&#39;AWS_REGION&#39;)\n      }.amazonaws.com/${key}`;\n\n      return { url, key };\n    } catch (error) {\n      console.error(&#39;S3 upload error:&#39;, error);\n      throw new Error(`File upload failed: ${error.message}`);\n    }\n  }\n\n  async getSignedDownloadUrl(key: string, expiresIn: number = 3600): Promise&lt;string&gt; {\n    const command = new GetObjectCommand({\n      Bucket: this.bucketName,\n      Key: key,\n    });\n\n    try {\n      const url = await getSignedUrl(this.s3Client, command, { expiresIn });\n      return url;\n    } catch (error) {\n      console.error(&#39;Failed to generate signed URL:&#39;, error);\n      throw new Error(`Could not generate download URL: ${error.message}`);\n    }\n  }\n\n  async deleteFile(key: string): Promise&lt;void&gt; {\n    const command = new DeleteObjectCommand({\n      Bucket: this.bucketName,\n      Key: key,\n    });\n\n    try {\n      await this.s3Client.send(command);\n      console.log(`File deleted: ${key}`);\n    } catch (error) {\n      console.error(&#39;Failed to delete file:&#39;, error);\n      throw new Error(`File deletion failed: ${error.message}`);\n    }\n  }\n\n  private validateFile(file: Express.Multer.File): void {\n    const maxSize = 10 * 1024 * 1024; // 10MB\n    const allowedTypes = [\n      &#39;application/pdf&#39;,\n      &#39;image/jpeg&#39;,\n      &#39;image/png&#39;,\n      &#39;application/msword&#39;,\n      &#39;application/vnd.openxmlformats-officedocument.wordprocessingml.document&#39;,\n    ];\n\n    if (file.size &gt; maxSize) {\n      throw new Error(&#39;File size exceeds 10MB limit&#39;);\n    }\n\n    if (!allowedTypes.includes(file.mimetype)) {\n      throw new Error(&#39;File type not allowed&#39;);\n    }\n  }\n}\n</code></pre>\n<h2>Integration in Controllers</h2>\n<p>Here&#39;s how these services work together in a controller:</p>\n<pre><code class=\"language-typescript\">// referrals.controller.ts\n@Controller(&#39;referrals&#39;)\nexport class ReferralsController {\n  constructor(\n    private referralsService: ReferralsService,\n    private emailService: EmailService,\n    private smsService: SmsService,\n    private fileUploadService: FileUploadService,\n  ) {}\n\n  @Post()\n  @UseInterceptors(FilesInterceptor(&#39;documents&#39;, 5))\n  async createReferral(\n    @Body() createReferralDto: CreateReferralDto,\n    @UploadedFiles() files: Express.Multer.File[],\n  ) {\n    // Create the referral\n    const referral = await this.referralsService.create(createReferralDto);\n\n    // Upload any attached documents\n    if (files &amp;&amp; files.length &gt; 0) {\n      const uploadPromises = files.map(file =&gt;\n        this.fileUploadService.uploadFile(file, `referrals/${referral.id}`)\n      );\n\n      const uploadedFiles = await Promise.all(uploadPromises);\n\n      await this.referralsService.attachDocuments(\n        referral.id,\n        uploadedFiles\n      );\n    }\n\n    // Send email notification\n    await this.emailService.sendReferralNotification(\n      createReferralDto.coordinatorEmail,\n      {\n        motherName: referral.motherName,\n        priority: referral.priority,\n        referredBy: referral.user.firstName,\n        notes: referral.notes,\n        portalUrl: `${process.env.PORTAL_URL}/referrals/${referral.id}`,\n      }\n    );\n\n    // Send SMS for urgent cases\n    if (referral.priority === &#39;URGENT&#39;) {\n      await this.smsService.sendUrgentAlert(\n        createReferralDto.coordinatorPhone,\n        `URGENT: New referral for ${referral.motherName}. Check portal immediately.`\n      );\n    }\n\n    return referral;\n  }\n}\n</code></pre>\n<h2>Best Practices</h2>\n<ol>\n<li><strong>Error Handling</strong>: AWS operations can fail. Always handle errors gracefully</li>\n<li><strong>Retry Logic</strong>: Implement exponential backoff for transient failures</li>\n<li><strong>Monitoring</strong>: Use CloudWatch to track service usage and errors</li>\n<li><strong>Cost Management</strong>: Be aware of pricing - SES, SNS, and S3 costs add up</li>\n<li><strong>Security</strong>: Never commit AWS credentials. Use environment variables or IAM roles</li>\n</ol>\n<h2>Conclusion</h2>\n<p>AWS services like SES, SNS, and S3 provide powerful building blocks for full-stack applications. The key is implementing them with proper error handling, monitoring, and cost awareness.</p>\n<p>These patterns have served me well across multiple projects and can scale from small applications to enterprise systems.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/aws-integration-patterns/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-07-04 00:00:00",
            "updated_at": "2024-07-04 00:00:00",
            "published_at": "2024-07-04 00:00:00",
            "custom_excerpt": "Practical patterns for integrating AWS services into full-stack applications, from email notifications to file uploads"
          },
          {
            "id": "34",
            "title": "Building Full-Stack Applications with NestJS and Next.js: A Perfect Match",
            "slug": "nestjs-nextjs-full-stack-architecture",
            "html": "<p>After 27 years of building web applications with various tech stacks, I&#39;ve found that the combination of NestJS and Next.js represents one of the most productive and maintainable approaches to full-stack development. Both at Servant and in my recent projects, this duo has proven invaluable.</p>\n<h2>Why This Stack Works</h2>\n<p>The synergy between NestJS and Next.js goes beyond both being TypeScript frameworks. They share similar philosophies around structure, conventions, and developer experience while solving different problems beautifully.</p>\n<h3>NestJS: The Backend Powerhouse</h3>\n<p>NestJS brings enterprise-grade architecture patterns to Node.js. Its dependency injection system, modular structure, and decorator-based approach make it easy to build scalable APIs:</p>\n<pre><code class=\"language-typescript\">// auth.controller.ts\n@Controller(&#39;auth&#39;)\nexport class AuthController {\n  constructor(\n    private readonly authService: AuthService,\n    private readonly jwtService: JwtService\n  ) {}\n\n  @Post(&#39;login&#39;)\n  @HttpCode(HttpStatus.OK)\n  async login(@Body() loginDto: LoginDto) {\n    const user = await this.authService.validateUser(\n      loginDto.email,\n      loginDto.password\n    );\n\n    if (!user) {\n      throw new UnauthorizedException(&#39;Invalid credentials&#39;);\n    }\n\n    return {\n      access_token: this.jwtService.sign({\n        sub: user.id,\n        email: user.email\n      })\n    };\n  }\n}\n</code></pre>\n<h3>Next.js: The Frontend Framework</h3>\n<p>Next.js provides server-side rendering, static generation, and API routes out of the box. For the EMA project at Servant, we leveraged Next.js&#39;s App Router for an optimized user experience:</p>\n<pre><code class=\"language-typescript\">// app/dashboard/page.tsx\nimport { DashboardStats } from &#39;@/components/dashboard-stats&#39;;\nimport { authOptions } from &#39;@/lib/auth&#39;;\nimport { getServerSession } from &#39;next-auth&#39;;\n\nexport default async function DashboardPage() {\n  const session = await getServerSession(authOptions);\n\n  // Fetch data on the server\n  const stats = await fetch(\n    `${process.env.API_URL}/stats`,\n    {\n      headers: {\n        Authorization: `Bearer ${session?.accessToken}`\n      }\n    }\n  ).then(res =&gt; res.json());\n\n  return (\n    &lt;main className=&quot;container mx-auto px-4 py-8&quot;&gt;\n      &lt;h1 className=&quot;text-3xl font-bold mb-6&quot;&gt;Dashboard&lt;/h1&gt;\n      \n    &lt;/main&gt;\n  );\n}\n</code></pre>\n<h2>Shared TypeScript Types</h2>\n<p>One of the biggest advantages is sharing type definitions between frontend and backend. I typically structure this with a shared types package:</p>\n<pre><code class=\"language-typescript\">// shared/types/user.types.ts\nexport interface User {\n  id: string;\n  email: string;\n  firstName: string;\n  lastName: string;\n  role: UserRole;\n}\n\nexport enum UserRole {\n  ADMIN = &#39;admin&#39;,\n  COORDINATOR = &#39;coordinator&#39;,\n  ADVOCATE = &#39;advocate&#39;,\n  MOTHER = &#39;mother&#39;\n}\n\nexport interface CreateUserDto {\n  email: string;\n  password: string;\n  firstName: string;\n  lastName: string;\n  role: UserRole;\n}\n</code></pre>\n<p>These types are imported in both the NestJS backend and Next.js frontend, ensuring type safety across the entire application.</p>\n<h2>API Documentation with Swagger</h2>\n<p>NestJS makes API documentation trivial with Swagger integration:</p>\n<pre><code class=\"language-typescript\">// main.ts\nconst config = new DocumentBuilder()\n  .setTitle(&#39;EMA Portal API&#39;)\n  .setDescription(&#39;API for advocates, coordinators, and mothers&#39;)\n  .setVersion(&#39;1.0&#39;)\n  .addBearerAuth()\n  .build();\n\nconst document = SwaggerModule.createDocument(app, config);\nSwaggerModule.setup(&#39;api/docs&#39;, app, document);\n</code></pre>\n<p>This auto-generated documentation became essential for frontend developers to understand available endpoints.</p>\n<h2>Deployment Strategy</h2>\n<p>For the Servant project, we used Railway for deployment, which made it simple to deploy both applications:</p>\n<ul>\n<li>NestJS API as a separate service</li>\n<li>Next.js portal as another service</li>\n<li>PostgreSQL database as a third service</li>\n</ul>\n<p>All connected through environment variables and internal networking.</p>\n<h2>Key Benefits</h2>\n<p>After building multiple projects with this stack, the benefits are clear:</p>\n<ol>\n<li><strong>End-to-end type safety</strong>: TypeScript across the entire stack reduces bugs significantly</li>\n<li><strong>Excellent DX</strong>: Both frameworks have outstanding developer experiences</li>\n<li><strong>Scalability</strong>: The modular architecture of both frameworks makes scaling straightforward</li>\n<li><strong>Rich ecosystems</strong>: Massive communities and plugin ecosystems</li>\n<li><strong>Performance</strong>: Both frameworks are optimized for production use</li>\n</ol>\n<h2>Conclusion</h2>\n<p>The NestJS + Next.js combination has become my go-to for enterprise applications. The shared language, similar architectural patterns, and excellent tooling create a productive environment that scales from small projects to large enterprise systems.</p>\n<p>If you&#39;re building a new full-stack TypeScript application, give this combination serious consideration. The investment in learning both frameworks pays dividends in productivity and maintainability.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/nestjs-nextjs-full-stack-architecture/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-06-21 00:00:00",
            "updated_at": "2024-06-21 00:00:00",
            "published_at": "2024-06-21 00:00:00",
            "custom_excerpt": "Why NestJS + Next.js became my go-to full-stack architecture after 27 years of building web applications. Production patterns from Servant and enterprise projects with type safety end-to-end."
          },
          {
            "id": "35",
            "title": "Domain-Driven Design: Lessons from a 320K+ LOC Codebase",
            "slug": "domain-driven-design-lessons-learned",
            "html": "<p>When we started building the Catalyst PSA platform, we knew we were in for the long haul. Professional Services Automation is complex—projects, time tracking, resource management, invoicing, expense tracking—all interconnected with subtle business rules. After 27 years of building software, I knew that without a solid domain model, we&#39;d end up with an unmaintainable mess.</p>\n<p>Domain-Driven Design (DDD) gave us the framework to tackle this complexity. Three years later our investment in DDD has paid enormous dividends. Here&#39;s what we learned.</p>\n<h2>What is Domain-Driven Design?</h2>\n<p>DDD, introduced by Eric Evans, is a software development approach that focuses on deeply understanding the business domain and expressing that understanding in code. It&#39;s not just about patterns—it&#39;s about creating a shared language between developers and domain experts, and modeling that language in your software.</p>\n<p>The core concepts include:</p>\n<ul>\n<li><strong>Ubiquitous Language</strong>: A shared vocabulary between technical and business teams</li>\n<li><strong>Bounded Contexts</strong>: Clear boundaries around related concepts</li>\n<li><strong>Entities and Value Objects</strong>: Domain objects with identity vs. without</li>\n<li><strong>Aggregates</strong>: Consistency boundaries around related entities</li>\n<li><strong>Domain Events</strong>: Things that happened in the domain</li>\n<li><strong>Repositories</strong>: Abstraction for retrieving domain objects</li>\n</ul>\n<h2>Building a Ubiquitous Language</h2>\n<p>Our first step was establishing a common language with our PSA domain experts. This sounds simple but was transformative.</p>\n<h3>Before: Developer Language vs. Business Language</h3>\n<p>Developers said: &quot;We need a table for work items with a foreign key to users and a status flag.&quot;</p>\n<p>Business experts said: &quot;When consultants log billable hours against client projects, we need to track whether those hours have been approved by the project manager and invoiced to the client.&quot;</p>\n<p>These describe the same thing, but the disconnect was causing confusion.</p>\n<h3>After: Ubiquitous Language</h3>\n<p>We settled on terms like:</p>\n<ul>\n<li><strong>Time Entry</strong> (not &quot;work item&quot; or &quot;timesheet record&quot;)</li>\n<li><strong>Billable Hours</strong> (not &quot;invoiceable time&quot;)</li>\n<li><strong>Project Assignment</strong> (not &quot;user project mapping&quot;)</li>\n<li><strong>Approval Workflow</strong> (not &quot;status flag&quot;)</li>\n</ul>\n<p>Then we used these exact terms in our code:</p>\n<pre><code class=\"language-typescript\">export class TimeEntry extends Entity {\n  constructor(\n    id: string,\n    private consultantId: string,\n    private projectAssignment: ProjectAssignment,\n    private billableHours: BillableHours,\n    private approvalWorkflow: ApprovalWorkflow\n  ) {\n    super(id);\n  }\n\n  submitForApproval(): void {\n    this.approvalWorkflow.submit();\n    this.addDomainEvent(new TimeEntrySubmittedEvent(this.id));\n  }\n\n  approveByProjectManager(managerId: string): void {\n    if (!this.projectAssignment.isManager(managerId)) {\n      throw new UnauthorizedApprovalError();\n    }\n    this.approvalWorkflow.approve();\n    this.addDomainEvent(new TimeEntryApprovedEvent(this.id));\n  }\n}\n</code></pre>\n<p>When business experts reviewed this code, they understood it immediately. When developers talked to clients, they used the same terminology. This alignment eliminated countless misunderstandings.</p>\n<h2>Identifying Bounded Contexts</h2>\n<p>The biggest challenge in DDD is finding the right boundaries for your contexts. A term like &quot;Project&quot; might mean different things in different parts of your system.</p>\n<h3>Our Bounded Contexts in Catalyst PSA</h3>\n<p>We identified seven major bounded contexts:</p>\n<ol>\n<li><strong>Project Management</strong>: Projects, tasks, milestones, dependencies</li>\n<li><strong>Time Tracking</strong>: Time entries, approvals, corrections</li>\n<li><strong>Resource Management</strong>: Consultant availability, skills, assignments</li>\n<li><strong>Billing &amp; Invoicing</strong>: Invoice generation, payment tracking</li>\n<li><strong>Expense Management</strong>: Expense claims, approvals, reimbursements</li>\n<li><strong>Client Management</strong>: Client information, contracts, contacts</li>\n<li><strong>Reporting &amp; Analytics</strong>: Cross-context data aggregation</li>\n</ol>\n<p>Each context had its own models. A &quot;Project&quot; in the Project Management context had tasks, dependencies, and Gantt chart data. A &quot;Project&quot; in the Billing context was just an invoice line item grouping. They shared an ID but were otherwise independent.</p>\n<h3>Bounded Context Implementation</h3>\n<p>We enforced context boundaries in code:</p>\n<pre><code>src/\n  project-management/\n    domain/\n    application/\n    infrastructure/\n  time-tracking/\n    domain/\n    application/\n    infrastructure/\n  billing/\n    domain/\n    application/\n    infrastructure/\n</code></pre>\n<p>Contexts could only communicate through:</p>\n<ol>\n<li><strong>Domain Events</strong>: Async, eventual consistency</li>\n<li><strong>APIs</strong>: Synchronous queries for read-only data</li>\n<li><strong>Shared Kernel</strong>: Common value objects (Money, Date ranges)</li>\n</ol>\n<p>We had linting rules to prevent direct imports across contexts:</p>\n<pre><code class=\"language-typescript\">// ❌ Not allowed\nimport { Project } from &#39;../../project-management/domain/project&#39;;\n\n// ✅ Allowed - consuming events\n@EventsHandler(ProjectCreatedEvent)\nexport class SyncProjectToBilling implements IEventHandler {\n  // ...\n}\n\n// ✅ Allowed - querying through API\nconst project = await this.projectQueryService.getById(projectId);\n</code></pre>\n<h2>Entities vs. Value Objects</h2>\n<p>Understanding the difference between entities and value objects was crucial for our domain model.</p>\n<h3>Entities: Objects with Identity</h3>\n<p>Entities have a unique identity that persists over time. Even if all their attributes change, they&#39;re still the same entity.</p>\n<pre><code class=\"language-typescript\">export class Consultant extends Entity {\n  constructor(\n    id: string,\n    private email: Email, // Value object\n    private name: PersonName, // Value object\n    private hourlyRate: Money, // Value object\n    private skills: Skill[] // Value objects\n  ) {\n    super(id);\n  }\n\n  updateHourlyRate(newRate: Money): void {\n    this.hourlyRate = newRate;\n    this.addDomainEvent(new ConsultantRateChangedEvent(this.id, newRate));\n  }\n}\n</code></pre>\n<p>A consultant with ID &quot;cons-123&quot; is always that consultant, even if they change their name, email, and rate.</p>\n<h3>Value Objects: Objects without Identity</h3>\n<p>Value objects are defined by their attributes. Two value objects with the same attributes are interchangeable.</p>\n<pre><code class=\"language-typescript\">export class Money {\n  constructor(\n    private readonly amount: number,\n    private readonly currency: Currency\n  ) {\n    if (amount &lt; 0) {\n      throw new InvalidMoneyError(&#39;Amount cannot be negative&#39;);\n    }\n  }\n\n  add(other: Money): Money {\n    if (!this.currency.equals(other.currency)) {\n      throw new CurrencyMismatchError();\n    }\n    return new Money(this.amount + other.amount, this.currency);\n  }\n\n  equals(other: Money): boolean {\n    return this.amount === other.amount &amp;&amp; this.currency.equals(other.currency);\n  }\n}\n</code></pre>\n<p>Money with value ($100, USD) is identical to any other ($100, USD). We don&#39;t care which specific instance we have.</p>\n<p>Value objects are immutable. Operations return new instances rather than modifying existing ones. This made our code much easier to reason about and prevented subtle bugs.</p>\n<h2>Aggregates: Consistency Boundaries</h2>\n<p>Aggregates were the hardest DDD concept to get right. An aggregate is a cluster of entities and value objects that are treated as a single unit for data changes.</p>\n<h3>The Problem: Transactional Boundaries</h3>\n<p>Initially, we made everything transactional. Updating a project would update all its tasks, all time entries on those tasks, update resource allocations, and recalculate budgets—all in one transaction. This was slow and caused deadlocks.</p>\n<h3>The Solution: Smaller Aggregates</h3>\n<p>We learned to keep aggregates small and use eventual consistency between them:</p>\n<pre><code class=\"language-typescript\">// Project aggregate - small and focused\nexport class Project extends AggregateRoot {\n  constructor(\n    id: string,\n    private name: string,\n    private clientId: string, // Reference to another aggregate\n    private status: ProjectStatus,\n    private budget: Money\n  ) {\n    super(id);\n  }\n\n  complete(): void {\n    if (this.status === ProjectStatus.COMPLETED) {\n      throw new ProjectAlreadyCompletedError();\n    }\n    this.status = ProjectStatus.COMPLETED;\n    this.addDomainEvent(new ProjectCompletedEvent(this.id));\n  }\n}\n\n// Task is a separate aggregate, not part of Project\nexport class Task extends AggregateRoot {\n  constructor(\n    id: string,\n    private projectId: string, // Reference to Project aggregate\n    private name: string,\n    private assigneeId: string\n  ) {\n    super(id);\n  }\n}\n</code></pre>\n<p>When a project completes, we emit an event. An event handler updates related tasks asynchronously:</p>\n<pre><code class=\"language-typescript\">@EventsHandler(ProjectCompletedEvent)\nexport class CompleteProjectTasksHandler {\n  async handle(event: ProjectCompletedEvent): Promise&lt;void&gt; {\n    const tasks = await this.taskRepository.findByProjectId(event.projectId);\n\n    for (const task of tasks) {\n      if (!task.isCompleted) {\n        task.complete();\n        await this.taskRepository.save(task);\n      }\n    }\n  }\n}\n</code></pre>\n<p>This kept our aggregates small and our transactions fast.</p>\n<h2>Domain Events: The Glue</h2>\n<p>Domain events were the secret to connecting our bounded contexts and aggregates without tight coupling.</p>\n<pre><code class=\"language-typescript\">export class TimeEntryApprovedEvent {\n  constructor(\n    public readonly timeEntryId: string,\n    public readonly consultantId: string,\n    public readonly projectId: string,\n    public readonly billableHours: number,\n    public readonly billableAmount: Money,\n    public readonly occurredAt: Date\n  ) {}\n}\n</code></pre>\n<p>Multiple contexts could react to the same event:</p>\n<pre><code class=\"language-typescript\">// In Billing context - create invoice line item\n@EventsHandler(TimeEntryApprovedEvent)\nexport class CreateInvoiceLineItemHandler {\n  async handle(event: TimeEntryApprovedEvent): Promise&lt;void&gt; {\n    // Add to invoice\n  }\n}\n\n// In Reporting context - update analytics\n@EventsHandler(TimeEntryApprovedEvent)\nexport class UpdateConsultantUtilizationHandler {\n  async handle(event: TimeEntryApprovedEvent): Promise&lt;void&gt; {\n    // Update utilization metrics\n  }\n}\n\n// In Time Tracking context - update read models\n@EventsHandler(TimeEntryApprovedEvent)\nexport class UpdateTimeEntryListHandler {\n  async handle(event: TimeEntryApprovedEvent): Promise&lt;void&gt; {\n    // Update denormalized query models\n  }\n}\n</code></pre>\n<p>Events made our system extensible. When we added expense management, we subscribed to relevant events from other contexts without modifying their code.</p>\n<h2>Repositories: Persistence Abstraction</h2>\n<p>Repositories hide persistence details from the domain:</p>\n<pre><code class=\"language-typescript\">export interface ProjectRepository {\n  findById(id: string): Promise&lt;Project | null&gt;;\n  save(project: Project): Promise&lt;void&gt;;\n  // Note: No update() method - we load, modify, and save\n}\n</code></pre>\n<p>The key insight: repositories work with full aggregates, not individual fields. You load an aggregate, call domain methods, and save it back. This keeps domain logic in the domain.</p>\n<h2>Lessons Learned After 320K+ Lines</h2>\n<h3>Lesson 1: Start with Ubiquitous Language, Not Patterns</h3>\n<p>We initially focused on implementing repositories, aggregates, and entities. This was backwards. We should have started by building a shared language with domain experts. The patterns follow naturally once you understand the domain.</p>\n<h3>Lesson 2: Bounded Contexts Prevent the Big Ball of Mud</h3>\n<p>As the codebase grew, bounded contexts saved us. New developers could understand one context without understanding the entire system. Contexts evolved independently—we rewrote the reporting context twice without touching other contexts.</p>\n<h3>Lesson 3: Keep Aggregates Small</h3>\n<p>Our first aggregates were huge—a Project aggregate containing all tasks, all time entries, all assignments. Loading a project meant loading hundreds of related objects. We learned to make aggregates as small as possible while maintaining invariants.</p>\n<h3>Lesson 4: Eventual Consistency is Okay</h3>\n<p>Business users understood that a project dashboard might take a second to update after they approve time entries. Eventual consistency let us keep aggregates small and the system fast.</p>\n<h3>Lesson 5: DDD is About Continuous Learning</h3>\n<p>Our domain model evolved constantly as we learned more about PSA. We refactored entities into value objects, split contexts, merged contexts. DDD gave us the tools to evolve the model safely.</p>\n<h2>When to Use DDD</h2>\n<p>DDD shines for:</p>\n<ul>\n<li>Complex business domains with subtle rules</li>\n<li>Long-lived applications that will evolve</li>\n<li>Large codebases with multiple teams</li>\n<li>Applications where business logic is the complexity (not technical challenges)</li>\n</ul>\n<p>Skip DDD for:</p>\n<ul>\n<li>Simple CRUD apps</li>\n<li>Technical problems (image processing, data pipelines)</li>\n<li>MVPs where speed matters more than design</li>\n<li>Applications with no complex business rules</li>\n</ul>\n<h2>Conclusion</h2>\n<p>Domain-Driven Design was essential for building and maintaining an enterprise SaaS platform. The ubiquitous language aligned our team, bounded contexts kept complexity manageable, and aggregates gave us the right consistency boundaries.</p>\n<p>After 27 years of building software, I&#39;ve learned that the hardest part isn&#39;t the technology—it&#39;s understanding the business domain and expressing that understanding in code. DDD provides the tools and patterns to do exactly that.</p>\n<p>If you&#39;re building complex business software that needs to evolve over years, invest in DDD. It will pay dividends as your codebase grows.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/domain-driven-design-lessons-learned/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-06-09 00:00:00",
            "updated_at": "2024-06-09 00:00:00",
            "published_at": "2024-06-09 00:00:00",
            "custom_excerpt": "Domain-Driven Design lessons from building Catalyst PSA. How DDD kept a massive enterprise platform maintainable over three years."
          },
          {
            "id": "36",
            "title": "Building Multi-Tenant SaaS: Security and Data Isolation",
            "slug": "building-multi-tenant-saas",
            "html": "<p>Building the Catalyst PSA platform as a multi-tenant SaaS application was one of our biggest architectural challenges. With hundreds of enterprise clients, each with sensitive financial and operational data, we couldn&#39;t afford a single security mistake. Over three years of running in production, our multi-tenant architecture has proven secure, scalable, and maintainable.</p>\n<p>Here&#39;s everything we learned about building multi-tenant SaaS applications the right way.</p>\n<h2>What is Multi-Tenancy?</h2>\n<p>Multi-tenancy means running a single application instance that serves multiple customers (tenants), while keeping each tenant&#39;s data completely isolated. It&#39;s the foundation of modern SaaS.</p>\n<p>The alternative approaches are:</p>\n<ul>\n<li><strong>Single-tenant</strong>: Each customer gets their own application instance and database</li>\n<li><strong>Multi-instance</strong>: Shared application code but separate databases per customer</li>\n<li><strong>Multi-tenant</strong>: Shared application and database with logical data isolation</li>\n</ul>\n<p>We chose multi-tenancy because it offers:</p>\n<ul>\n<li>Lower infrastructure costs (one database vs. hundreds)</li>\n<li>Easier maintenance (one codebase to update)</li>\n<li>Simpler deployment (one application to deploy)</li>\n<li>Better resource utilization (shared compute)</li>\n</ul>\n<p>But it requires solving hard problems: data isolation, security, and performance at scale.</p>\n<h2>Multi-Tenancy Models: Choosing the Right Approach</h2>\n<p>There are three main multi-tenancy models for databases:</p>\n<h3>1. Separate Database Per Tenant</h3>\n<p>Each tenant gets their own database. Maximum isolation but expensive and hard to manage at scale.</p>\n<pre><code>tenant_1_db\ntenant_2_db\ntenant_3_db\n...\n</code></pre>\n<p>We ruled this out because managing hundreds of databases would be operational nightmare. Schema migrations alone would take hours.</p>\n<h3>2. Separate Schema Per Tenant</h3>\n<p>One database, but each tenant gets their own schema (namespace).</p>\n<pre><code>Database: rook_psa\n  - tenant_1 schema\n  - tenant_2 schema\n  - tenant_3 schema\n</code></pre>\n<p>Better than separate databases, but still creates hundreds of schemas. PostgreSQL can handle this, but query planning and connection pooling become complex.</p>\n<h3>3. Shared Schema with Tenant ID Column</h3>\n<p>All tenants share the same tables, with a <code>tenant_id</code> column on every table for isolation.</p>\n<pre><code class=\"language-sql\">CREATE TABLE projects (\n  id UUID PRIMARY KEY,\n  tenant_id UUID NOT NULL,\n  name VARCHAR(255) NOT NULL,\n  -- other columns\n);\n</code></pre>\n<p>This is what we chose. It&#39;s the most scalable approach and simplest to manage, but requires discipline to prevent data leakage.</p>\n<h2>Implementing Row-Level Security</h2>\n<p>Our first line of defense was PostgreSQL&#39;s Row-Level Security (RLS). RLS enforces tenant isolation at the database level, so even a buggy query can&#39;t leak data across tenants.</p>\n<h3>Setting Up RLS</h3>\n<pre><code class=\"language-sql\">-- Enable RLS on the table\nALTER TABLE projects ENABLE ROW LEVEL SECURITY;\n\n-- Create policy to only show rows for current tenant\nCREATE POLICY tenant_isolation_policy ON projects\n  USING (tenant_id = current_setting(&#39;app.current_tenant_id&#39;)::uuid);\n\n-- Policy applies to all operations\nCREATE POLICY tenant_isolation_policy_all ON projects\n  FOR ALL\n  USING (tenant_id = current_setting(&#39;app.current_tenant_id&#39;)::uuid)\n  WITH CHECK (tenant_id = current_setting(&#39;app.current_tenant_id&#39;)::uuid);\n</code></pre>\n<p>Before executing any queries, we set the current tenant:</p>\n<pre><code class=\"language-typescript\">export class TenantContextInterceptor implements NestInterceptor {\n  intercept(context: ExecutionContext, next: CallHandler): Observable&lt;any&gt; {\n    const request = context.switchToHttp().getRequest();\n    const tenantId = request.user.tenantId;\n\n    // Set PostgreSQL session variable\n    return from(this.setTenantContext(tenantId)).pipe(\n      mergeMap(() =&gt; next.handle())\n    );\n  }\n\n  private async setTenantContext(tenantId: string): Promise&lt;void&gt; {\n    await this.db.execute(\n      sql`SET LOCAL app.current_tenant_id = ${tenantId}`\n    );\n  }\n}\n</code></pre>\n<p>Now every query automatically filters by tenant ID. Even if we write:</p>\n<pre><code class=\"language-typescript\">const projects = await this.db.select().from(projects);\n</code></pre>\n<p>PostgreSQL RLS ensures we only get the current tenant&#39;s projects. This is defense in depth—even bugs can&#39;t leak data.</p>\n<h3>Performance Considerations with RLS</h3>\n<p>RLS has a cost—PostgreSQL has to apply the policy on every query. We addressed this with:</p>\n<ol>\n<li><strong>Indexes on tenant_id</strong>: Every table has an index on <code>(tenant_id, id)</code></li>\n</ol>\n<pre><code class=\"language-sql\">CREATE INDEX idx_projects_tenant ON projects(tenant_id, id);\n</code></pre>\n<ol start=\"2\">\n<li><strong>Partition tables by tenant_id</strong> for huge tables:</li>\n</ol>\n<pre><code class=\"language-sql\">CREATE TABLE time_entries (\n  id UUID,\n  tenant_id UUID,\n  -- other columns\n) PARTITION BY HASH (tenant_id);\n</code></pre>\n<ol start=\"3\">\n<li><strong>Connection pooling per tenant</strong> for large customers:</li>\n</ol>\n<pre><code class=\"language-typescript\">export class TenantAwareConnectionPool {\n  private pools: Map&lt;string, Pool&gt; = new Map();\n\n  getConnection(tenantId: string): Pool {\n    if (this.isLargeTenant(tenantId)) {\n      // Dedicated pool for large tenants\n      return this.getOrCreatePool(tenantId);\n    }\n    // Shared pool for small tenants\n    return this.getSharedPool();\n  }\n}\n</code></pre>\n<h2>Application-Level Tenant Isolation</h2>\n<p>RLS is our safety net, but we also enforce tenant isolation in application code for performance and clarity.</p>\n<h3>Tenant-Scoped Repositories</h3>\n<p>Every repository automatically scopes queries to the current tenant:</p>\n<pre><code class=\"language-typescript\">export class ProjectRepository {\n  constructor(\n    private readonly db: DrizzleService,\n    private readonly tenantContext: TenantContext\n  ) {}\n\n  async findById(id: string): Promise&lt;Project | null&gt; {\n    const result = await this.db\n      .select()\n      .from(projects)\n      .where(\n        and(\n          eq(projects.id, id),\n          eq(projects.tenantId, this.tenantContext.currentTenantId)\n        )\n      )\n      .limit(1);\n\n    return result[0] ? this.toDomain(result[0]) : null;\n  }\n\n  async save(project: Project): Promise&lt;void&gt; {\n    // Ensure tenant ID is set\n    const data = {\n      ...this.toPersistence(project),\n      tenantId: this.tenantContext.currentTenantId,\n    };\n\n    await this.db\n      .insert(projects)\n      .values(data)\n      .onConflictDoUpdate({\n        target: [projects.id],\n        set: data,\n      });\n  }\n}\n</code></pre>\n<p>Notice how <code>tenantId</code> is always included in queries and set on inserts. This is redundant with RLS, but makes our code explicit and prevents performance penalties from RLS policy checks.</p>\n<h3>Tenant Context from JWT</h3>\n<p>We extract tenant ID from authenticated user&#39;s JWT token:</p>\n<pre><code class=\"language-typescript\">@Injectable()\nexport class JwtStrategy extends PassportStrategy(Strategy) {\n  validate(payload: JwtPayload): AuthenticatedUser {\n    return {\n      userId: payload.sub,\n      tenantId: payload.tenantId,\n      email: payload.email,\n      roles: payload.roles,\n    };\n  }\n}\n</code></pre>\n<p>Every request carries tenant context. We never trust client-provided tenant IDs—always from the authenticated token.</p>\n<h2>Preventing Cross-Tenant Data Leakage</h2>\n<p>Even with RLS and scoped repositories, we added additional safeguards:</p>\n<h3>1. Foreign Key Validation</h3>\n<p>When creating relationships across entities, validate tenant consistency:</p>\n<pre><code class=\"language-typescript\">export class AssignConsultantToProjectUseCase {\n  async execute(projectId: string, consultantId: string): Promise&lt;void&gt; {\n    const project = await this.projectRepository.findById(projectId);\n    const consultant = await this.consultantRepository.findById(consultantId);\n\n    // This check is critical!\n    if (project.tenantId !== consultant.tenantId) {\n      throw new TenantMismatchError(\n        &#39;Cannot assign consultant from different tenant&#39;\n      );\n    }\n\n    await this.assignmentRepository.create(projectId, consultantId);\n  }\n}\n</code></pre>\n<h3>2. Audit Logging</h3>\n<p>We log every cross-tenant attempt (which should never happen):</p>\n<pre><code class=\"language-typescript\">@Injectable()\nexport class TenantSecurityAuditor {\n  logCrossTenantAttempt(\n    userId: string,\n    userTenantId: string,\n    attemptedTenantId: string,\n    resource: string\n  ): void {\n    this.logger.error(&#39;SECURITY: Cross-tenant access attempt&#39;, {\n      userId,\n      userTenantId,\n      attemptedTenantId,\n      resource,\n      timestamp: new Date(),\n    });\n\n    // Trigger security alert\n    this.alertingService.sendSecurityAlert({\n      severity: &#39;HIGH&#39;,\n      type: &#39;CROSS_TENANT_ACCESS_ATTEMPT&#39;,\n      details: { userId, resource },\n    });\n  }\n}\n</code></pre>\n<h3>3. Integration Tests</h3>\n<p>We wrote extensive tests to ensure tenant isolation:</p>\n<pre><code class=\"language-typescript\">describe(&#39;Tenant Isolation&#39;, () =&gt; {\n  it(&#39;should not allow access to another tenant projects&#39;, async () =&gt; {\n    const tenant1Token = await createAuthToken({ tenantId: &#39;tenant-1&#39; });\n    const tenant2Token = await createAuthToken({ tenantId: &#39;tenant-2&#39; });\n\n    // Create project as tenant 1\n    const project = await request(app)\n      .post(&#39;/projects&#39;)\n      .set(&#39;Authorization&#39;, `Bearer ${tenant1Token}`)\n      .send({ name: &#39;Tenant 1 Project&#39; });\n\n    // Try to access as tenant 2\n    const response = await request(app)\n      .get(`/projects/${project.body.id}`)\n      .set(&#39;Authorization&#39;, `Bearer ${tenant2Token}`);\n\n    expect(response.status).toBe(404); // Not found, not 403\n  });\n});\n</code></pre>\n<p>We return 404, not 403, to avoid leaking that the resource exists.</p>\n<h2>Managing Shared Resources</h2>\n<p>Some resources need to be shared across tenants (like system users, feature flags, or lookup tables).</p>\n<h3>Approach 1: No tenant_id Column</h3>\n<p>For truly global data:</p>\n<pre><code class=\"language-sql\">CREATE TABLE system_feature_flags (\n  id UUID PRIMARY KEY,\n  feature_name VARCHAR(100) NOT NULL,\n  enabled BOOLEAN DEFAULT false\n  -- No tenant_id\n);\n\n-- Disable RLS for system tables\nALTER TABLE system_feature_flags DISABLE ROW LEVEL SECURITY;\n</code></pre>\n<h3>Approach 2: NULL tenant_id for Shared Data</h3>\n<p>For data that can be shared or tenant-specific:</p>\n<pre><code class=\"language-sql\">CREATE TABLE industry_codes (\n  id UUID PRIMARY KEY,\n  tenant_id UUID, -- NULL for shared codes\n  code VARCHAR(50),\n  description TEXT\n);\n\nCREATE POLICY industry_codes_policy ON industry_codes\n  USING (tenant_id IS NULL OR tenant_id = current_setting(&#39;app.current_tenant_id&#39;)::uuid);\n</code></pre>\n<p>This lets tenants see both shared industry codes and their custom ones.</p>\n<h2>Handling Schema Migrations</h2>\n<p>With shared schemas, migrations affect all tenants simultaneously. We developed a safe migration process:</p>\n<h3>1. Backward-Compatible Changes Only</h3>\n<pre><code class=\"language-sql\">-- ✅ Safe: Adding nullable column\nALTER TABLE projects ADD COLUMN archived_at TIMESTAMP NULL;\n\n-- ❌ Unsafe: Adding non-nullable column\nALTER TABLE projects ADD COLUMN status VARCHAR(50) NOT NULL;\n\n-- ✅ Safe version:\nALTER TABLE projects ADD COLUMN status VARCHAR(50) NULL DEFAULT &#39;active&#39;;\n-- Then deploy code that sets status\n-- Then make column non-nullable in next migration\n</code></pre>\n<h3>2. Blue-Green Deployments</h3>\n<p>We run two application versions during migrations:</p>\n<ol>\n<li>Deploy new code (compatible with old schema)</li>\n<li>Run migration</li>\n<li>Shift traffic to new code</li>\n<li>Shutdown old code</li>\n</ol>\n<h3>3. Data Migrations in Batches</h3>\n<p>For large data migrations, we process by tenant:</p>\n<pre><code class=\"language-typescript\">async function migrateProjectStatuses() {\n  const tenants = await db.select().from(tenants);\n\n  for (const tenant of tenants) {\n    await db.transaction(async (tx) =&gt; {\n      await tx\n        .update(projects)\n        .set({ status: &#39;active&#39; })\n        .where(\n          and(\n            eq(projects.tenantId, tenant.id),\n            isNull(projects.status)\n          )\n        );\n    });\n\n    console.log(`Migrated tenant ${tenant.id}`);\n  }\n}\n</code></pre>\n<p>This prevents long-running transactions that lock all tenants.</p>\n<h2>Performance at Scale</h2>\n<p>With 300+ tenants and millions of rows, performance required careful optimization:</p>\n<h3>1. Tenant-Aware Indexing</h3>\n<p>Every query filters by <code>tenant_id</code>, so it&#39;s the first column in composite indexes:</p>\n<pre><code class=\"language-sql\">-- Good\nCREATE INDEX idx_projects_tenant_status ON projects(tenant_id, status);\n\n-- Bad\nCREATE INDEX idx_projects_status_tenant ON projects(status, tenant_id);\n</code></pre>\n<h3>2. Tenant-Based Sharding for Large Customers</h3>\n<p>Our largest customers got dedicated database instances:</p>\n<pre><code class=\"language-typescript\">export class TenantRouter {\n  getDatabaseConnection(tenantId: string): DatabaseConnection {\n    const tenant = this.getTenant(tenantId);\n\n    if (tenant.dedicatedDatabase) {\n      return this.getDedicatedConnection(tenant.databaseUrl);\n    }\n\n    return this.getSharedConnection();\n  }\n}\n</code></pre>\n<h3>3. Caching with Tenant Isolation</h3>\n<p>Cache keys include tenant ID:</p>\n<pre><code class=\"language-typescript\">const cacheKey = `tenant:${tenantId}:project:${projectId}`;\nconst cached = await this.redis.get(cacheKey);\n</code></pre>\n<p>This prevents cross-tenant cache pollution.</p>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>RLS is essential</strong>: It saved us from several bugs that could have been security incidents</li>\n<li><strong>Make tenant_id explicit</strong>: Include it in every query for performance and clarity</li>\n<li><strong>Test tenant isolation religiously</strong>: It&#39;s too important to trust</li>\n<li><strong>Start with shared schema</strong>: You can always shard later if needed</li>\n<li><strong>Monitor per-tenant metrics</strong>: Detect noisy neighbors and performance issues early</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Building multi-tenant SaaS is complex, but with the right architecture, it&#39;s secure and scalable. Row-Level Security, tenant-scoped repositories, explicit tenant validation, and comprehensive testing gave us confidence serving hundreds of enterprise clients from a single application instance.</p>\n<p>After 27 years of building applications, I can say that multi-tenancy done right is the most cost-effective way to deliver SaaS at scale. The key is paranoia about data isolation and defense in depth at every layer.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/building-multi-tenant-saas/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-05-17 00:00:00",
            "updated_at": "2024-05-17 00:00:00",
            "published_at": "2024-05-17 00:00:00",
            "custom_excerpt": "Building multi-tenant SaaS architecture for Catalyst PSA, serving hundreds of enterprise clients with complete data isolation. Three years of production lessons on security, performance, and scalability."
          },
          {
            "id": "37",
            "title": "Implementing Secure JWT Authentication in NestJS Applications",
            "slug": "jwt-authentication-best-practices",
            "html": "<p>Authentication is the foundation of any secure application. During my work on the EMA project at Servant, I implemented a comprehensive JWT-based authentication system with role-based access control for four distinct user types: admins, coordinators, advocates, and mothers.</p>\n<h2>Why JWT?</h2>\n<p>JSON Web Tokens provide a stateless authentication mechanism that scales well and works seamlessly across different services. Unlike session-based auth, JWTs don&#39;t require server-side session storage, making them ideal for modern distributed systems.</p>\n<h2>The Authentication Flow</h2>\n<p>Here&#39;s how we structured the authentication service in NestJS:</p>\n<pre><code class=\"language-typescript\">// auth.service.ts\nimport { Injectable, UnauthorizedException } from &#39;@nestjs/common&#39;;\nimport { JwtService } from &#39;@nestjs/jwt&#39;;\nimport { UsersService } from &#39;../users/users.service&#39;;\nimport * as bcrypt from &#39;bcrypt&#39;;\n\n@Injectable()\nexport class AuthService {\n  constructor(\n    private usersService: UsersService,\n    private jwtService: JwtService,\n  ) {}\n\n  async validateUser(email: string, password: string): Promise&lt;any&gt; {\n    const user = await this.usersService.findByEmail(email);\n\n    if (!user) {\n      throw new UnauthorizedException(&#39;Invalid credentials&#39;);\n    }\n\n    const isPasswordValid = await bcrypt.compare(\n      password,\n      user.passwordHash\n    );\n\n    if (!isPasswordValid) {\n      throw new UnauthorizedException(&#39;Invalid credentials&#39;);\n    }\n\n    const { passwordHash, ...result } = user;\n    return result;\n  }\n\n  async login(user: any) {\n    const payload = {\n      email: user.email,\n      sub: user.id,\n      role: user.role\n    };\n\n    return {\n      access_token: this.jwtService.sign(payload),\n      user: {\n        id: user.id,\n        email: user.email,\n        firstName: user.firstName,\n        lastName: user.lastName,\n        role: user.role\n      }\n    };\n  }\n\n  async refreshToken(userId: string) {\n    const user = await this.usersService.findById(userId);\n\n    if (!user) {\n      throw new UnauthorizedException(&#39;User not found&#39;);\n    }\n\n    return this.login(user);\n  }\n}\n</code></pre>\n<h2>Role-Based Access Control</h2>\n<p>The real power comes from implementing role-based guards:</p>\n<pre><code class=\"language-typescript\">// roles.guard.ts\nimport { Injectable, CanActivate, ExecutionContext } from &#39;@nestjs/common&#39;;\nimport { Reflector } from &#39;@nestjs/core&#39;;\nimport { UserRole } from &#39;../shared/enums/user-role.enum&#39;;\n\n@Injectable()\nexport class RolesGuard implements CanActivate {\n  constructor(private reflector: Reflector) {}\n\n  canActivate(context: ExecutionContext): boolean {\n    const requiredRoles = this.reflector.getAllAndOverride&lt;UserRole[]&gt;(\n      &#39;roles&#39;,\n      [context.getHandler(), context.getClass()]\n    );\n\n    if (!requiredRoles) {\n      return true;\n    }\n\n    const { user } = context.switchToHttp().getRequest();\n\n    return requiredRoles.some((role) =&gt; user.role === role);\n  }\n}\n\n// roles.decorator.ts\nimport { SetMetadata } from &#39;@nestjs/common&#39;;\nimport { UserRole } from &#39;../shared/enums/user-role.enum&#39;;\n\nexport const Roles = (...roles: UserRole[]) =&gt; SetMetadata(&#39;roles&#39;, roles);\n</code></pre>\n<p>Now we can protect routes based on user roles:</p>\n<pre><code class=\"language-typescript\">// referrals.controller.ts\n@Controller(&#39;referrals&#39;)\n@UseGuards(JwtAuthGuard, RolesGuard)\nexport class ReferralsController {\n  constructor(private readonly referralsService: ReferralsService) {}\n\n  @Post()\n  @Roles(UserRole.COORDINATOR, UserRole.ADVOCATE)\n  async createReferral(@Body() createReferralDto: CreateReferralDto) {\n    return this.referralsService.create(createReferralDto);\n  }\n\n  @Get()\n  @Roles(UserRole.ADMIN, UserRole.COORDINATOR)\n  async getAllReferrals() {\n    return this.referralsService.findAll();\n  }\n\n  @Get(&#39;my-referrals&#39;)\n  @Roles(UserRole.ADVOCATE)\n  async getMyReferrals(@Request() req) {\n    return this.referralsService.findByAdvocate(req.user.id);\n  }\n}\n</code></pre>\n<h2>JWT Strategy Implementation</h2>\n<p>The Passport strategy validates tokens on each request:</p>\n<pre><code class=\"language-typescript\">// jwt.strategy.ts\nimport { Injectable, UnauthorizedException } from &#39;@nestjs/common&#39;;\nimport { PassportStrategy } from &#39;@nestjs/passport&#39;;\nimport { ExtractJwt, Strategy } from &#39;passport-jwt&#39;;\nimport { ConfigService } from &#39;@nestjs/config&#39;;\n\n@Injectable()\nexport class JwtStrategy extends PassportStrategy(Strategy) {\n  constructor(\n    private configService: ConfigService,\n  ) {\n    super({\n      jwtFromRequest: ExtractJwt.fromAuthHeaderAsBearerToken(),\n      ignoreExpiration: false,\n      secretOrKey: configService.get&lt;string&gt;(&#39;JWT_SECRET&#39;),\n    });\n  }\n\n  async validate(payload: any) {\n    return {\n      id: payload.sub,\n      email: payload.email,\n      role: payload.role\n    };\n  }\n}\n</code></pre>\n<h2>Security Best Practices</h2>\n<p>Based on 27 years of experience, here are critical security considerations:</p>\n<ol>\n<li><strong>Never store passwords in plain text</strong>: Always use bcrypt or similar</li>\n</ol>\n<pre><code class=\"language-typescript\">const saltRounds = 10;\nconst hashedPassword = await bcrypt.hash(password, saltRounds);\n</code></pre>\n<ol start=\"2\">\n<li><strong>Use environment variables for secrets</strong>:</li>\n</ol>\n<pre><code class=\"language-typescript\">// .env\nJWT_SECRET=your-super-secret-key-change-this\nJWT_EXPIRATION=1h\n</code></pre>\n<ol start=\"3\">\n<li><p><strong>Implement token refresh mechanisms</strong>: Short-lived access tokens with refresh tokens</p>\n</li>\n<li><p><strong>Add rate limiting</strong>: Prevent brute force attacks</p>\n</li>\n</ol>\n<pre><code class=\"language-typescript\">import { ThrottlerModule } from &#39;@nestjs/throttler&#39;;\n\n@Module({\n  imports: [\n    ThrottlerModule.forRoot({\n      ttl: 60,\n      limit: 10,\n    }),\n  ],\n})\nexport class AppModule {}\n</code></pre>\n<ol start=\"5\">\n<li><strong>Validate all inputs</strong>: Use class-validator</li>\n</ol>\n<pre><code class=\"language-typescript\">import { IsEmail, IsNotEmpty, MinLength } from &#39;class-validator&#39;;\n\nexport class LoginDto {\n  @IsEmail()\n  @IsNotEmpty()\n  email: string;\n\n  @IsNotEmpty()\n  @MinLength(8)\n  password: string;\n}\n</code></pre>\n<h2>Frontend Integration</h2>\n<p>On the Next.js side, we used the token for authenticated API requests:</p>\n<pre><code class=\"language-typescript\">// lib/api-client.ts\nexport class ApiClient {\n  private baseUrl: string;\n  private token: string | null = null;\n\n  constructor(baseUrl: string) {\n    this.baseUrl = baseUrl;\n  }\n\n  setToken(token: string) {\n    this.token = token;\n  }\n\n  async request(endpoint: string, options: RequestInit = {}) {\n    const headers = {\n      &#39;Content-Type&#39;: &#39;application/json&#39;,\n      ...(this.token &amp;&amp; { Authorization: `Bearer ${this.token}` }),\n      ...options.headers,\n    };\n\n    const response = await fetch(`${this.baseUrl}${endpoint}`, {\n      ...options,\n      headers,\n    });\n\n    if (!response.ok) {\n      throw new Error(`API error: ${response.statusText}`);\n    }\n\n    return response.json();\n  }\n}\n</code></pre>\n<h2>Conclusion</h2>\n<p>Implementing secure authentication requires attention to detail and following best practices. JWT provides a solid foundation, but security is about the entire system - from password hashing to rate limiting to proper token management.</p>\n<p>The investment in a robust authentication system pays dividends in security, scalability, and peace of mind.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/jwt-authentication-best-practices/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-05-17 00:00:00",
            "updated_at": "2024-05-17 00:00:00",
            "published_at": "2024-05-17 00:00:00",
            "custom_excerpt": "Implementing secure JWT authentication with role-based access control for the EMA project at Servant, handling four distinct user types from admins to mothers with complex permissions."
          },
          {
            "id": "38",
            "title": "Modernizing Legacy PHP Applications: From Monolith to React",
            "slug": "modernizing-legacy-php-applications",
            "html": "<p>At Big D Companies, I faced a challenge familiar to many developers: a legacy PHP-based ERP system managing SCADA automation that desperately needed modernization. The system worked, but maintaining it was becoming increasingly difficult, and adding new features felt like archaeology.</p>\n<h2>The Legacy Situation</h2>\n<p>The existing system was a classic PHP monolith:</p>\n<ul>\n<li>Tight coupling between presentation and business logic</li>\n<li>Limited test coverage</li>\n<li>Performance issues from inefficient database queries</li>\n<li>Hard to onboard new developers</li>\n<li>MySQL database that needed to be preserved</li>\n</ul>\n<h2>The Migration Strategy</h2>\n<p>We couldn&#39;t do a &quot;big bang&quot; rewrite - the system was too critical. Instead, we took a gradual approach that allowed us to deliver value incrementally while maintaining business continuity.</p>\n<h3>Step 1: API Layer First</h3>\n<p>The first step was extracting the data layer behind a REST API. This allowed us to start building the new frontend while the old system continued running:</p>\n<pre><code class=\"language-javascript\">// api/routes/devices.js\nconst express = require(&#39;express&#39;);\nconst router = express.Router();\nconst mysql = require(&#39;mysql2/promise&#39;);\n\nconst pool = mysql.createPool({\n  host: process.env.DB_HOST,\n  user: process.env.DB_USER,\n  password: process.env.DB_PASSWORD,\n  database: process.env.DB_NAME,\n  waitForConnections: true,\n  connectionLimit: 10,\n});\n\nrouter.get(&#39;/devices&#39;, async (req, res) =&gt; {\n  try {\n    const [rows] = await pool.query(`\n      SELECT\n        d.id,\n        d.name,\n        d.type,\n        d.status,\n        d.last_reading,\n        s.name as site_name\n      FROM devices d\n      LEFT JOIN sites s ON d.site_id = s.id\n      WHERE d.active = 1\n      ORDER BY d.name\n    `);\n\n    res.json(rows);\n  } catch (error) {\n    console.error(&#39;Database error:&#39;, error);\n    res.status(500).json({ error: &#39;Internal server error&#39; });\n  }\n});\n\nrouter.get(&#39;/devices/:id/readings&#39;, async (req, res) =&gt; {\n  const { id } = req.params;\n  const { startDate, endDate } = req.query;\n\n  try {\n    const [rows] = await pool.query(`\n      SELECT\n        timestamp,\n        temperature,\n        pressure,\n        flow_rate\n      FROM device_readings\n      WHERE device_id = ?\n        AND timestamp BETWEEN ? AND ?\n      ORDER BY timestamp DESC\n    `, [id, startDate, endDate]);\n\n    res.json(rows);\n  } catch (error) {\n    console.error(&#39;Database error:&#39;, error);\n    res.status(500).json({ error: &#39;Internal server error&#39; });\n  }\n});\n\nmodule.exports = router;\n</code></pre>\n<h3>Step 2: Build the React Frontend</h3>\n<p>With the API in place, we built a modern React/Next.js frontend:</p>\n<pre><code class=\"language-typescript\">// components/DeviceMonitor.tsx\nimport { useState, useEffect } from &#39;react&#39;;\nimport { Card, CardHeader, CardContent } from &#39;@/components/ui/card&#39;;\nimport { Badge } from &#39;@/components/ui/badge&#39;;\n\ninterface Device {\n  id: number;\n  name: string;\n  type: string;\n  status: &#39;online&#39; | &#39;offline&#39; | &#39;warning&#39;;\n  last_reading: string;\n  site_name: string;\n}\n\nexport default function DeviceMonitor() {\n  const [devices, setDevices] = useState\n            &lt;/div&gt;\n          &lt;/CardHeader&gt;\n          \n        &lt;/Card&gt;\n      ))}\n    &lt;/div&gt;\n  );\n}\n</code></pre>\n<h3>Step 3: Progressive Enhancement</h3>\n<p>We didn&#39;t migrate everything at once. Instead, we used a routing strategy that gradually replaced PHP pages:</p>\n<pre><code class=\"language-javascript\">// server.js\nconst express = require(&#39;express&#39;);\nconst { createProxyMiddleware } = require(&#39;http-proxy-middleware&#39;);\nconst next = require(&#39;next&#39;);\n\nconst dev = process.env.NODE_ENV !== &#39;production&#39;;\nconst app = next({ dev });\nconst handle = app.getRequestHandler();\n\nconst PORT = process.env.PORT || 3000;\nconst PHP_SERVER = &#39;http://localhost:8080&#39;;\n\napp.prepare().then(() =&gt; {\n  const server = express();\n\n  // New React routes\n  server.use(&#39;/dashboard&#39;, (req, res) =&gt; handle(req, res));\n  server.use(&#39;/devices&#39;, (req, res) =&gt; handle(req, res));\n  server.use(&#39;/api&#39;, (req, res) =&gt; handle(req, res));\n\n  // Proxy everything else to PHP (legacy routes)\n  server.use(&#39;*&#39;, createProxyMiddleware({\n    target: PHP_SERVER,\n    changeOrigin: true,\n  }));\n\n  server.listen(PORT, () =&gt; {\n    console.log(`&gt; Ready on http://localhost:${PORT}`);\n  });\n});\n</code></pre>\n<h2>Testing Strategy</h2>\n<p>Testing legacy systems is challenging. We used Jest for new code and maintained the existing test suite:</p>\n<pre><code class=\"language-typescript\">// __tests__/DeviceMonitor.test.tsx\nimport { render, screen, waitFor } from &#39;@testing-library/react&#39;;\nimport DeviceMonitor from &#39;@/components/DeviceMonitor&#39;;\n\n// Mock fetch\nglobal.fetch = jest.fn();\n\ndescribe(&#39;DeviceMonitor&#39;, () =&gt; {\n  beforeEach(() =&gt; {\n    (fetch as jest.Mock).mockClear();\n  });\n\n  it(&#39;displays devices after loading&#39;, async () =&gt; {\n    (fetch as jest.Mock).mockResolvedValueOnce({\n      json: async () =&gt; ([\n        {\n          id: 1,\n          name: &#39;Pump 1&#39;,\n          type: &#39;Centrifugal&#39;,\n          status: &#39;online&#39;,\n          last_reading: &#39;2024-04-28T10:00:00Z&#39;,\n          site_name: &#39;Site A&#39;\n        }\n      ])\n    });\n\n    render();\n\n    await waitFor(() =&gt; {\n      expect(screen.getByText(&#39;Pump 1&#39;)).toBeInTheDocument();\n    });\n\n    expect(screen.getByText(&#39;online&#39;)).toBeInTheDocument();\n  });\n});\n</code></pre>\n<h2>Performance Improvements</h2>\n<p>The migration yielded significant performance improvements:</p>\n<ol>\n<li><strong>Database Query Optimization</strong>: Indexed queries reduced load times by 70%</li>\n<li><strong>Client-Side Rendering</strong>: React&#39;s virtual DOM made the UI much snappier</li>\n<li><strong>Code Splitting</strong>: Next.js automatically split code, reducing initial load</li>\n<li><strong>Caching</strong>: Implemented Redis caching for frequently accessed data</li>\n</ol>\n<h2>CI/CD Pipeline</h2>\n<p>We used CircleCI for automated testing and deployment:</p>\n<pre><code class=\"language-yaml\">version: 2.1\n\njobs:\n  test:\n    docker:\n      - image: circleci/node:16\n      - image: circleci/mysql:8.0\n        environment:\n          MYSQL_ROOT_PASSWORD: testpass\n          MYSQL_DATABASE: test_db\n    steps:\n      - checkout\n      - restore_cache:\n          keys:\n            - v1-dependencies-{{ checksum &quot;package.json&quot; }}\n      - run: npm install\n      - run: npm test\n      - save_cache:\n          paths:\n            - node_modules\n          key: v1-dependencies-{{ checksum &quot;package.json&quot; }}\n\n  deploy:\n    docker:\n      - image: circleci/node:16\n    steps:\n      - checkout\n      - run: npm install\n      - run: npm run build\n      - run: npm run deploy\n\nworkflows:\n  version: 2\n  test-and-deploy:\n    jobs:\n      - test\n      - deploy:\n          requires:\n            - test\n          filters:\n            branches:\n              only: main\n</code></pre>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Incremental is better than all-at-once</strong>: Gradual migration reduced risk</li>\n<li><strong>API-first enables flexibility</strong>: The API layer gave us freedom to iterate on the frontend</li>\n<li><strong>Keep the database</strong>: We preserved MySQL, which simplified migration significantly</li>\n<li><strong>Maintain the old system</strong>: We continued fixing critical bugs in PHP during migration</li>\n<li><strong>Performance matters</strong>: Users loved the speed improvements</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Modernizing legacy systems is challenging but incredibly rewarding. The key is finding a migration path that delivers value incrementally while maintaining business continuity.</p>\n<p>The PHP system at Big D Companies now has a modern React frontend, improved performance, and a maintainable codebase - all while keeping the business running smoothly throughout the transition.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/modernizing-legacy-php-applications/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-04-27 00:00:00",
            "updated_at": "2024-04-27 00:00:00",
            "published_at": "2024-04-27 00:00:00",
            "custom_excerpt": "A practical guide to migrating legacy PHP systems to modern React-based architectures while maintaining business continuity"
          },
          {
            "id": "39",
            "title": "CQRS Pattern Implementation in Enterprise Applications",
            "slug": "cqrs-pattern-enterprise-applications",
            "html": "<p>When the Catalyst PSA platform started experiencing performance bottlenecks with complex reporting queries slowing down transaction processing, we knew we needed a better approach. That&#39;s when we implemented Command Query Responsibility Segregation (CQRS), and it fundamentally changed how we architected our enterprise application.</p>\n<h2>The Problem: Reads and Writes Don&#39;t Scale the Same Way</h2>\n<p>Traditional CRUD applications use the same models and database structures for both reading and writing data. This works fine for simple apps, but enterprise applications have different characteristics:</p>\n<ul>\n<li><strong>Writes</strong> are typically transactional, need strong consistency, and involve complex business logic</li>\n<li><strong>Reads</strong> need to be fast, can tolerate eventual consistency, and often require data denormalized across multiple entities</li>\n</ul>\n<p>In Catalyst PSA, we had project managers running complex reports that joined 10+ tables, locking rows and slowing down time entry submissions from field workers. The same database schema that was perfect for maintaining data integrity was terrible for reporting performance.</p>\n<h2>What is CQRS?</h2>\n<p>CQRS separates your application into two distinct paths:</p>\n<ul>\n<li><strong>Command Side</strong>: Handles all state changes (Creates, Updates, Deletes)</li>\n<li><strong>Query Side</strong>: Handles all data retrieval (Reads)</li>\n</ul>\n<p>Each side can use different models, databases, and optimization strategies. The key insight is that the way you write data doesn&#39;t have to be the way you read it.</p>\n<h2>Our CQRS Implementation in Catalyst PSA</h2>\n<p>Here&#39;s how we implemented CQRS in our NestJS application:</p>\n<h3>Command Side: Write Models</h3>\n<p>Commands represent intentions to change state. They&#39;re explicit about what needs to happen:</p>\n<pre><code class=\"language-typescript\">// commands/create-project.command.ts\nexport class CreateProjectCommand {\n  constructor(\n    public readonly name: string,\n    public readonly clientId: string,\n    public readonly startDate: Date,\n    public readonly managerId: string,\n    public readonly budget: number\n  ) {}\n}\n\n// command-handlers/create-project.handler.ts\n@CommandHandler(CreateProjectCommand)\nexport class CreateProjectHandler implements ICommandHandler&lt;CreateProjectCommand&gt; {\n  constructor(\n    @Inject(&#39;ProjectRepositoryPort&#39;)\n    private readonly repository: ProjectRepositoryPort,\n    private readonly eventBus: EventBus\n  ) {}\n\n  async execute(command: CreateProjectCommand): Promise&lt;string&gt; {\n    // Validate business rules\n    const project = Project.create(\n      command.name,\n      command.clientId,\n      command.startDate,\n      command.managerId,\n      command.budget\n    );\n\n    await this.repository.save(project);\n\n    // Publish event for query side\n    this.eventBus.publish(\n      new ProjectCreatedEvent(\n        project.id,\n        project.name,\n        project.clientId,\n        command.managerId\n      )\n    );\n\n    return project.id;\n  }\n}\n</code></pre>\n<p>Notice how commands are fire-and-forget operations that return minimal data. They focus purely on executing the business logic and publishing events.</p>\n<h3>Query Side: Read Models</h3>\n<p>Queries are optimized for specific use cases. We created read models denormalized for fast retrieval:</p>\n<pre><code class=\"language-typescript\">// queries/get-project-dashboard.query.ts\nexport class GetProjectDashboardQuery {\n  constructor(public readonly projectId: string) {}\n}\n\n// query-handlers/get-project-dashboard.handler.ts\n@QueryHandler(GetProjectDashboardQuery)\nexport class GetProjectDashboardHandler implements IQueryHandler&lt;GetProjectDashboardQuery&gt; {\n  constructor(private readonly db: DrizzleService) {}\n\n  async execute(query: GetProjectDashboardQuery): Promise&lt;ProjectDashboardDTO&gt; {\n    // Direct database query with denormalized data\n    const result = await this.db\n      .select({\n        projectId: projectDashboards.id,\n        projectName: projectDashboards.name,\n        clientName: projectDashboards.clientName,\n        managerName: projectDashboards.managerName,\n        totalHours: projectDashboards.totalHours,\n        totalBilled: projectDashboards.totalBilled,\n        budgetRemaining: projectDashboards.budgetRemaining,\n        activeTaskCount: projectDashboards.activeTaskCount,\n        completedTaskCount: projectDashboards.completedTaskCount,\n        teamMembers: projectDashboards.teamMembers, // JSON array\n      })\n      .from(projectDashboards)\n      .where(eq(projectDashboards.id, query.projectId))\n      .limit(1);\n\n    return result[0];\n  }\n}\n</code></pre>\n<p>The query side reads from denormalized tables specifically designed for each view. No joins, no complex aggregations—just fast lookups.</p>\n<h3>Event Handlers: Keeping Read Models in Sync</h3>\n<p>The magic happens in event handlers that update read models when commands change state:</p>\n<pre><code class=\"language-typescript\">@EventsHandler(ProjectCreatedEvent)\nexport class ProjectCreatedReadModelUpdater implements IEventHandler&lt;ProjectCreatedEvent&gt; {\n  constructor(private readonly db: DrizzleService) {}\n\n  async handle(event: ProjectCreatedEvent): Promise&lt;void&gt; {\n    // Fetch additional data needed for read model\n    const client = await this.db\n      .select()\n      .from(clients)\n      .where(eq(clients.id, event.clientId))\n      .limit(1);\n\n    const manager = await this.db\n      .select()\n      .from(users)\n      .where(eq(users.id, event.managerId))\n      .limit(1);\n\n    // Insert denormalized record\n    await this.db.insert(projectDashboards).values({\n      id: event.projectId,\n      name: event.name,\n      clientId: event.clientId,\n      clientName: client[0].name,\n      managerId: event.managerId,\n      managerName: `${manager[0].firstName} ${manager[0].lastName}`,\n      totalHours: 0,\n      totalBilled: 0,\n      budgetRemaining: event.budget,\n      activeTaskCount: 0,\n      completedTaskCount: 0,\n      teamMembers: [],\n    });\n  }\n}\n</code></pre>\n<p>When time entries are added, tasks completed, or team members assigned, separate event handlers update the relevant parts of the read model.</p>\n<h2>Structuring the Codebase</h2>\n<p>We organized our CQRS code by feature:</p>\n<pre><code>src/projects/\n  commands/\n    create-project.command.ts\n    update-project.command.ts\n  command-handlers/\n    create-project.handler.ts\n    update-project.handler.ts\n  queries/\n    get-project-dashboard.query.ts\n    get-project-list.query.ts\n  query-handlers/\n    get-project-dashboard.handler.ts\n    get-project-list.handler.ts\n  events/\n    project-created.event.ts\n    project-updated.event.ts\n  event-handlers/\n    project-created-read-model-updater.ts\n    project-updated-read-model-updater.ts\n</code></pre>\n<p>This made it easy to see all the ways you could interact with projects.</p>\n<h2>The Results: Dramatic Performance Improvements</h2>\n<p>After implementing CQRS for our most-used features:</p>\n<ul>\n<li><strong>Dashboard load times</strong>: Reduced from 3-5 seconds to under 200ms</li>\n<li><strong>Report generation</strong>: 10x faster (from 30s to 3s for complex reports)</li>\n<li><strong>Write throughput</strong>: Unchanged—no performance penalty on the command side</li>\n<li><strong>Database contention</strong>: Eliminated—reads and writes hit different tables</li>\n</ul>\n<p>We also gained the ability to scale reads and writes independently. The query side could use read replicas, caching, even different databases like MongoDB for document-heavy reads.</p>\n<h2>Challenges and Lessons Learned</h2>\n<h3>Challenge 1: Eventual Consistency</h3>\n<p>CQRS means your read models are eventually consistent, not immediately consistent. For Catalyst PSA, this was mostly fine—users understood that dashboards might lag by a second or two. But for some operations (like &quot;create project and redirect to project page&quot;), we needed to ensure the read model was updated first.</p>\n<p>Our solution was to make critical event handlers synchronous:</p>\n<pre><code class=\"language-typescript\">@EventsHandler(ProjectCreatedEvent)\nexport class ProjectCreatedReadModelUpdater implements IEventHandler&lt;ProjectCreatedEvent&gt; {\n  async handle(event: ProjectCreatedEvent): Promise&lt;void&gt; {\n    await this.updateReadModel(event);\n    // Handler won&#39;t complete until read model is updated\n  }\n}\n</code></pre>\n<h3>Challenge 2: Data Duplication</h3>\n<p>Denormalized read models mean data duplication. When a client changes their name, we had to update it in multiple read models. This was manageable with good event handlers, but it required discipline.</p>\n<h3>Challenge 3: Choosing What to CQRS</h3>\n<p>We didn&#39;t use CQRS everywhere—that would have been overkill. We applied it to:</p>\n<ul>\n<li>Complex dashboards and reports</li>\n<li>High-traffic features (time entry lists)</li>\n<li>Features where read/write patterns differed significantly</li>\n</ul>\n<p>Simple CRUD operations stayed traditional. Not every feature needs CQRS.</p>\n<h3>Challenge 4: Testing Event Handlers</h3>\n<p>Event handlers became critical infrastructure. If they failed, read models would be stale. We wrote extensive integration tests:</p>\n<pre><code class=\"language-typescript\">describe(&#39;ProjectCreatedReadModelUpdater&#39;, () =&gt; {\n  it(&#39;should create denormalized dashboard record&#39;, async () =&gt; {\n    const event = new ProjectCreatedEvent(\n      &#39;proj-1&#39;,\n      &#39;Test Project&#39;,\n      &#39;client-1&#39;,\n      &#39;manager-1&#39;\n    );\n\n    await handler.handle(event);\n\n    const dashboard = await db\n      .select()\n      .from(projectDashboards)\n      .where(eq(projectDashboards.id, &#39;proj-1&#39;));\n\n    expect(dashboard[0]).toMatchObject({\n      name: &#39;Test Project&#39;,\n      clientName: &#39;Test Client&#39;,\n      managerName: &#39;John Doe&#39;,\n    });\n  });\n});\n</code></pre>\n<h2>When to Use CQRS</h2>\n<p>CQRS isn&#39;t for every application. Use it when:</p>\n<ul>\n<li>Read and write operations have different performance characteristics</li>\n<li>You need to scale reads and writes independently</li>\n<li>Your read models need different structures than your write models</li>\n<li>You&#39;re building complex reporting on top of transactional data</li>\n</ul>\n<p>Don&#39;t use it when:</p>\n<ul>\n<li>Your app is simple CRUD with no performance issues</li>\n<li>Read and write patterns are similar</li>\n<li>You&#39;re building an MVP and need to move fast</li>\n</ul>\n<h2>Beyond CQRS: Event Sourcing</h2>\n<p>We explored Event Sourcing (storing events as the source of truth, not current state) but decided against it for Catalyst PSA. Event Sourcing adds significant complexity and was overkill for our needs. CQRS alone gave us 90% of the benefits without the complexity.</p>\n<h2>Conclusion</h2>\n<p>Implementing CQRS in the Catalyst PSA platform was one of our best architectural decisions. It solved real performance problems, made our codebase more maintainable by making read and write operations explicit, and gave us the flexibility to optimize each side independently.</p>\n<p>The key is not to use CQRS everywhere, but to apply it strategically where read/write patterns diverge. Combined with Hexagonal Architecture and DDD, CQRS helped us build a platform that scaled while staying maintainable and performant.</p>\n<p>After 27 years of building applications, I can confidently say that CQRS is an essential pattern for any enterprise application that needs to handle both complex transactions and high-performance reads.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/cqrs-pattern-enterprise-applications/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-04-21 00:00:00",
            "updated_at": "2024-04-21 00:00:00",
            "published_at": "2024-04-21 00:00:00",
            "custom_excerpt": "Implementing CQRS in Catalyst PSA to solve performance bottlenecks when complex reports locked tables and slowed transactions. Real-world patterns for separating reads and writes at enterprise scale."
          },
          {
            "id": "40",
            "title": "Building Offline-First ERP Systems for the Oil & Gas Industry",
            "slug": "building-offline-first-erp-oil-gas",
            "html": "<p>Working in the oil and gas industry presents unique challenges that most web developers never encounter. During my time at Key Energy Services, I led a team building a large-scale ERP solution where one requirement stood out: the app had to work when there was no internet connection.</p>\n<h2>The Connectivity Challenge</h2>\n<p>Oil fields aren&#39;t known for their reliable internet. Workers need access to critical systems whether they&#39;re in a remote well site with spotty cellular coverage or in an office with full broadband. This reality forced us to fundamentally rethink how we approached data synchronization and application architecture.</p>\n<h2>Architecture Decisions</h2>\n<p>We built the system using Ruby on Rails for the backend API, Meteor for real-time capabilities, and Backbone.js for the frontend. The key was implementing a robust offline-first architecture:</p>\n<pre><code class=\"language-javascript\">// Simplified example of our sync strategy\nclass DataSyncManager {\n  constructor() {\n    this.localDB = new LocalDatabase();\n    this.syncQueue = [];\n    this.isOnline = navigator.onLine;\n  }\n\n  async saveData(entity, data) {\n    // Always save locally first\n    await this.localDB.save(entity, data);\n\n    if (this.isOnline) {\n      try {\n        await this.syncToServer(entity, data);\n      } catch (error) {\n        this.queueForSync(entity, data);\n      }\n    } else {\n      this.queueForSync(entity, data);\n    }\n  }\n\n  async syncToServer(entity, data) {\n    const response = await fetch(`/api/${entity}`, {\n      method: &#39;POST&#39;,\n      body: JSON.stringify(data)\n    });\n\n    if (!response.ok) throw new Error(&#39;Sync failed&#39;);\n  }\n}\n</code></pre>\n<h2>Conflict Resolution</h2>\n<p>The hardest part wasn&#39;t storing data locally. It was handling conflicts when multiple users edited the same record offline. We implemented a last-write-wins strategy with timestamp-based conflict detection, but also built manual conflict resolution tools for critical data.</p>\n<h2>Testing Offline Scenarios</h2>\n<p>Testing offline functionality required discipline. We used Docker to create network conditions that simulated various connectivity scenarios - from complete offline to slow 2G connections. This helped us catch edge cases before they reached production.</p>\n<pre><code class=\"language-bash\"># Example Docker network throttling\ndocker run --network=slownet \\\n  --cap-add=NET_ADMIN \\\n  my-erp-app\n\n# In another container, throttle the network\ntc qdisc add dev eth0 root netem delay 2000ms 200ms\n</code></pre>\n<h2>Key Takeaways</h2>\n<p>Building offline-first applications taught me several valuable lessons:</p>\n<ol>\n<li><p><strong>Local-first is resilient-first</strong>: By treating the local database as the source of truth and syncing as a background operation, users never felt blocked by connectivity issues.</p>\n</li>\n<li><p><strong>Progressive enhancement matters</strong>: Critical features worked offline, while nice-to-haves required connectivity. This prioritization was essential.</p>\n</li>\n<li><p><strong>Sync is hard</strong>: Implementing reliable bidirectional sync is more complex than most developers anticipate. Budget time accordingly.</p>\n</li>\n<li><p><strong>User feedback is critical</strong>: Clear indicators of online/offline state and pending syncs helped users trust the system.</p>\n</li>\n</ol>\n<p>The principles I learned building ERP systems for oil and gas have served me well in other domains. Any application dealing with unreliable connectivity - whether it&#39;s field service, healthcare, or mobile-first apps - benefits from an offline-first approach.</p>\n<p>In an era where we assume constant connectivity, building for the offline case makes applications more robust for everyone.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/building-offline-first-erp-oil-gas/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-03-14 00:00:00",
            "updated_at": "2024-03-14 00:00:00",
            "published_at": "2024-03-14 00:00:00",
            "custom_excerpt": "Lessons learned from developing large-scale ERP solutions for oil and gas companies where connectivity is unreliable"
          },
          {
            "id": "41",
            "title": "Hexagonal Architecture in Practice: Building the Catalyst PSA Platform",
            "slug": "hexagonal-architecture-practical-guide",
            "html": "<p>After 27 years of building software, I&#39;ve seen architectural patterns come and go. But Hexagonal Architecture—also known as Ports and Adapters—has proven to be one of the most effective patterns for building maintainable enterprise applications. When we built the Catalyst PSA platform, Hexagonal Architecture was instrumental in keeping our codebase manageable and testable.</p>\n<h2>What is Hexagonal Architecture?</h2>\n<p>Hexagonal Architecture, introduced by Alistair Cockburn, is about creating a clear separation between your business logic and external concerns like databases, APIs, and user interfaces. The core idea is simple: your business logic shouldn&#39;t know or care about where data comes from or where it goes.</p>\n<p>The architecture consists of three main layers:</p>\n<ul>\n<li><strong>Domain Layer</strong>: Your pure business logic</li>\n<li><strong>Application Layer</strong>: Use cases and orchestration</li>\n<li><strong>Infrastructure Layer</strong>: External dependencies (databases, APIs, message queues)</li>\n</ul>\n<h2>Why We Chose Hexagonal Architecture for Catalyst PSA</h2>\n<p>When you&#39;re building a platform that needs to scale to hundreds of thousands of lines of code and serve enterprise clients, maintainability becomes paramount. Here&#39;s why Hexagonal Architecture made sense:</p>\n<h3>1. Technology Independence</h3>\n<p>In enterprise SaaS, requirements change. Today you&#39;re using PostgreSQL, tomorrow the client needs MongoDB support. With Hexagonal Architecture, we could swap out our persistence layer without touching business logic. We actually did this when migrating from Prisma to Drizzle—the domain layer didn&#39;t need a single change.</p>\n<h3>2. Testability</h3>\n<p>Testing becomes trivial when your business logic doesn&#39;t depend on external services. We could write comprehensive unit tests for our domain layer without spinning up databases or mocking complex dependencies. Our test coverage exceeded 85% because testing was actually enjoyable.</p>\n<h3>3. Clear Boundaries</h3>\n<p>In large teams, clear boundaries prevent chaos. Developers knew exactly where code belonged. If it&#39;s business logic, it goes in the domain. If it&#39;s about how we store data, it&#39;s infrastructure.</p>\n<h2>Implementing Hexagonal Architecture with NestJS</h2>\n<p>Let&#39;s look at how we structured this in practice using NestJS. Here&#39;s a real example from our project management module:</p>\n<h3>The Domain Layer</h3>\n<pre><code class=\"language-typescript\">// domain/entities/project.entity.ts\nexport class Project {\n  constructor(\n    private readonly id: string,\n    private name: string,\n    private status: ProjectStatus,\n    private clientId: string,\n    private startDate: Date,\n    private endDate?: Date\n  ) {}\n\n  changeName(newName: string): void {\n    if (!newName || newName.trim().length === 0) {\n      throw new Error(&#39;Project name cannot be empty&#39;);\n    }\n    this.name = newName;\n  }\n\n  complete(completionDate: Date): void {\n    if (completionDate &lt; this.startDate) {\n      throw new Error(&#39;Completion date cannot be before start date&#39;);\n    }\n    this.status = ProjectStatus.COMPLETED;\n    this.endDate = completionDate;\n  }\n\n  // Pure business logic - no dependencies\n}\n</code></pre>\n<p>Notice how the domain entity has zero dependencies. It&#39;s pure TypeScript with business rules.</p>\n<h3>The Port (Interface)</h3>\n<pre><code class=\"language-typescript\">// domain/ports/project.repository.port.ts\nexport interface ProjectRepositoryPort {\n  findById(id: string): Promise&lt;Project | null&gt;;\n  findByClientId(clientId: string): Promise&lt;Project[]&gt;;\n  save(project: Project): Promise&lt;void&gt;;\n  delete(id: string): Promise&lt;void&gt;;\n}\n</code></pre>\n<p>This is the contract that infrastructure must fulfill. The domain defines what it needs, not how it&#39;s implemented.</p>\n<h3>The Adapter (Implementation)</h3>\n<pre><code class=\"language-typescript\">// infrastructure/persistence/project.repository.ts\n@Injectable()\nexport class ProjectRepository implements ProjectRepositoryPort {\n  constructor(private readonly db: DrizzleService) {}\n\n  async findById(id: string): Promise&lt;Project | null&gt; {\n    const result = await this.db\n      .select()\n      .from(projects)\n      .where(eq(projects.id, id))\n      .limit(1);\n\n    if (!result.length) return null;\n    return this.toDomain(result[0]);\n  }\n\n  async save(project: Project): Promise&lt;void&gt; {\n    const data = this.toPersistence(project);\n    await this.db\n      .insert(projects)\n      .values(data)\n      .onConflictDoUpdate({\n        target: projects.id,\n        set: data,\n      });\n  }\n\n  private toDomain(record: any): Project {\n    // Map database record to domain entity\n  }\n\n  private toPersistence(project: Project): any {\n    // Map domain entity to database record\n  }\n}\n</code></pre>\n<p>The adapter handles all the messy details of data persistence. The domain remains blissfully ignorant.</p>\n<h3>The Application Layer</h3>\n<pre><code class=\"language-typescript\">// application/use-cases/complete-project.use-case.ts\n@Injectable()\nexport class CompleteProjectUseCase {\n  constructor(\n    @Inject(&#39;ProjectRepositoryPort&#39;)\n    private readonly projectRepository: ProjectRepositoryPort,\n    private readonly eventBus: EventBus\n  ) {}\n\n  async execute(projectId: string, completionDate: Date): Promise&lt;void&gt; {\n    const project = await this.projectRepository.findById(projectId);\n\n    if (!project) {\n      throw new NotFoundException(&#39;Project not found&#39;);\n    }\n\n    project.complete(completionDate);\n\n    await this.projectRepository.save(project);\n\n    this.eventBus.publish(new ProjectCompletedEvent(projectId));\n  }\n}\n</code></pre>\n<p>Use cases orchestrate the workflow but delegate business rules to the domain.</p>\n<h2>Lessons Learned from 320K+ Lines</h2>\n<h3>Lesson 1: Don&#39;t Overthink the Domain Model</h3>\n<p>Early on, we tried to make our domain entities too smart, adding complex validation and business logic everywhere. This made them hard to construct and test. We learned to keep entities focused on their core invariants and move complex workflows to domain services.</p>\n<h3>Lesson 2: Mapping is Tedious but Essential</h3>\n<p>The mapping between domain entities and persistence models feels like boilerplate. We were tempted to skip it and use database models directly in our domain. Don&#39;t do this. The separation is worth its weight in gold when you need to change your database schema without breaking business logic.</p>\n<h3>Lesson 3: Feature Folders Work Better Than Layer Folders</h3>\n<p>Instead of organizing code by layer (all entities in one folder, all repositories in another), we organized by feature:</p>\n<pre><code>src/\n  projects/\n    domain/\n    application/\n    infrastructure/\n  time-tracking/\n    domain/\n    application/\n    infrastructure/\n</code></pre>\n<p>This made it easier to find related code and understand feature boundaries.</p>\n<h3>Lesson 4: Use Dependency Injection Wisely</h3>\n<p>NestJS&#39;s dependency injection made it easy to wire up ports and adapters. We registered repositories as providers:</p>\n<pre><code class=\"language-typescript\">const providers = [\n  {\n    provide: &#39;ProjectRepositoryPort&#39;,\n    useClass: ProjectRepository,\n  },\n];\n</code></pre>\n<p>This made testing a breeze—just swap the implementation in tests.</p>\n<h2>The Results</h2>\n<p>After three years and a large codebase, Hexagonal Architecture proved its worth:</p>\n<ul>\n<li><strong>Zero regressions</strong> when we migrated from Prisma to Drizzle</li>\n<li><strong>85%+ test coverage</strong> maintained as the codebase grew</li>\n<li><strong>Faster onboarding</strong> for new developers who could understand one layer at a time</li>\n<li><strong>Flexible deployment</strong> options—we could run the same core logic in serverless functions or long-running processes</li>\n</ul>\n<h2>When Not to Use Hexagonal Architecture</h2>\n<p>Hexagonal Architecture isn&#39;t free. It requires discipline and adds some complexity. For small projects or MVPs, it might be overkill. We used it for Catalyst PSA because we knew it would grow large and need to evolve over years.</p>\n<p>If you&#39;re building a simple CRUD app that won&#39;t change much, stick with a simpler architecture. But if you&#39;re building enterprise software that needs to stand the test of time, Hexagonal Architecture is worth the investment.</p>\n<h2>Conclusion</h2>\n<p>Hexagonal Architecture transformed how we built the Catalyst PSA platform. It gave us the flexibility to evolve our technology choices, the confidence to refactor without fear, and the clarity to work as a large team on a massive codebase.</p>\n<p>The key is starting with clean boundaries and maintaining the discipline not to let infrastructure concerns leak into your domain. It&#39;s not always easy, but after 27 years of writing code, I can tell you it&#39;s one of the best architectural decisions you can make for enterprise applications.</p>\n",
            "feature_image": "https://jasoncochran.io/blog/hexagonal-architecture-practical-guide/ai-generated.png",
            "type": "post",
            "status": "published",
            "visibility": "public",
            "created_at": "2024-03-14 00:00:00",
            "updated_at": "2024-03-14 00:00:00",
            "published_at": "2024-03-14 00:00:00",
            "custom_excerpt": "Why Hexagonal Architecture was essential for maintaining Catalyst PSA's lare codebase. Real-world implementation patterns from building enterprise SaaS with NestJS and complete technology independence."
          }
        ],
        "posts_meta": [
          {
            "post_id": "1",
            "feature_image_alt": "Claude Opus 4.5: Anthropic Reclaims the Coding Crown with 80.9% SWE-bench Score",
            "feature_image_caption": null
          },
          {
            "post_id": "2",
            "feature_image_alt": "The BMAD Method: Transforming AI-Assisted Development with Structured Workflows",
            "feature_image_caption": null
          },
          {
            "post_id": "3",
            "feature_image_alt": "Google Gemini 3: Achieving Benchmark Dominance Across AI Leaderboards",
            "feature_image_caption": null
          },
          {
            "post_id": "4",
            "feature_image_alt": "OpenSpec: Aligning AI and Developers Through Spec-Driven Development",
            "feature_image_caption": null
          },
          {
            "post_id": "5",
            "feature_image_alt": "How AI-Assisted Development Helps Me Ship Faster Without Compromising Quality",
            "feature_image_caption": null
          },
          {
            "post_id": "6",
            "feature_image_alt": "My 2025 Development Stack: Built for Speed, Designed for Scale",
            "feature_image_caption": null
          },
          {
            "post_id": "7",
            "feature_image_alt": "Why Permian Basin Operators Need Modern SCADA Systems",
            "feature_image_caption": null
          },
          {
            "post_id": "8",
            "feature_image_alt": "Getting Started with Next.js 14",
            "feature_image_caption": null
          },
          {
            "post_id": "9",
            "feature_image_alt": "TypeScript Best Practices for 2025",
            "feature_image_caption": null
          },
          {
            "post_id": "10",
            "feature_image_alt": "Building Scalable React Applications",
            "feature_image_caption": null
          },
          {
            "post_id": "11",
            "feature_image_alt": "Mobile-First Design for Field Operations",
            "feature_image_caption": null
          },
          {
            "post_id": "12",
            "feature_image_alt": "Database Query Optimization for Large Datasets",
            "feature_image_caption": null
          },
          {
            "post_id": "13",
            "feature_image_alt": "Building Enterprise Forms: Complex Validation and UX",
            "feature_image_caption": null
          },
          {
            "post_id": "14",
            "feature_image_alt": "Monorepo Management for Multi-App Projects",
            "feature_image_caption": null
          },
          {
            "post_id": "15",
            "feature_image_alt": "React Native Performance Optimization Techniques",
            "feature_image_caption": null
          },
          {
            "post_id": "16",
            "feature_image_alt": "27 years of Web Development: Key Lessons Learned",
            "feature_image_caption": null
          },
          {
            "post_id": "17",
            "feature_image_alt": "NFT Platform Development: Ethereum Integration Best Practices",
            "feature_image_caption": null
          },
          {
            "post_id": "18",
            "feature_image_alt": "Role-Based Access Control in Modern Web Apps",
            "feature_image_caption": null
          },
          {
            "post_id": "19",
            "feature_image_alt": "GraphQL vs REST: When to Use Each",
            "feature_image_caption": null
          },
          {
            "post_id": "20",
            "feature_image_alt": "VoIP Integration: Building Telecom Solutions with Node.js",
            "feature_image_caption": null
          },
          {
            "post_id": "21",
            "feature_image_alt": "Building Scalable APIs with NestJS",
            "feature_image_caption": null
          },
          {
            "post_id": "22",
            "feature_image_alt": "Building Blockchain-Integrated Applications with Node.js",
            "feature_image_caption": null
          },
          {
            "post_id": "23",
            "feature_image_alt": "From Legacy to Modern: Migration Strategies That Work",
            "feature_image_caption": null
          },
          {
            "post_id": "24",
            "feature_image_alt": "Building Real-Time Features with WebSockets and Redis",
            "feature_image_caption": null
          },
          {
            "post_id": "25",
            "feature_image_alt": "Testing Strategies for Enterprise Applications: Jest, Enzyme, and Beyond",
            "feature_image_caption": null
          },
          {
            "post_id": "26",
            "feature_image_alt": "CI/CD Deployment Strategies: From CircleCI to Railway",
            "feature_image_caption": null
          },
          {
            "post_id": "27",
            "feature_image_alt": "Prisma vs Drizzle: A Practical Comparison from Production Experience",
            "feature_image_caption": null
          },
          {
            "post_id": "28",
            "feature_image_alt": "Migrating from Prisma to Drizzle: A Practical Guide",
            "feature_image_caption": null
          },
          {
            "post_id": "29",
            "feature_image_alt": "Advanced TypeScript Patterns for Enterprise Applications",
            "feature_image_caption": null
          },
          {
            "post_id": "30",
            "feature_image_alt": "Offline-First Mobile Apps: Architecture and Sync Strategies",
            "feature_image_caption": null
          },
          {
            "post_id": "31",
            "feature_image_alt": "React Native in the Real World: Lessons from Cross-Platform Development",
            "feature_image_caption": null
          },
          {
            "post_id": "32",
            "feature_image_alt": "Database-Agnostic Architecture: Why and How",
            "feature_image_caption": null
          },
          {
            "post_id": "33",
            "feature_image_alt": "AWS Integration Patterns: SES, SNS, and S3 in Production Applications",
            "feature_image_caption": null
          },
          {
            "post_id": "34",
            "feature_image_alt": "Building Full-Stack Applications with NestJS and Next.js: A Perfect Match",
            "feature_image_caption": null
          },
          {
            "post_id": "35",
            "feature_image_alt": "Domain-Driven Design: Lessons from a 320K+ LOC Codebase",
            "feature_image_caption": null
          },
          {
            "post_id": "36",
            "feature_image_alt": "Building Multi-Tenant SaaS: Security and Data Isolation",
            "feature_image_caption": null
          },
          {
            "post_id": "37",
            "feature_image_alt": "Implementing Secure JWT Authentication in NestJS Applications",
            "feature_image_caption": null
          },
          {
            "post_id": "38",
            "feature_image_alt": "Modernizing Legacy PHP Applications: From Monolith to React",
            "feature_image_caption": null
          },
          {
            "post_id": "39",
            "feature_image_alt": "CQRS Pattern Implementation in Enterprise Applications",
            "feature_image_caption": null
          },
          {
            "post_id": "40",
            "feature_image_alt": "Building Offline-First ERP Systems for the Oil & Gas Industry",
            "feature_image_caption": null
          },
          {
            "post_id": "41",
            "feature_image_alt": "Hexagonal Architecture in Practice: Building the Catalyst PSA Platform",
            "feature_image_caption": null
          }
        ],
        "tags": [
          {
            "id": "1",
            "slug": "ai",
            "name": "Ai"
          },
          {
            "id": "2",
            "slug": "anthropic",
            "name": "Anthropic"
          },
          {
            "id": "3",
            "slug": "claude",
            "name": "Claude"
          },
          {
            "id": "4",
            "slug": "coding",
            "name": "Coding"
          },
          {
            "id": "5",
            "slug": "benchmarks",
            "name": "Benchmarks"
          },
          {
            "id": "6",
            "slug": "development-workflow",
            "name": "Development workflow"
          },
          {
            "id": "7",
            "slug": "productivity",
            "name": "Productivity"
          },
          {
            "id": "8",
            "slug": "architecture",
            "name": "Architecture"
          },
          {
            "id": "9",
            "slug": "agile",
            "name": "Agile"
          },
          {
            "id": "10",
            "slug": "google",
            "name": "Google"
          },
          {
            "id": "11",
            "slug": "gemini",
            "name": "Gemini"
          },
          {
            "id": "12",
            "slug": "agi",
            "name": "Agi"
          },
          {
            "id": "13",
            "slug": "tools",
            "name": "Tools"
          },
          {
            "id": "14",
            "slug": "nextjs",
            "name": "Nextjs"
          },
          {
            "id": "15",
            "slug": "nestjs",
            "name": "Nestjs"
          },
          {
            "id": "16",
            "slug": "typescript",
            "name": "Typescript"
          },
          {
            "id": "17",
            "slug": "oil-gas",
            "name": "Oil gas"
          },
          {
            "id": "18",
            "slug": "scada",
            "name": "Scada"
          },
          {
            "id": "19",
            "slug": "permian-basin",
            "name": "Permian basin"
          },
          {
            "id": "20",
            "slug": "field-operations",
            "name": "Field operations"
          },
          {
            "id": "21",
            "slug": "rust",
            "name": "Rust"
          },
          {
            "id": "22",
            "slug": "next.js",
            "name": "Next.js"
          },
          {
            "id": "23",
            "slug": "react",
            "name": "React"
          },
          {
            "id": "24",
            "slug": "typescript",
            "name": "TypeScript"
          },
          {
            "id": "25",
            "slug": "web-development",
            "name": "Web Development"
          },
          {
            "id": "26",
            "slug": "javascript",
            "name": "JavaScript"
          },
          {
            "id": "27",
            "slug": "best-practices",
            "name": "Best Practices"
          },
          {
            "id": "28",
            "slug": "architecture",
            "name": "Architecture"
          },
          {
            "id": "29",
            "slug": "scalability",
            "name": "Scalability"
          },
          {
            "id": "30",
            "slug": "mobile",
            "name": "Mobile"
          },
          {
            "id": "31",
            "slug": "ux",
            "name": "Ux"
          },
          {
            "id": "32",
            "slug": "react-native",
            "name": "React native"
          },
          {
            "id": "33",
            "slug": "design",
            "name": "Design"
          },
          {
            "id": "34",
            "slug": "offline",
            "name": "Offline"
          },
          {
            "id": "35",
            "slug": "database",
            "name": "Database"
          },
          {
            "id": "36",
            "slug": "performance",
            "name": "Performance"
          },
          {
            "id": "37",
            "slug": "postgresql",
            "name": "Postgresql"
          },
          {
            "id": "38",
            "slug": "sql",
            "name": "Sql"
          },
          {
            "id": "39",
            "slug": "optimization",
            "name": "Optimization"
          },
          {
            "id": "40",
            "slug": "indexing",
            "name": "Indexing"
          },
          {
            "id": "41",
            "slug": "forms",
            "name": "Forms"
          },
          {
            "id": "42",
            "slug": "validation",
            "name": "Validation"
          },
          {
            "id": "43",
            "slug": "react",
            "name": "React"
          },
          {
            "id": "44",
            "slug": "react-hook-form",
            "name": "React hook form"
          },
          {
            "id": "45",
            "slug": "zod",
            "name": "Zod"
          },
          {
            "id": "46",
            "slug": "monorepo",
            "name": "Monorepo"
          },
          {
            "id": "47",
            "slug": "turborepo",
            "name": "Turborepo"
          },
          {
            "id": "48",
            "slug": "tooling",
            "name": "Tooling"
          },
          {
            "id": "49",
            "slug": "dx",
            "name": "Dx"
          },
          {
            "id": "50",
            "slug": "profiling",
            "name": "Profiling"
          },
          {
            "id": "51",
            "slug": "career",
            "name": "Career"
          },
          {
            "id": "52",
            "slug": "lessons-learned",
            "name": "Lessons learned"
          },
          {
            "id": "53",
            "slug": "software-engineering",
            "name": "Software engineering"
          },
          {
            "id": "54",
            "slug": "leadership",
            "name": "Leadership"
          },
          {
            "id": "55",
            "slug": "nft",
            "name": "Nft"
          },
          {
            "id": "56",
            "slug": "ethereum",
            "name": "Ethereum"
          },
          {
            "id": "57",
            "slug": "blockchain",
            "name": "Blockchain"
          },
          {
            "id": "58",
            "slug": "web3",
            "name": "Web3"
          },
          {
            "id": "59",
            "slug": "smart-contracts",
            "name": "Smart contracts"
          },
          {
            "id": "60",
            "slug": "ipfs",
            "name": "Ipfs"
          },
          {
            "id": "61",
            "slug": "rbac",
            "name": "Rbac"
          },
          {
            "id": "62",
            "slug": "security",
            "name": "Security"
          },
          {
            "id": "63",
            "slug": "authorization",
            "name": "Authorization"
          },
          {
            "id": "64",
            "slug": "permissions",
            "name": "Permissions"
          },
          {
            "id": "65",
            "slug": "access-control",
            "name": "Access control"
          },
          {
            "id": "66",
            "slug": "graphql",
            "name": "Graphql"
          },
          {
            "id": "67",
            "slug": "rest",
            "name": "Rest"
          },
          {
            "id": "68",
            "slug": "api-design",
            "name": "Api design"
          },
          {
            "id": "69",
            "slug": "voip",
            "name": "Voip"
          },
          {
            "id": "70",
            "slug": "nodejs",
            "name": "Nodejs"
          },
          {
            "id": "71",
            "slug": "sip",
            "name": "Sip"
          },
          {
            "id": "72",
            "slug": "telecom",
            "name": "Telecom"
          },
          {
            "id": "73",
            "slug": "asterisk",
            "name": "Asterisk"
          },
          {
            "id": "74",
            "slug": "telephony",
            "name": "Telephony"
          },
          {
            "id": "75",
            "slug": "api",
            "name": "Api"
          },
          {
            "id": "76",
            "slug": "scalability",
            "name": "Scalability"
          },
          {
            "id": "77",
            "slug": "migration",
            "name": "Migration"
          },
          {
            "id": "78",
            "slug": "legacy-code",
            "name": "Legacy code"
          },
          {
            "id": "79",
            "slug": "modernization",
            "name": "Modernization"
          },
          {
            "id": "80",
            "slug": "refactoring",
            "name": "Refactoring"
          },
          {
            "id": "81",
            "slug": "websockets",
            "name": "Websockets"
          },
          {
            "id": "82",
            "slug": "redis",
            "name": "Redis"
          },
          {
            "id": "83",
            "slug": "realtime",
            "name": "Realtime"
          },
          {
            "id": "84",
            "slug": "pub-sub",
            "name": "Pub sub"
          },
          {
            "id": "85",
            "slug": "testing",
            "name": "Testing"
          },
          {
            "id": "86",
            "slug": "jest",
            "name": "Jest"
          },
          {
            "id": "87",
            "slug": "quality",
            "name": "Quality"
          },
          {
            "id": "88",
            "slug": "best-practices",
            "name": "Best practices"
          },
          {
            "id": "89",
            "slug": "cicd",
            "name": "Cicd"
          },
          {
            "id": "90",
            "slug": "devops",
            "name": "Devops"
          },
          {
            "id": "91",
            "slug": "deployment",
            "name": "Deployment"
          },
          {
            "id": "92",
            "slug": "automation",
            "name": "Automation"
          },
          {
            "id": "93",
            "slug": "prisma",
            "name": "Prisma"
          },
          {
            "id": "94",
            "slug": "drizzle",
            "name": "Drizzle"
          },
          {
            "id": "95",
            "slug": "orm",
            "name": "Orm"
          },
          {
            "id": "96",
            "slug": "patterns",
            "name": "Patterns"
          },
          {
            "id": "97",
            "slug": "type-safety",
            "name": "Type safety"
          },
          {
            "id": "98",
            "slug": "advanced",
            "name": "Advanced"
          },
          {
            "id": "99",
            "slug": "offline-first",
            "name": "Offline first"
          },
          {
            "id": "100",
            "slug": "sqlite",
            "name": "Sqlite"
          },
          {
            "id": "101",
            "slug": "sync",
            "name": "Sync"
          },
          {
            "id": "102",
            "slug": "cross-platform",
            "name": "Cross platform"
          },
          {
            "id": "103",
            "slug": "mongodb",
            "name": "Mongodb"
          },
          {
            "id": "104",
            "slug": "aws",
            "name": "Aws"
          },
          {
            "id": "105",
            "slug": "cloud",
            "name": "Cloud"
          },
          {
            "id": "106",
            "slug": "ddd",
            "name": "Ddd"
          },
          {
            "id": "107",
            "slug": "domain-driven-design",
            "name": "Domain driven design"
          },
          {
            "id": "108",
            "slug": "enterprise",
            "name": "Enterprise"
          },
          {
            "id": "109",
            "slug": "software-design",
            "name": "Software design"
          },
          {
            "id": "110",
            "slug": "saas",
            "name": "Saas"
          },
          {
            "id": "111",
            "slug": "multi-tenancy",
            "name": "Multi tenancy"
          },
          {
            "id": "112",
            "slug": "authentication",
            "name": "Authentication"
          },
          {
            "id": "113",
            "slug": "jwt",
            "name": "Jwt"
          },
          {
            "id": "114",
            "slug": "php",
            "name": "Php"
          },
          {
            "id": "115",
            "slug": "cqrs",
            "name": "Cqrs"
          },
          {
            "id": "116",
            "slug": "erp",
            "name": "Erp"
          },
          {
            "id": "117",
            "slug": "hexagonal-architecture",
            "name": "Hexagonal architecture"
          },
          {
            "id": "118",
            "slug": "design-patterns",
            "name": "Design patterns"
          }
        ],
        "posts_tags": [
          {
            "post_id": "1",
            "tag_id": "1"
          },
          {
            "post_id": "1",
            "tag_id": "2"
          },
          {
            "post_id": "1",
            "tag_id": "3"
          },
          {
            "post_id": "1",
            "tag_id": "4"
          },
          {
            "post_id": "1",
            "tag_id": "5"
          },
          {
            "post_id": "2",
            "tag_id": "1"
          },
          {
            "post_id": "2",
            "tag_id": "6"
          },
          {
            "post_id": "2",
            "tag_id": "7"
          },
          {
            "post_id": "2",
            "tag_id": "8"
          },
          {
            "post_id": "2",
            "tag_id": "9"
          },
          {
            "post_id": "3",
            "tag_id": "1"
          },
          {
            "post_id": "3",
            "tag_id": "10"
          },
          {
            "post_id": "3",
            "tag_id": "5"
          },
          {
            "post_id": "3",
            "tag_id": "11"
          },
          {
            "post_id": "3",
            "tag_id": "12"
          },
          {
            "post_id": "4",
            "tag_id": "1"
          },
          {
            "post_id": "4",
            "tag_id": "6"
          },
          {
            "post_id": "4",
            "tag_id": "13"
          },
          {
            "post_id": "4",
            "tag_id": "8"
          },
          {
            "post_id": "5",
            "tag_id": "1"
          },
          {
            "post_id": "5",
            "tag_id": "7"
          },
          {
            "post_id": "5",
            "tag_id": "6"
          },
          {
            "post_id": "5",
            "tag_id": "13"
          },
          {
            "post_id": "6",
            "tag_id": "13"
          },
          {
            "post_id": "6",
            "tag_id": "14"
          },
          {
            "post_id": "6",
            "tag_id": "15"
          },
          {
            "post_id": "6",
            "tag_id": "16"
          },
          {
            "post_id": "6",
            "tag_id": "1"
          },
          {
            "post_id": "6",
            "tag_id": "7"
          },
          {
            "post_id": "7",
            "tag_id": "17"
          },
          {
            "post_id": "7",
            "tag_id": "18"
          },
          {
            "post_id": "7",
            "tag_id": "19"
          },
          {
            "post_id": "7",
            "tag_id": "20"
          },
          {
            "post_id": "7",
            "tag_id": "21"
          },
          {
            "post_id": "8",
            "tag_id": "22"
          },
          {
            "post_id": "8",
            "tag_id": "23"
          },
          {
            "post_id": "8",
            "tag_id": "24"
          },
          {
            "post_id": "8",
            "tag_id": "25"
          },
          {
            "post_id": "9",
            "tag_id": "24"
          },
          {
            "post_id": "9",
            "tag_id": "26"
          },
          {
            "post_id": "9",
            "tag_id": "27"
          },
          {
            "post_id": "10",
            "tag_id": "23"
          },
          {
            "post_id": "10",
            "tag_id": "28"
          },
          {
            "post_id": "10",
            "tag_id": "29"
          },
          {
            "post_id": "11",
            "tag_id": "30"
          },
          {
            "post_id": "11",
            "tag_id": "31"
          },
          {
            "post_id": "11",
            "tag_id": "20"
          },
          {
            "post_id": "11",
            "tag_id": "32"
          },
          {
            "post_id": "11",
            "tag_id": "33"
          },
          {
            "post_id": "11",
            "tag_id": "34"
          },
          {
            "post_id": "12",
            "tag_id": "35"
          },
          {
            "post_id": "12",
            "tag_id": "36"
          },
          {
            "post_id": "12",
            "tag_id": "37"
          },
          {
            "post_id": "12",
            "tag_id": "38"
          },
          {
            "post_id": "12",
            "tag_id": "39"
          },
          {
            "post_id": "12",
            "tag_id": "40"
          },
          {
            "post_id": "13",
            "tag_id": "41"
          },
          {
            "post_id": "13",
            "tag_id": "42"
          },
          {
            "post_id": "13",
            "tag_id": "31"
          },
          {
            "post_id": "13",
            "tag_id": "43"
          },
          {
            "post_id": "13",
            "tag_id": "44"
          },
          {
            "post_id": "13",
            "tag_id": "45"
          },
          {
            "post_id": "14",
            "tag_id": "46"
          },
          {
            "post_id": "14",
            "tag_id": "47"
          },
          {
            "post_id": "14",
            "tag_id": "32"
          },
          {
            "post_id": "14",
            "tag_id": "14"
          },
          {
            "post_id": "14",
            "tag_id": "48"
          },
          {
            "post_id": "14",
            "tag_id": "49"
          },
          {
            "post_id": "15",
            "tag_id": "32"
          },
          {
            "post_id": "15",
            "tag_id": "36"
          },
          {
            "post_id": "15",
            "tag_id": "30"
          },
          {
            "post_id": "15",
            "tag_id": "39"
          },
          {
            "post_id": "15",
            "tag_id": "50"
          },
          {
            "post_id": "16",
            "tag_id": "51"
          },
          {
            "post_id": "16",
            "tag_id": "52"
          },
          {
            "post_id": "16",
            "tag_id": "53"
          },
          {
            "post_id": "16",
            "tag_id": "8"
          },
          {
            "post_id": "16",
            "tag_id": "54"
          },
          {
            "post_id": "17",
            "tag_id": "55"
          },
          {
            "post_id": "17",
            "tag_id": "56"
          },
          {
            "post_id": "17",
            "tag_id": "57"
          },
          {
            "post_id": "17",
            "tag_id": "58"
          },
          {
            "post_id": "17",
            "tag_id": "59"
          },
          {
            "post_id": "17",
            "tag_id": "60"
          },
          {
            "post_id": "18",
            "tag_id": "61"
          },
          {
            "post_id": "18",
            "tag_id": "62"
          },
          {
            "post_id": "18",
            "tag_id": "63"
          },
          {
            "post_id": "18",
            "tag_id": "64"
          },
          {
            "post_id": "18",
            "tag_id": "65"
          },
          {
            "post_id": "19",
            "tag_id": "66"
          },
          {
            "post_id": "19",
            "tag_id": "67"
          },
          {
            "post_id": "19",
            "tag_id": "68"
          },
          {
            "post_id": "19",
            "tag_id": "8"
          },
          {
            "post_id": "19",
            "tag_id": "36"
          },
          {
            "post_id": "20",
            "tag_id": "69"
          },
          {
            "post_id": "20",
            "tag_id": "70"
          },
          {
            "post_id": "20",
            "tag_id": "71"
          },
          {
            "post_id": "20",
            "tag_id": "72"
          },
          {
            "post_id": "20",
            "tag_id": "73"
          },
          {
            "post_id": "20",
            "tag_id": "74"
          },
          {
            "post_id": "21",
            "tag_id": "15"
          },
          {
            "post_id": "21",
            "tag_id": "75"
          },
          {
            "post_id": "21",
            "tag_id": "70"
          },
          {
            "post_id": "21",
            "tag_id": "8"
          },
          {
            "post_id": "21",
            "tag_id": "76"
          },
          {
            "post_id": "21",
            "tag_id": "16"
          },
          {
            "post_id": "22",
            "tag_id": "57"
          },
          {
            "post_id": "22",
            "tag_id": "56"
          },
          {
            "post_id": "22",
            "tag_id": "70"
          },
          {
            "post_id": "22",
            "tag_id": "55"
          },
          {
            "post_id": "23",
            "tag_id": "77"
          },
          {
            "post_id": "23",
            "tag_id": "78"
          },
          {
            "post_id": "23",
            "tag_id": "79"
          },
          {
            "post_id": "23",
            "tag_id": "80"
          },
          {
            "post_id": "23",
            "tag_id": "8"
          },
          {
            "post_id": "24",
            "tag_id": "81"
          },
          {
            "post_id": "24",
            "tag_id": "82"
          },
          {
            "post_id": "24",
            "tag_id": "83"
          },
          {
            "post_id": "24",
            "tag_id": "84"
          },
          {
            "post_id": "24",
            "tag_id": "76"
          },
          {
            "post_id": "24",
            "tag_id": "15"
          },
          {
            "post_id": "25",
            "tag_id": "85"
          },
          {
            "post_id": "25",
            "tag_id": "86"
          },
          {
            "post_id": "25",
            "tag_id": "87"
          },
          {
            "post_id": "25",
            "tag_id": "88"
          },
          {
            "post_id": "26",
            "tag_id": "89"
          },
          {
            "post_id": "26",
            "tag_id": "90"
          },
          {
            "post_id": "26",
            "tag_id": "91"
          },
          {
            "post_id": "26",
            "tag_id": "92"
          },
          {
            "post_id": "27",
            "tag_id": "93"
          },
          {
            "post_id": "27",
            "tag_id": "94"
          },
          {
            "post_id": "27",
            "tag_id": "35"
          },
          {
            "post_id": "27",
            "tag_id": "16"
          },
          {
            "post_id": "28",
            "tag_id": "93"
          },
          {
            "post_id": "28",
            "tag_id": "94"
          },
          {
            "post_id": "28",
            "tag_id": "95"
          },
          {
            "post_id": "28",
            "tag_id": "77"
          },
          {
            "post_id": "28",
            "tag_id": "16"
          },
          {
            "post_id": "28",
            "tag_id": "37"
          },
          {
            "post_id": "29",
            "tag_id": "16"
          },
          {
            "post_id": "29",
            "tag_id": "96"
          },
          {
            "post_id": "29",
            "tag_id": "97"
          },
          {
            "post_id": "29",
            "tag_id": "98"
          },
          {
            "post_id": "30",
            "tag_id": "30"
          },
          {
            "post_id": "30",
            "tag_id": "32"
          },
          {
            "post_id": "30",
            "tag_id": "99"
          },
          {
            "post_id": "30",
            "tag_id": "100"
          },
          {
            "post_id": "30",
            "tag_id": "101"
          },
          {
            "post_id": "30",
            "tag_id": "8"
          },
          {
            "post_id": "31",
            "tag_id": "32"
          },
          {
            "post_id": "31",
            "tag_id": "30"
          },
          {
            "post_id": "31",
            "tag_id": "102"
          },
          {
            "post_id": "31",
            "tag_id": "16"
          },
          {
            "post_id": "32",
            "tag_id": "35"
          },
          {
            "post_id": "32",
            "tag_id": "8"
          },
          {
            "post_id": "32",
            "tag_id": "37"
          },
          {
            "post_id": "32",
            "tag_id": "103"
          },
          {
            "post_id": "32",
            "tag_id": "100"
          },
          {
            "post_id": "32",
            "tag_id": "94"
          },
          {
            "post_id": "33",
            "tag_id": "104"
          },
          {
            "post_id": "33",
            "tag_id": "105"
          },
          {
            "post_id": "33",
            "tag_id": "8"
          },
          {
            "post_id": "33",
            "tag_id": "70"
          },
          {
            "post_id": "34",
            "tag_id": "15"
          },
          {
            "post_id": "34",
            "tag_id": "14"
          },
          {
            "post_id": "34",
            "tag_id": "16"
          },
          {
            "post_id": "34",
            "tag_id": "8"
          },
          {
            "post_id": "35",
            "tag_id": "106"
          },
          {
            "post_id": "35",
            "tag_id": "107"
          },
          {
            "post_id": "35",
            "tag_id": "108"
          },
          {
            "post_id": "35",
            "tag_id": "8"
          },
          {
            "post_id": "35",
            "tag_id": "109"
          },
          {
            "post_id": "36",
            "tag_id": "110"
          },
          {
            "post_id": "36",
            "tag_id": "111"
          },
          {
            "post_id": "36",
            "tag_id": "62"
          },
          {
            "post_id": "36",
            "tag_id": "37"
          },
          {
            "post_id": "36",
            "tag_id": "8"
          },
          {
            "post_id": "37",
            "tag_id": "112"
          },
          {
            "post_id": "37",
            "tag_id": "62"
          },
          {
            "post_id": "37",
            "tag_id": "113"
          },
          {
            "post_id": "37",
            "tag_id": "15"
          },
          {
            "post_id": "38",
            "tag_id": "78"
          },
          {
            "post_id": "38",
            "tag_id": "79"
          },
          {
            "post_id": "38",
            "tag_id": "43"
          },
          {
            "post_id": "38",
            "tag_id": "114"
          },
          {
            "post_id": "39",
            "tag_id": "115"
          },
          {
            "post_id": "39",
            "tag_id": "8"
          },
          {
            "post_id": "39",
            "tag_id": "108"
          },
          {
            "post_id": "39",
            "tag_id": "36"
          },
          {
            "post_id": "39",
            "tag_id": "76"
          },
          {
            "post_id": "40",
            "tag_id": "17"
          },
          {
            "post_id": "40",
            "tag_id": "99"
          },
          {
            "post_id": "40",
            "tag_id": "116"
          },
          {
            "post_id": "40",
            "tag_id": "8"
          },
          {
            "post_id": "41",
            "tag_id": "8"
          },
          {
            "post_id": "41",
            "tag_id": "117"
          },
          {
            "post_id": "41",
            "tag_id": "108"
          },
          {
            "post_id": "41",
            "tag_id": "15"
          },
          {
            "post_id": "41",
            "tag_id": "118"
          }
        ],
        "users": [
          {
            "id": "1",
            "name": "Jason Cochran",
            "slug": "jason-cochran",
            "email": "jason@jasoncochran.io",
            "roles": [
              "Owner"
            ]
          }
        ],
        "posts_authors": [
          {
            "post_id": "1",
            "author_id": "1"
          },
          {
            "post_id": "2",
            "author_id": "1"
          },
          {
            "post_id": "3",
            "author_id": "1"
          },
          {
            "post_id": "4",
            "author_id": "1"
          },
          {
            "post_id": "5",
            "author_id": "1"
          },
          {
            "post_id": "6",
            "author_id": "1"
          },
          {
            "post_id": "7",
            "author_id": "1"
          },
          {
            "post_id": "8",
            "author_id": "1"
          },
          {
            "post_id": "9",
            "author_id": "1"
          },
          {
            "post_id": "10",
            "author_id": "1"
          },
          {
            "post_id": "11",
            "author_id": "1"
          },
          {
            "post_id": "12",
            "author_id": "1"
          },
          {
            "post_id": "13",
            "author_id": "1"
          },
          {
            "post_id": "14",
            "author_id": "1"
          },
          {
            "post_id": "15",
            "author_id": "1"
          },
          {
            "post_id": "16",
            "author_id": "1"
          },
          {
            "post_id": "17",
            "author_id": "1"
          },
          {
            "post_id": "18",
            "author_id": "1"
          },
          {
            "post_id": "19",
            "author_id": "1"
          },
          {
            "post_id": "20",
            "author_id": "1"
          },
          {
            "post_id": "21",
            "author_id": "1"
          },
          {
            "post_id": "22",
            "author_id": "1"
          },
          {
            "post_id": "23",
            "author_id": "1"
          },
          {
            "post_id": "24",
            "author_id": "1"
          },
          {
            "post_id": "25",
            "author_id": "1"
          },
          {
            "post_id": "26",
            "author_id": "1"
          },
          {
            "post_id": "27",
            "author_id": "1"
          },
          {
            "post_id": "28",
            "author_id": "1"
          },
          {
            "post_id": "29",
            "author_id": "1"
          },
          {
            "post_id": "30",
            "author_id": "1"
          },
          {
            "post_id": "31",
            "author_id": "1"
          },
          {
            "post_id": "32",
            "author_id": "1"
          },
          {
            "post_id": "33",
            "author_id": "1"
          },
          {
            "post_id": "34",
            "author_id": "1"
          },
          {
            "post_id": "35",
            "author_id": "1"
          },
          {
            "post_id": "36",
            "author_id": "1"
          },
          {
            "post_id": "37",
            "author_id": "1"
          },
          {
            "post_id": "38",
            "author_id": "1"
          },
          {
            "post_id": "39",
            "author_id": "1"
          },
          {
            "post_id": "40",
            "author_id": "1"
          },
          {
            "post_id": "41",
            "author_id": "1"
          }
        ]
      }
    }
  ]
}